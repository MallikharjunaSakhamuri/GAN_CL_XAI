{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f019c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow[and-cuda]\n",
    "# !pip install numpy==1.24.3 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e29e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd4938d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Malli\\anaconda3\\envs\\baceenv\\lib\\site-packages\\torch_geometric\\typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "C:\\Users\\Malli\\AppData\\Local\\Temp\\ipykernel_89788\\3939243607.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "import shap\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, MessagePassing\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from torch_geometric.data import Data\n",
    "from rdkit.Chem import RemoveHs\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "from rdkit import RDLogger\n",
    "# Suppress RDKit warnings\n",
    "RDLogger.DisableLog('rdApp.warning')\n",
    "import tensorflow as tensorflow\n",
    "import traceback\n",
    "import os\n",
    "from datetime import datetime\n",
    "from rdkit.Chem import rdDepictor\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit.Chem import AllChem, Draw, rdDepictor\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "class GraphDiscriminator(nn.Module):\n",
    "    \"\"\"Reimplementation of original discriminator architecture\"\"\"\n",
    "    def __init__(self, node_dim: int, edge_dim: int, hidden_dim: int = 128, output_dim: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature encoding\n",
    "        self.node_encoder = nn.Sequential(\n",
    "            nn.Linear(node_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "        \n",
    "        # Projection head\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x = torch.cat([data.x_cat.float(), data.x_phys], dim=-1)\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr.float()\n",
    "        batch = data.batch\n",
    "        \n",
    "        # Initial feature encoding\n",
    "        x = self.node_encoder(x)\n",
    "        edge_attr = self.edge_encoder(edge_attr)\n",
    "        \n",
    "        # Graph convolutions\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Projection\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Load Encoder\n",
    "def load_encoder(model_path, device='cpu'):\n",
    "    \"\"\"Load trained encoder\"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    encoder = GraphDiscriminator(\n",
    "        node_dim=checkpoint['model_info'].get('node_dim'),\n",
    "        edge_dim=checkpoint['model_info'].get('edge_dim'),\n",
    "        hidden_dim=checkpoint['model_info'].get('hidden_dim', 128),\n",
    "        output_dim=checkpoint['model_info'].get('output_dim', 128)\n",
    "    )\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    encoder.eval()\n",
    "    return encoder.to(device)\n",
    "\n",
    "# Load Embeddings\n",
    "def load_embeddings(filepath):\n",
    "    \"\"\"Load embeddings and labels\"\"\"\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data['embeddings'], data['labels']\n",
    "\n",
    "# Paths from your saved model\n",
    "encoder_path = './checkpoints/encoders/final_encoder_20250216_111050.pt'\n",
    "embedding_path = './embeddings/final_embeddings_20250216_111005.pkl'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load encoder and embeddings\n",
    "encoder = load_encoder(encoder_path, device)\n",
    "embeddings, graph_data = load_embeddings(embedding_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "044b2c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.atom_list = list(range(1, 119))\n",
    "        self.chirality_list = [\n",
    "            Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "            Chem.rdchem.ChiralType.CHI_OTHER\n",
    "        ]\n",
    "        self.bond_list = [\n",
    "            Chem.rdchem.BondType.SINGLE,\n",
    "            Chem.rdchem.BondType.DOUBLE, \n",
    "            Chem.rdchem.BondType.TRIPLE,\n",
    "            Chem.rdchem.BondType.AROMATIC\n",
    "        ]\n",
    "        self.bonddir_list = [\n",
    "            Chem.rdchem.BondDir.NONE,\n",
    "            Chem.rdchem.BondDir.ENDUPRIGHT,\n",
    "            Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
    "        ]\n",
    "\n",
    "    def calc_atom_features(self, atom: Chem.Atom) -> Tuple[list, list]:\n",
    "        \"\"\"Calculate atom features with better error handling\"\"\"\n",
    "        try:\n",
    "            # Basic features\n",
    "            atom_feat = [\n",
    "                self.atom_list.index(atom.GetAtomicNum()),\n",
    "                self.chirality_list.index(atom.GetChiralTag())\n",
    "            ]\n",
    "\n",
    "            # Physical features with error handling\n",
    "            phys_feat = []\n",
    "            \n",
    "            # Molecular weight contribution\n",
    "            try:\n",
    "                contrib_mw = Descriptors.ExactMolWt(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
    "                phys_feat.append(contrib_mw)\n",
    "            except:\n",
    "                phys_feat.append(0.0)\n",
    "                \n",
    "            # LogP contribution    \n",
    "            try:\n",
    "                contrib_logp = Descriptors.MolLogP(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
    "                phys_feat.append(contrib_logp)\n",
    "            except:\n",
    "                phys_feat.append(0.0)\n",
    "                \n",
    "            # Add other physical properties\n",
    "            phys_feat.extend([\n",
    "                atom.GetFormalCharge(),\n",
    "                int(atom.GetHybridization()),\n",
    "                int(atom.GetIsAromatic()),\n",
    "                atom.GetTotalNumHs(),\n",
    "                atom.GetTotalValence(),\n",
    "                atom.GetDegree()\n",
    "            ])\n",
    "            \n",
    "            return atom_feat, phys_feat\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating atom features: {e}\")\n",
    "            return [0, 0], [0.0] * 9\n",
    "\n",
    "    def get_atom_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Extract atom features for the whole molecule\"\"\"\n",
    "        atom_feats = []\n",
    "        phys_feats = []\n",
    "        \n",
    "        if mol is None:\n",
    "            return torch.tensor([[0, 0]], dtype=torch.long), torch.tensor([[0.0] * 9], dtype=torch.float)\n",
    "            \n",
    "        for atom in mol.GetAtoms():\n",
    "            atom_feat, phys_feat = self.calc_atom_features(atom)\n",
    "            atom_feats.append(atom_feat)\n",
    "            phys_feats.append(phys_feat)\n",
    "\n",
    "        x = torch.tensor(atom_feats, dtype=torch.long)\n",
    "        phys = torch.tensor(phys_feats, dtype=torch.float)\n",
    "        \n",
    "        return x, phys\n",
    "    \n",
    "    def remove_unbonded_hydrogens(mol):\n",
    "        params = Chem.RemoveHsParameters()\n",
    "        params.removeDegreeZero = True\n",
    "        mol = Chem.RemoveHs(mol, params)\n",
    "        return mol\n",
    "\n",
    "\n",
    "    def get_bond_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Extract bond features with better error handling\"\"\"\n",
    "        if mol is None:\n",
    "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
    "            \n",
    "        row, col, edge_feat = [], [], []\n",
    "        \n",
    "        for bond in mol.GetBonds():\n",
    "            try:\n",
    "                start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "                \n",
    "                # Add edges in both directions\n",
    "                row += [start, end]\n",
    "                col += [end, start]\n",
    "                \n",
    "                # Bond features\n",
    "                bond_type = self.bond_list.index(bond.GetBondType())\n",
    "                bond_dir = self.bonddir_list.index(bond.GetBondDir())\n",
    "                \n",
    "                # Calculate additional properties\n",
    "                feat = [\n",
    "                    bond_type,\n",
    "                    bond_dir,\n",
    "                    int(bond.GetIsConjugated()),\n",
    "                    int(self._is_rotatable(bond)),\n",
    "                    self._get_bond_length(mol, start, end)\n",
    "                ]\n",
    "                \n",
    "                edge_feat.extend([feat, feat])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing bond: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not row:  # If no valid bonds were processed\n",
    "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
    "\n",
    "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "        edge_attr = torch.tensor(edge_feat, dtype=torch.float)\n",
    "        \n",
    "        return edge_index, edge_attr\n",
    "\n",
    "    def _is_rotatable(self, bond: Chem.Bond) -> bool:\n",
    "        \"\"\"Check if bond is rotatable\"\"\"\n",
    "        return (bond.GetBondType() == Chem.rdchem.BondType.SINGLE and \n",
    "                not bond.IsInRing() and\n",
    "                len(bond.GetBeginAtom().GetNeighbors()) > 1 and\n",
    "                len(bond.GetEndAtom().GetNeighbors()) > 1)\n",
    "\n",
    "    def _get_bond_length(self, mol: Chem.Mol, start: int, end: int) -> float:\n",
    "        \"\"\"Get bond length with error handling\"\"\"\n",
    "        try:\n",
    "            conf = mol.GetConformer()\n",
    "            if conf.Is3D():\n",
    "                return Chem.rdMolTransforms.GetBondLength(conf, start, end)\n",
    "        except:\n",
    "            pass\n",
    "        return 0.0\n",
    "\n",
    "    def process_molecule(self, smiles: str) -> Data:\n",
    "        \"\"\"Process SMILES string to graph data\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                print(f\"Invalid SMILES: {smiles}\")\n",
    "                return None  # Skip invalid molecules\n",
    "            mol = RemoveHs(mol)\n",
    "\n",
    "            # Add explicit hydrogens\n",
    "            mol = Chem.AddHs(mol, addCoords=True)\n",
    "\n",
    "            # Sanitize molecule\n",
    "            Chem.SanitizeMol(mol)\n",
    "\n",
    "            # Check if the molecule has atoms\n",
    "            if mol.GetNumAtoms() == 0:\n",
    "                print(\"Molecule has no atoms, skipping.\")\n",
    "                return None\n",
    "\n",
    "            # Generate 3D coordinates\n",
    "            if not mol.GetNumConformers():\n",
    "                status = AllChem.EmbedMolecule(mol, AllChem.ETKDG())\n",
    "                if status != 0:\n",
    "                    print(\"Failed to generate 3D conformer\")\n",
    "                    return None  # Skip failed molecules\n",
    "\n",
    "                # Try MMFF or UFF optimization\n",
    "                try:\n",
    "                    AllChem.MMFFOptimizeMolecule(mol)\n",
    "                except:\n",
    "                    AllChem.UFFOptimizeMolecule(mol)\n",
    "\n",
    "            # Extract features\n",
    "            x_cat, x_phys = self.get_atom_features(mol)\n",
    "            edge_index, edge_attr = self.get_bond_features(mol)\n",
    "\n",
    "            return Data(\n",
    "                x_cat=x_cat, \n",
    "                x_phys=x_phys,\n",
    "                edge_index=edge_index, \n",
    "                edge_attr=edge_attr,\n",
    "                num_nodes=x_cat.size(0)\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing molecule {smiles}: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c661bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Batch, Data\n",
    "from typing import List, Tuple\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "class GraphModelWrapper:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "        self.original_x_cat = None\n",
    "        self.original_x_phys = None\n",
    "        self.batch = None\n",
    "        \n",
    "    def __call__(self, features):\n",
    "        \"\"\"\n",
    "        Custom call method to handle graph data\n",
    "        features: feature matrix (numpy array)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            if isinstance(features, np.ndarray):\n",
    "                features = torch.tensor(features, dtype=torch.float).to(self.device)\n",
    "            \n",
    "            # Get original shapes\n",
    "            num_nodes = self.original_x_cat.size(0)\n",
    "            num_cat_features = self.original_x_cat.size(1)\n",
    "            \n",
    "            # Reshape features to match original dimensions\n",
    "            x_cat = features.reshape(num_nodes, num_cat_features).to(torch.long)\n",
    "            \n",
    "            # Create a new batch with modified features\n",
    "            new_data = Data(\n",
    "                x_cat=x_cat,\n",
    "                x_phys=self.original_x_phys,\n",
    "                edge_index=self.batch.edge_index,\n",
    "                edge_attr=self.batch.edge_attr,\n",
    "                batch=self.batch.batch if hasattr(self.batch, 'batch') else None\n",
    "            )\n",
    "            \n",
    "            # Get model output\n",
    "            outputs = self.model(new_data)\n",
    "            return outputs.cpu().numpy()\n",
    "\n",
    "class ModifiedGraphWrapper:\n",
    "    def __init__(self, model, device, batch):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.original_batch = batch\n",
    "        self.model.eval()\n",
    "        self.num_nodes = batch.x_cat.shape[0]\n",
    "        self.num_features = batch.x_cat.shape[1]\n",
    "        self.num_phys_features = batch.x_phys.shape[1]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Convert input to tensor\n",
    "                x = torch.tensor(x, dtype=torch.float).to(self.device)\n",
    "                \n",
    "                # Reshape x to match the expected input shape\n",
    "                if len(x.shape) == 1:\n",
    "                    x = x.reshape(1, self.num_nodes, self.num_features)\n",
    "                else:\n",
    "                    x = x.reshape(-1, self.num_nodes, self.num_features)\n",
    "                \n",
    "                print(f\"Processing batch of size {x.shape[0]}\")\n",
    "                \n",
    "                all_results = []\n",
    "                for idx in range(x.shape[0]):\n",
    "                    # Extract categorical features\n",
    "                    x_cat = x[idx].to(torch.long)\n",
    "                    \n",
    "                    # Ensure proper dimensions for x_cat and x_phys\n",
    "                    if len(x_cat.shape) == 2:\n",
    "                        x_cat = x_cat.unsqueeze(0)\n",
    "                    x_phys = self.original_batch.x_phys.unsqueeze(0)\n",
    "                    \n",
    "                    # Create consistent batch dimension\n",
    "                    batch_idx = torch.zeros(self.num_nodes, dtype=torch.long, device=self.device)\n",
    "                    \n",
    "                    # Create data object for this sample\n",
    "                    new_data = Data(\n",
    "                        x_cat=x_cat.squeeze(0),\n",
    "                        x_phys=x_phys.squeeze(0),\n",
    "                        edge_index=self.original_batch.edge_index,\n",
    "                        edge_attr=self.original_batch.edge_attr,\n",
    "                        batch=batch_idx,\n",
    "                        num_nodes=self.num_nodes\n",
    "                    ).to(self.device)\n",
    "                    \n",
    "                    # Get node features\n",
    "                    node_features = torch.cat([new_data.x_cat.float(), new_data.x_phys], dim=-1)\n",
    "                    x_encoded = self.model.node_encoder(node_features)\n",
    "                    \n",
    "                    # Get intermediate representations\n",
    "                    x1 = F.relu(self.model.conv1(x_encoded, new_data.edge_index))\n",
    "                    x2 = F.relu(self.model.conv2(x1, new_data.edge_index))\n",
    "                    x3 = self.model.conv3(x2, new_data.edge_index)\n",
    "                    \n",
    "                    # Combine representations from different layers\n",
    "                    combined_features = torch.stack([x1, x2, x3], dim=0)\n",
    "                    node_embeddings = torch.mean(combined_features, dim=0)\n",
    "                    \n",
    "                    # Compute node importance\n",
    "                    node_importance = torch.norm(node_embeddings, dim=1).cpu().numpy()\n",
    "                    all_results.append(node_importance)\n",
    "                \n",
    "                result = np.array(all_results)\n",
    "                print(f\"Result shape: {result.shape}\")\n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in model wrapper: {e}\")\n",
    "                print(f\"Debug info:\")\n",
    "                print(f\"x shape: {x.shape}\")\n",
    "                if 'node_features' in locals():\n",
    "                    print(f\"node_features shape: {node_features.shape}\")\n",
    "                if 'node_embeddings' in locals():\n",
    "                    print(f\"node_embeddings shape: {node_embeddings.shape}\")\n",
    "                raise\n",
    "\n",
    "def explain_graph_model(model, graph_data, device, num_samples=100):\n",
    "    \"\"\"Generate SHAP explanations for graph neural network\"\"\"\n",
    "    batch = Batch.from_data_list([graph_data]).to(device)\n",
    "    \n",
    "    # Initialize wrapper\n",
    "    model_wrapper = ModifiedGraphWrapper(model, device, batch)\n",
    "    \n",
    "    # Create background data\n",
    "    background = batch.x_cat.cpu().numpy().astype(float)\n",
    "    background_flat = background.reshape(-1)\n",
    "    \n",
    "    # Generate background samples\n",
    "    n_background = 50\n",
    "    background_samples = []\n",
    "    for _ in range(n_background):\n",
    "        perturbed = background_flat.copy()\n",
    "        noise = np.random.normal(0, 0.3, perturbed.shape)\n",
    "        perturbed = np.clip(perturbed + noise, 0, None)\n",
    "        background_samples.append(perturbed)\n",
    "    \n",
    "    background_matrix = np.stack(background_samples)\n",
    "    print(f\"Background matrix shape: {background_matrix.shape}\")\n",
    "    \n",
    "    try:\n",
    "        # Test run\n",
    "        test_output = model_wrapper(background_matrix[0:1])\n",
    "        print(f\"Test output shape: {test_output.shape}\")\n",
    "        \n",
    "        # Initialize explainer\n",
    "        explainer = shap.KernelExplainer(\n",
    "            model_wrapper,\n",
    "            background_matrix,\n",
    "            link=\"identity\",\n",
    "            feature_perturbation=\"interventional\"\n",
    "        )\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        shap_values = explainer.shap_values(\n",
    "            background_flat,\n",
    "            nsamples=200,\n",
    "            l1_reg=\"num_features(10)\",\n",
    "            silent=True\n",
    "        )\n",
    "        \n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = np.array(shap_values)\n",
    "        \n",
    "        print(f\"Raw SHAP values shape: {shap_values.shape}\")\n",
    "        \n",
    "        # Process SHAP values to get node importance\n",
    "        # Sum absolute SHAP values across all features for each node\n",
    "        num_nodes = batch.x_cat.shape[0]\n",
    "        node_importance = np.zeros(num_nodes)\n",
    "        \n",
    "        # Aggregate SHAP values per node\n",
    "        features_per_node = batch.x_cat.shape[1]  # number of features per node\n",
    "        for i in range(num_nodes):\n",
    "            start_idx = i * features_per_node\n",
    "            end_idx = (i + 1) * features_per_node\n",
    "            node_importance[i] = np.abs(shap_values[start_idx:end_idx]).sum()\n",
    "        \n",
    "        # Normalize importance scores\n",
    "        node_importance = (node_importance - node_importance.min()) / \\\n",
    "                         (node_importance.max() - node_importance.min() + 1e-10)\n",
    "        \n",
    "        print(\"\\nNode importance statistics:\")\n",
    "        print(f\"Shape: {node_importance.shape}\")\n",
    "        print(f\"Range: {node_importance.min():.6f} to {node_importance.max():.6f}\")\n",
    "        print(f\"Mean: {node_importance.mean():.6f}\")\n",
    "        print(f\"Std: {node_importance.std():.6f}\")\n",
    "        \n",
    "        # Print top 5 most important nodes\n",
    "        sorted_indices = np.argsort(node_importance)[::-1]\n",
    "        print(\"\\nTop 5 most important nodes:\")\n",
    "        for i in range(5):\n",
    "            idx = sorted_indices[i]\n",
    "            print(f\"Node {idx}: {node_importance[idx]:.6f}\")\n",
    "        \n",
    "        return node_importance, shap_values\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in SHAP calculation: {e}\")\n",
    "        print(f\"Debug info:\")\n",
    "        print(f\"- Background matrix shape: {background_matrix.shape}\")\n",
    "        print(f\"- Number of nodes: {batch.x_cat.shape[0]}\")\n",
    "        print(f\"- Feature dimension: {batch.x_cat.shape[1]}\")\n",
    "        if 'shap_values' in locals():\n",
    "            print(f\"- SHAP values shape: {shap_values.shape}\")\n",
    "        raise\n",
    "\n",
    "        \n",
    "def visualize_molecule_with_importance(smiles, node_importance, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a figure with both molecular visualization and bar plot of importance scores\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import AllChem, Draw\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    \n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    num_atoms = mol.GetNumAtoms()\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12), \n",
    "                                  gridspec_kw={'height_ratios': [1.5, 1]})\n",
    "    \n",
    "    # 1. Molecular Visualization\n",
    "    atom_colors = {}\n",
    "    atom_info = []\n",
    "    \n",
    "    for i in range(num_atoms):\n",
    "        atom = mol.GetAtomWithIdx(i)\n",
    "        importance = node_importance[i]\n",
    "        atom_info.append((i, atom.GetSymbol(), importance))\n",
    "        \n",
    "        # Create color gradient from white (0) to red (1)\n",
    "        intensity = importance\n",
    "        atom_colors[i] = (1.0, 1.0 - intensity, 1.0 - intensity)\n",
    "    \n",
    "    # Generate 2D coordinates\n",
    "    AllChem.Compute2DCoords(mol)\n",
    "    \n",
    "    # Draw molecule\n",
    "    img = Draw.MolToImage(\n",
    "        mol,\n",
    "        highlightAtoms=list(range(num_atoms)),\n",
    "        highlightColor=None,\n",
    "        highlightAtomColors=atom_colors,\n",
    "        size=(400, 400)\n",
    "    )\n",
    "    \n",
    "    # Show molecule in first subplot\n",
    "    ax1.imshow(img)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Molecular Structure with SHAP Importance')\n",
    "    \n",
    "    # 2. Bar Plot\n",
    "    # Sort atoms by importance\n",
    "    atom_info.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Create bar plot\n",
    "    indices, symbols, scores = zip(*atom_info)\n",
    "    atom_labels = [f\"{symbols[i]}{indices[i]}\" for i in range(len(indices))]\n",
    "    \n",
    "    bars = ax2.bar(range(len(scores)), scores)\n",
    "    \n",
    "    # Color bars using same gradient as molecule\n",
    "    for idx, bar in enumerate(bars):\n",
    "        bar.set_facecolor((1.0, 1.0 - scores[idx], 1.0 - scores[idx]))\n",
    "    \n",
    "    # Customize bar plot\n",
    "    ax2.set_xticks(range(len(scores)))\n",
    "    ax2.set_xticklabels(atom_labels, rotation=45, ha='right')\n",
    "    ax2.set_ylabel('SHAP Importance Score')\n",
    "    ax2.set_title('Atom-wise SHAP Importance Scores')\n",
    "    \n",
    "    # Add grid for easier reading\n",
    "    ax2.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    return fig\n",
    "        \n",
    "def visualize_molecule_importance(smiles, node_importance, save_path=None):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    num_atoms = mol.GetNumAtoms()\n",
    "    \n",
    "    # Ensure we have the right number of importance scores\n",
    "    if len(node_importance) >= num_atoms:\n",
    "        # Get importance scores for atoms\n",
    "        imp_scores = node_importance[:num_atoms]\n",
    "        \n",
    "        # Use percentile-based normalization\n",
    "        min_val = np.percentile(imp_scores, 10)\n",
    "        max_val = np.percentile(imp_scores, 90)\n",
    "        \n",
    "        if max_val > min_val:\n",
    "            normalized_scores = (imp_scores - min_val) / (max_val - min_val)\n",
    "            normalized_scores = np.clip(normalized_scores, 0, 1)\n",
    "        else:\n",
    "            normalized_scores = np.zeros_like(imp_scores)\n",
    "        \n",
    "        # Apply non-linear scaling to enhance differences\n",
    "#         normalized_scores = np.power(normalized_scores, 0.5)\n",
    "        normalized_scores = np.exp(4 * normalized_scores) / np.exp(4)\n",
    "        \n",
    "        # Create atom colors\n",
    "        atom_colors = {}\n",
    "        importance_info = []\n",
    "        \n",
    "        for i in range(num_atoms):\n",
    "            atom = mol.GetAtomWithIdx(i)\n",
    "            score = normalized_scores[i]\n",
    "            raw_score = node_importance[i]\n",
    "            \n",
    "            if score < 0.3:\n",
    "                color = (1.0, 1.0, 1.0)  # White for low importance\n",
    "            elif score < 0.6:\n",
    "                color = (1.0, 0.7, 0.7)  # Light red for medium importance\n",
    "            else:\n",
    "                color = (1.0, 0.0, 0.0)  # Deep red for high importance\n",
    "            \n",
    "            atom_colors[i] = color\n",
    "            \n",
    "#             # Enhanced color gradient\n",
    "#             red = min(1.0, score * 1.2)\n",
    "#             white = max(0.0, 1.0 - score * 1.5)\n",
    "#             atom_colors[i] = (1.0, white, white)          \n",
    "            \n",
    "            importance_info.append((i, atom.GetSymbol(), raw_score, score))\n",
    "        \n",
    "        # Sort and print atom importance\n",
    "        importance_info.sort(key=lambda x: x[2], reverse=True)\n",
    "        print(\"\\nAtom Importance Ranking (Top 5):\")\n",
    "        for idx, symbol, raw_score, norm_score in importance_info[:5]:\n",
    "            print(f\"Atom {idx} ({symbol}): raw importance = {raw_score:.6f}, \"\n",
    "                  f\"normalized = {norm_score:.4f}\")\n",
    "        \n",
    "        # Generate 2D coordinates\n",
    "        AllChem.Compute2DCoords(mol)\n",
    "        \n",
    "        # Create list of bonds to highlight\n",
    "        bonds = []\n",
    "        for bond in mol.GetBonds():\n",
    "            begin_idx = bond.GetBeginAtomIdx()\n",
    "            end_idx = bond.GetEndAtomIdx()\n",
    "            # Use average of connected atoms' importance\n",
    "            bond_importance = (normalized_scores[begin_idx] + normalized_scores[end_idx]) / 2\n",
    "            if bond_importance > 0.5:  # Only highlight significant bonds\n",
    "                bonds.append(bond.GetIdx())\n",
    "        \n",
    "        # Draw molecule with enhanced visualization\n",
    "        img = Draw.MolToImage(\n",
    "            mol,\n",
    "            highlightAtoms=list(range(num_atoms)),\n",
    "            highlightColor=None,\n",
    "            highlightAtomColors=atom_colors,\n",
    "            highlightBonds=bonds,  # List of bond indices\n",
    "            size=(800, 800),\n",
    "            highlightRadius=0.5\n",
    "        )\n",
    "        \n",
    "        if save_path:\n",
    "            img.save(save_path)\n",
    "            \n",
    "            # Create legend with enhanced gradient\n",
    "            fig, ax = plt.subplots(figsize=(8, 1))\n",
    "            gradient = np.linspace(0, 1, 256).reshape(1, -1)\n",
    "            \n",
    "            # Enhanced colormap\n",
    "            colors = [(1,1,1), (1,0.5,0.5), (1,0,0)]\n",
    "            cmap = LinearSegmentedColormap.from_list(\"custom_enhanced_red\", colors, N=256)\n",
    "            \n",
    "            ax.imshow(gradient, aspect='auto', cmap=cmap)\n",
    "            ax.set_xticks([0, 128, 255])\n",
    "            ax.set_xticklabels(['Low', 'Medium', 'High'])\n",
    "            ax.set_yticks([])\n",
    "            plt.title('Atom Importance Scale')\n",
    "            plt.savefig(save_path.replace('.png', '_legend.png'), \n",
    "                       bbox_inches='tight', dpi=300)\n",
    "            plt.close()\n",
    "        \n",
    "        return img\n",
    "\n",
    "    \n",
    "# Also modify the generate_explanations function to print more information:\n",
    "def generate_explanations(model, dataset, device, idx=1):\n",
    "    \"\"\"\n",
    "    Generate and visualize explanations for a specific molecule\n",
    "    \"\"\"\n",
    "    # Get specific molecule\n",
    "    graph_data = dataset[idx]\n",
    "    \n",
    "    print(f\"Processing molecule {idx}\")\n",
    "    print(f\"Graph data shape - x_cat: {graph_data.x_cat.shape}, x_phys: {graph_data.x_phys.shape}\")\n",
    "    \n",
    "    # Generate SHAP explanations\n",
    "    node_importance, shap_values = explain_graph_model(\n",
    "        model,\n",
    "        graph_data,\n",
    "        device,\n",
    "        num_samples=100\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated node importance shape: {node_importance.shape}\")\n",
    "\n",
    "    # Create visualization with both molecule and bar plot\n",
    "    if hasattr(graph_data, 'smiles'):\n",
    "        print(f\"Visualizing molecule with SMILES: {graph_data.smiles}\")\n",
    "#         img = visualize_molecule_with_importance(\n",
    "        img = visualize_molecule_importance(\n",
    "            graph_data.smiles,\n",
    "            node_importance,\n",
    "            save_path=f\"molecule_explanation/molecule_explanation_{timestamp}.png\"\n",
    "        )\n",
    "        return node_importance, shap_values, img\n",
    "    else:\n",
    "        print(\"Warning: No SMILES data found for visualization\")\n",
    "        return node_importance, shap_values, None    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa1cbc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data loading...\n",
      "1. Loaded dataset with 41 graphs.\n",
      "2. Failed SMILES count: 0\n",
      "\n",
      "Selected molecule index: 1\n",
      "Processing molecule 1\n",
      "Graph data shape - x_cat: torch.Size([42, 2]), x_phys: torch.Size([42, 8])\n",
      "Background matrix shape: (50, 84)\n",
      "Processing batch of size 1\n",
      "Result shape: (1, 42)\n",
      "Test output shape: (1, 42)\n",
      "Processing batch of size 50\n",
      "Result shape: (50, 42)\n",
      "Processing batch of size 1\n",
      "Result shape: (1, 42)\n",
      "Processing batch of size 10000\n",
      "Result shape: (10000, 42)\n",
      "Raw SHAP values shape: (84, 42)\n",
      "\n",
      "Node importance statistics:\n",
      "Shape: (42,)\n",
      "Range: 0.000000 to 1.000000\n",
      "Mean: 0.210749\n",
      "Std: 0.300511\n",
      "\n",
      "Top 5 most important nodes:\n",
      "Node 2: 1.000000\n",
      "Node 14: 0.964057\n",
      "Node 13: 0.951046\n",
      "Node 9: 0.820947\n",
      "Node 15: 0.710237\n",
      "Generated node importance shape: (42,)\n",
      "Visualizing molecule with SMILES: CC[NH+](CC)C1CCC([NH2+]C2CC2)(C(=O)[O-])C1\n",
      "\n",
      "Atom Importance Ranking (Top 5):\n",
      "Atom 2 (N): raw importance = 1.000000, normalized = 1.0000\n",
      "Atom 14 (O): raw importance = 0.964057, normalized = 1.0000\n",
      "Atom 13 (C): raw importance = 0.951046, normalized = 0.9690\n",
      "Atom 9 (N): raw importance = 0.820947, normalized = 0.4405\n",
      "Atom 15 (O): raw importance = 0.710237, normalized = 0.2252\n",
      "Visualization saved as 'molecule_explanation/molecule_explanation_20250217_105715.png'\n",
      "Legend saved as 'molecule_explanation_legend_20250217_105715.png'\n"
     ]
    }
   ],
   "source": [
    "# First, let's modify your data loading code to store SMILES with each graph\n",
    "print(\"Starting data loading...\")\n",
    "extractor = MolecularFeatureExtractor()\n",
    "smiles_file = \"D:\\\\PhD\\\\Chapter3\\\\Unsupervised_GAN_Code\\\\pubchem-41-clean.txt\"\n",
    "\n",
    "dataset = []\n",
    "failed_smiles = []\n",
    "\n",
    "# Modified data loading to store SMILES\n",
    "with open(smiles_file, 'r') as f:\n",
    "    for line in f:\n",
    "        smiles = line.strip()\n",
    "        data = extractor.process_molecule(smiles)\n",
    "        if data is not None:\n",
    "            # Add SMILES as an attribute to the Data object\n",
    "            data.smiles = smiles  # Add this line\n",
    "            dataset.append(data)\n",
    "        else:\n",
    "            failed_smiles.append(smiles)\n",
    "\n",
    "print(f\"1. Loaded dataset with {len(dataset)} graphs.\")\n",
    "print(f\"2. Failed SMILES count: {len(failed_smiles)}\")\n",
    "\n",
    "if not dataset:\n",
    "    print(\"No valid graphs generated.\")\n",
    "    \n",
    "# Make sure to import needed libraries\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw\n",
    "import traceback\n",
    "\n",
    "os.makedirs('molecule_explanation', exist_ok=True)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "try:\n",
    "    print(\"\\nSelected molecule index: 1\")\n",
    "    node_importance, shap_values, img = generate_explanations(\n",
    "        encoder,\n",
    "        dataset,\n",
    "        device,\n",
    "        idx=1\n",
    "    )\n",
    "    \n",
    "    if img is not None:        \n",
    "        img.save(f'molecule_explanation/molecule_explanation_{timestamp}.png')\n",
    "        print(f\"Visualization saved as 'molecule_explanation/molecule_explanation_{timestamp}.png'\")\n",
    "        print(f\"Legend saved as 'molecule_explanation_legend_{timestamp}.png'\")        \n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError generating explanations: {e}\")\n",
    "    print(\"\\nModel architecture:\")\n",
    "    print(encoder)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f6299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
