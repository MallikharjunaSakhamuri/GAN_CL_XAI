{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b74cd233",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing embeddings:\n",
      "  - Before: ./embeddings/before_training_20250301_193202.npz\n",
      "  - After: ./embeddings/after_training_20250301_193202.npz\n",
      "  - Output: ./analysis_results\n",
      "Loaded 16 molecules from before-training embeddings\n",
      "Loaded 16 molecules from after-training embeddings\n",
      "Found 1 common molecules for analysis\n",
      "Extracting molecular properties...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/16 [00:00<?, ?it/s][20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 2876.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting functional group features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/16 [00:00<?, ?it/s][20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 16340.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting structural features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/16 [00:00<?, ?it/s][20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "[20:03:55] SMILES Parse Error: syntax error while parsing: unknown\n",
      "[20:03:55] SMILES Parse Error: check for mistakes around position 1:\n",
      "[20:03:55] unknown\n",
      "[20:03:55] ^\n",
      "[20:03:55] SMILES Parse Error: Failed parsing SMILES 'unknown' for input: 'unknown'\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 10630.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing property prediction analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Change'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 888\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 888\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 843\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# Create analyzer and run analysis\u001b[39;00m\n\u001b[0;32m    842\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m EmbeddingAnalyzer(output_dir\u001b[38;5;241m=\u001b[39moutput_dir)\n\u001b[1;32m--> 843\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_complete_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbefore_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mafter_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results:\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnalysis Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 782\u001b[0m, in \u001b[0;36mEmbeddingAnalyzer.run_complete_analysis\u001b[1;34m(self, before_path, after_path)\u001b[0m\n\u001b[0;32m    779\u001b[0m structural_features_df\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstructural_features.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    781\u001b[0m \u001b[38;5;66;03m# Run property prediction analysis\u001b[39;00m\n\u001b[1;32m--> 782\u001b[0m prop_prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproperty_prediction_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbefore_emb_common\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mafter_emb_common\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    783\u001b[0m prop_prediction\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproperty_prediction_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    784\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproperty_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m prop_prediction\n",
      "Cell \u001b[1;32mIn[1], line 311\u001b[0m, in \u001b[0;36mEmbeddingAnalyzer.property_prediction_analysis\u001b[1;34m(self, before_emb, after_emb, properties_df, props_to_analyze)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# Create DataFrame and sort by absolute change\u001b[39;00m\n\u001b[0;32m    310\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n\u001b[1;32m--> 311\u001b[0m results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbs_Change\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mresults_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mChange\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mabs()\n\u001b[0;32m    312\u001b[0m results_df \u001b[38;5;241m=\u001b[39m results_df\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbs_Change\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results_df\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\baceenv\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\baceenv\\lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Change'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, AllChem, Lipinski\n",
    "from rdkit.Chem.rdMolDescriptors import CalcTPSA\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.gridspec as gridspec\n",
    "import argparse\n",
    "\n",
    "\n",
    "class EmbeddingAnalyzer:\n",
    "    \"\"\"Class for analyzing molecular embeddings before and after GAN-CL training\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir='./analysis_results'):\n",
    "        \"\"\"Initialize the analyzer\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Directory to save analysis results\n",
    "        \"\"\"\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Set plotting style\n",
    "        sns.set_style('whitegrid')\n",
    "        plt.rcParams.update({\n",
    "            'font.family': 'sans-serif',\n",
    "            'font.size': 12,\n",
    "            'axes.labelsize': 14,\n",
    "            'axes.titlesize': 16,\n",
    "            'xtick.labelsize': 12,\n",
    "            'ytick.labelsize': 12,\n",
    "            'legend.fontsize': 12\n",
    "        })\n",
    "        \n",
    "        # Set colors for before/after\n",
    "        self.colors = {\n",
    "            'before': '#1f77b4',  # blue\n",
    "            'after': '#ff7f0e'    # orange\n",
    "        }\n",
    "    \n",
    "    def load_embeddings(self, before_path, after_path):\n",
    "        \"\"\"Load embeddings and SMILES from .npz files\n",
    "        \n",
    "        Args:\n",
    "            before_path: Path to before-training embeddings\n",
    "            after_path: Path to after-training embeddings\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (before_embeddings, before_smiles, after_embeddings, after_smiles)\n",
    "        \"\"\"\n",
    "        # Load before-training data\n",
    "        before_data = np.load(before_path, allow_pickle=True)\n",
    "        before_embeddings = before_data['embeddings']\n",
    "        before_smiles = before_data['smiles']\n",
    "        \n",
    "        # Load after-training data\n",
    "        after_data = np.load(after_path, allow_pickle=True)\n",
    "        after_embeddings = after_data['embeddings']\n",
    "        after_smiles = after_data['smiles']\n",
    "        \n",
    "        print(f\"Loaded {len(before_smiles)} molecules from before-training embeddings\")\n",
    "        print(f\"Loaded {len(after_smiles)} molecules from after-training embeddings\")\n",
    "        \n",
    "        return before_embeddings, before_smiles, after_embeddings, after_smiles\n",
    "    \n",
    "    def extract_molecular_properties(self, smiles_list):\n",
    "        \"\"\"Extract molecular properties from SMILES strings\n",
    "        \n",
    "        Args:\n",
    "            smiles_list: List of SMILES strings\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with molecular properties\n",
    "        \"\"\"\n",
    "        print(\"Extracting molecular properties...\")\n",
    "        properties = []\n",
    "        valid_smiles = []\n",
    "        \n",
    "        for smiles in tqdm(smiles_list):\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                continue\n",
    "                \n",
    "            valid_smiles.append(smiles)\n",
    "            properties.append({\n",
    "                'MW': Descriptors.ExactMolWt(mol),\n",
    "                'LogP': Descriptors.MolLogP(mol),\n",
    "                'TPSA': CalcTPSA(mol),\n",
    "                'NumHAcceptors': Lipinski.NumHAcceptors(mol),\n",
    "                'NumHDonors': Lipinski.NumHDonors(mol),\n",
    "                'NumRotatableBonds': Descriptors.NumRotatableBonds(mol),\n",
    "                'NumRings': mol.GetRingInfo().NumRings(),\n",
    "                'NumHeavyAtoms': mol.GetNumHeavyAtoms(),\n",
    "                'NumAtoms': mol.GetNumAtoms(),\n",
    "                'NumBonds': mol.GetNumBonds(),\n",
    "                'IsAromatic': 1 if any(atom.GetIsAromatic() for atom in mol.GetAtoms()) else 0\n",
    "            })\n",
    "        \n",
    "        properties_df = pd.DataFrame(properties, index=valid_smiles)\n",
    "        return properties_df\n",
    "    \n",
    "    def extract_functional_groups(self, smiles_list):\n",
    "        \"\"\"Extract functional group features from SMILES strings\n",
    "        \n",
    "        Args:\n",
    "            smiles_list: List of SMILES strings\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with functional group features\n",
    "        \"\"\"\n",
    "        print(\"Extracting functional group features...\")\n",
    "        features = []\n",
    "        valid_smiles = []\n",
    "        \n",
    "        # Define SMARTS patterns for functional groups\n",
    "        fg_patterns = {\n",
    "            'Alcohol': '[OX2H]',\n",
    "            'Amine': '[NX3;H2,H1,H0]',\n",
    "            'Carboxyl': '[CX3](=O)[OX2H1]',\n",
    "            'Carbonyl': '[CX3]=O',\n",
    "            'Ether': '[OD2][!O]',\n",
    "            'Ester': '[#6][CX3](=O)[OX2H0][#6]',\n",
    "            'Amide': '[NX3][CX3](=[OX1])',\n",
    "            'Halogen': '[F,Cl,Br,I]',\n",
    "            'Nitro': '[NX3](=O)=O',\n",
    "            'Nitrile': '[NX1]#[CX2]',\n",
    "            'Sulfone': '[#16X4](=[OX1])(=[OX1])',\n",
    "            'Sulfonamide': '[#16X4]([NX3])=O',\n",
    "            'Phosphate': '[PX4](=[OX1])([OX2])([OX2])[OX2]'\n",
    "        }\n",
    "        \n",
    "        # Compile the patterns\n",
    "        compiled_patterns = {name: Chem.MolFromSmarts(smarts) for name, smarts in fg_patterns.items()}\n",
    "        \n",
    "        # Extract features for each molecule\n",
    "        for smiles in tqdm(smiles_list):\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                continue\n",
    "                \n",
    "            valid_smiles.append(smiles)\n",
    "            \n",
    "            # Count each functional group\n",
    "            fg_counts = {}\n",
    "            for name, pattern in compiled_patterns.items():\n",
    "                if pattern is not None:\n",
    "                    matches = mol.GetSubstructMatches(pattern)\n",
    "                    fg_counts[name] = len(matches)\n",
    "                else:\n",
    "                    fg_counts[name] = 0\n",
    "            \n",
    "            features.append(fg_counts)\n",
    "        \n",
    "        features_df = pd.DataFrame(features, index=valid_smiles)\n",
    "        return features_df\n",
    "    \n",
    "    def extract_structural_features(self, smiles_list):\n",
    "        \"\"\"Extract structural features from SMILES strings\n",
    "        \n",
    "        Args:\n",
    "            smiles_list: List of SMILES strings\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with structural features\n",
    "        \"\"\"\n",
    "        print(\"Extracting structural features...\")\n",
    "        features = []\n",
    "        valid_smiles = []\n",
    "        \n",
    "        for smiles in tqdm(smiles_list):\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                continue\n",
    "                \n",
    "            valid_smiles.append(smiles)\n",
    "            \n",
    "            # Get ring information\n",
    "            ri = mol.GetRingInfo()\n",
    "            rings = ri.AtomRings()\n",
    "            \n",
    "            # Check for aromatic atoms\n",
    "            aromatic_atoms = sum(1 for atom in mol.GetAtoms() if atom.GetIsAromatic())\n",
    "            \n",
    "            # Check for heterocycles\n",
    "            heterocycles = 0\n",
    "            for ring in rings:\n",
    "                if any(mol.GetAtomWithIdx(idx).GetAtomicNum() != 6 for idx in ring):\n",
    "                    heterocycles += 1\n",
    "            \n",
    "            # Check for fused rings\n",
    "            fused_rings = 0\n",
    "            if len(rings) >= 2:\n",
    "                for i in range(len(rings)):\n",
    "                    for j in range(i+1, len(rings)):\n",
    "                        if len(set(rings[i]).intersection(set(rings[j]))) > 1:\n",
    "                            fused_rings += 1\n",
    "            \n",
    "            # Check for spiro rings\n",
    "            spiro_rings = 0\n",
    "            if len(rings) >= 2:\n",
    "                for i in range(len(rings)):\n",
    "                    for j in range(i+1, len(rings)):\n",
    "                        if len(set(rings[i]).intersection(set(rings[j]))) == 1:\n",
    "                            spiro_rings += 1\n",
    "            \n",
    "            # Check for bridged rings (simplified)\n",
    "            bridged_pattern = Chem.MolFromSmarts('[D4R]')\n",
    "            bridged_rings = 1 if bridged_pattern and mol.HasSubstructMatch(bridged_pattern) else 0\n",
    "            \n",
    "            # Check for macrocycles (ring size >= 8)\n",
    "            macrocycles = 0\n",
    "            for ring in rings:\n",
    "                if len(ring) >= 8:\n",
    "                    macrocycles += 1\n",
    "            \n",
    "            # Linear chain (no branching)\n",
    "            linear_chain = 1 if mol.GetNumBonds() == mol.GetNumAtoms() - 1 and len(rings) == 0 else 0\n",
    "            \n",
    "            # Branching (atoms with > 2 neighbors)\n",
    "            branched = sum(1 for atom in mol.GetAtoms() if atom.GetDegree() > 2)\n",
    "            \n",
    "            features.append({\n",
    "                'IsAromatic': 1 if aromatic_atoms > 0 else 0,\n",
    "                'NumAromaticAtoms': aromatic_atoms,\n",
    "                'NumHeterocycles': heterocycles,\n",
    "                'NumFusedRings': fused_rings,\n",
    "                'NumSpiroRings': spiro_rings,\n",
    "                'HasBridgedRings': bridged_rings,\n",
    "                'NumMacrocycles': macrocycles,\n",
    "                'IsLinearChain': linear_chain,\n",
    "                'NumBranches': branched,\n",
    "                'NumRings': len(rings)\n",
    "            })\n",
    "        \n",
    "        features_df = pd.DataFrame(features, index=valid_smiles)\n",
    "        return features_df\n",
    "    \n",
    "    def property_prediction_analysis(self, before_emb, after_emb, properties_df, \n",
    "                                    props_to_analyze=None):\n",
    "        \"\"\"Analyze how well embeddings predict molecular properties\n",
    "        \n",
    "        Args:\n",
    "            before_emb: Embeddings before training\n",
    "            after_emb: Embeddings after training\n",
    "            properties_df: DataFrame with molecular properties\n",
    "            props_to_analyze: List of properties to analyze (default: None = all numeric)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with prediction performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Performing property prediction analysis...\")\n",
    "        \n",
    "        # Get properties to analyze\n",
    "        if props_to_analyze is None:\n",
    "            props_to_analyze = []\n",
    "            for col in properties_df.columns:\n",
    "                if pd.api.types.is_numeric_dtype(properties_df[col]):\n",
    "                    props_to_analyze.append(col)\n",
    "        \n",
    "        # Standardize embeddings\n",
    "        scaler_before = StandardScaler()\n",
    "        scaler_after = StandardScaler()\n",
    "        \n",
    "        before_emb_scaled = scaler_before.fit_transform(before_emb)\n",
    "        after_emb_scaled = scaler_after.fit_transform(after_emb)\n",
    "        \n",
    "        # Initialize results\n",
    "        results = []\n",
    "        \n",
    "        # For each property, train models and evaluate\n",
    "        for prop in props_to_analyze:\n",
    "            print(f\"Analyzing property: {prop}\")\n",
    "            \n",
    "            if prop in properties_df.columns:\n",
    "                y = properties_df[prop].values\n",
    "                \n",
    "                # Ridge regression with cross-validation\n",
    "                model = Ridge(alpha=1.0)\n",
    "                \n",
    "                # Evaluate on before embeddings\n",
    "                before_scores = cross_val_score(model, before_emb_scaled, y, \n",
    "                                              cv=5, scoring='r2')\n",
    "                before_r2 = before_scores.mean()\n",
    "                \n",
    "                # Evaluate on after embeddings\n",
    "                after_scores = cross_val_score(model, after_emb_scaled, y, \n",
    "                                             cv=5, scoring='r2')\n",
    "                after_r2 = after_scores.mean()\n",
    "                \n",
    "                results.append({\n",
    "                    'Property': prop,\n",
    "                    'Before_R2': before_r2,\n",
    "                    'After_R2': after_r2,\n",
    "                    'Change': after_r2 - before_r2,\n",
    "                    'Percent_Change': (after_r2 - before_r2) / (abs(before_r2) + 1e-10) * 100\n",
    "                })\n",
    "        \n",
    "        # Create DataFrame and sort by absolute change\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df['Abs_Change'] = results_df['Change'].abs()\n",
    "        results_df = results_df.sort_values('Abs_Change', ascending=False)\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def embedding_sensitivity_analysis(self, before_emb, after_emb, properties_df, \n",
    "                                      props_to_analyze=None):\n",
    "        \"\"\"Analyze how sensitive embeddings are to different properties\n",
    "        \n",
    "        Args:\n",
    "            before_emb: Embeddings before training\n",
    "            after_emb: Embeddings after training\n",
    "            properties_df: DataFrame with molecular properties\n",
    "            props_to_analyze: List of properties to analyze (default: None = all numeric)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with sensitivity metrics\n",
    "        \"\"\"\n",
    "        print(\"Performing embedding sensitivity analysis...\")\n",
    "        \n",
    "        # Get properties to analyze\n",
    "        if props_to_analyze is None:\n",
    "            props_to_analyze = []\n",
    "            for col in properties_df.columns:\n",
    "                if pd.api.types.is_numeric_dtype(properties_df[col]):\n",
    "                    props_to_analyze.append(col)\n",
    "        \n",
    "        # Standardize embeddings\n",
    "        scaler_before = StandardScaler()\n",
    "        scaler_after = StandardScaler()\n",
    "        \n",
    "        before_emb_scaled = scaler_before.fit_transform(before_emb)\n",
    "        after_emb_scaled = scaler_after.fit_transform(after_emb)\n",
    "        \n",
    "        # Initialize results\n",
    "        results = []\n",
    "        \n",
    "        # For each property, analyze sensitivity\n",
    "        for prop in props_to_analyze:\n",
    "            print(f\"Analyzing sensitivity to: {prop}\")\n",
    "            \n",
    "            if prop in properties_df.columns:\n",
    "                values = properties_df[prop].values\n",
    "                \n",
    "                # For each molecule, find molecules with similar and different property values\n",
    "                similar_dists_before = []\n",
    "                different_dists_before = []\n",
    "                similar_dists_after = []\n",
    "                different_dists_after = []\n",
    "                \n",
    "                # Define similarity threshold (10% of range)\n",
    "                prop_range = np.max(values) - np.min(values)\n",
    "                threshold = 0.1 * prop_range\n",
    "                \n",
    "                for i in range(len(values)):\n",
    "                    for j in range(i+1, len(values)):\n",
    "                        # Check if property values are similar\n",
    "                        is_similar = abs(values[i] - values[j]) < threshold\n",
    "                        \n",
    "                        # Calculate distances in embedding spaces\n",
    "                        before_dist = np.linalg.norm(before_emb_scaled[i] - before_emb_scaled[j])\n",
    "                        after_dist = np.linalg.norm(after_emb_scaled[i] - after_emb_scaled[j])\n",
    "                        \n",
    "                        if is_similar:\n",
    "                            similar_dists_before.append(before_dist)\n",
    "                            similar_dists_after.append(after_dist)\n",
    "                        else:\n",
    "                            different_dists_before.append(before_dist)\n",
    "                            different_dists_after.append(after_dist)\n",
    "                \n",
    "                # Calculate average distances\n",
    "                avg_similar_before = np.mean(similar_dists_before) if similar_dists_before else np.nan\n",
    "                avg_different_before = np.mean(different_dists_before) if different_dists_before else np.nan\n",
    "                avg_similar_after = np.mean(similar_dists_after) if similar_dists_after else np.nan\n",
    "                avg_different_after = np.mean(different_dists_after) if different_dists_after else np.nan\n",
    "                \n",
    "                # Calculate sensitivity ratios\n",
    "                sensitivity_before = avg_different_before / avg_similar_before if avg_similar_before else np.nan\n",
    "                sensitivity_after = avg_different_after / avg_similar_after if avg_similar_after else np.nan\n",
    "                \n",
    "                results.append({\n",
    "                    'Property': prop,\n",
    "                    'Sensitivity_Before': sensitivity_before,\n",
    "                    'Sensitivity_After': sensitivity_after,\n",
    "                    'Change': sensitivity_after - sensitivity_before,\n",
    "                    'Percent_Change': (sensitivity_after - sensitivity_before) / (abs(sensitivity_before) + 1e-10) * 100\n",
    "                })\n",
    "        \n",
    "        # Create DataFrame and sort by absolute change\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df['Abs_Change'] = results_df['Change'].abs()\n",
    "        results_df = results_df.sort_values('Abs_Change', ascending=False)\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def feature_importance_analysis(self, before_emb, after_emb, features_df):\n",
    "        \"\"\"Analyze which molecular features are most important for embeddings\n",
    "        \n",
    "        Args:\n",
    "            before_emb: Embeddings before training\n",
    "            after_emb: Embeddings after training\n",
    "            features_df: DataFrame with molecular features (binary or counts)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with feature importance scores\n",
    "        \"\"\"\n",
    "        print(\"Performing feature importance analysis...\")\n",
    "        \n",
    "        # Standardize embeddings\n",
    "        scaler_before = StandardScaler()\n",
    "        scaler_after = StandardScaler()\n",
    "        \n",
    "        before_emb_scaled = scaler_before.fit_transform(before_emb)\n",
    "        after_emb_scaled = scaler_after.fit_transform(after_emb)\n",
    "        \n",
    "        # Initialize importance scores\n",
    "        before_importance = np.zeros(len(features_df.columns))\n",
    "        after_importance = np.zeros(len(features_df.columns))\n",
    "        \n",
    "        # For a subset of dimensions in the embeddings, train models and get feature importance\n",
    "        n_dims = min(5, before_emb.shape[1])  # Use top 5 dimensions or fewer\n",
    "        \n",
    "        for i in range(n_dims):\n",
    "            print(f\"Analyzing dimension {i+1}/{n_dims}...\")\n",
    "            \n",
    "            # Target is a single embedding dimension\n",
    "            y_before = before_emb_scaled[:, i]\n",
    "            y_after = after_emb_scaled[:, i]\n",
    "            \n",
    "            # Train random forest model for before embeddings\n",
    "            model_before = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            model_before.fit(features_df.values, y_before)\n",
    "            before_importance += model_before.feature_importances_\n",
    "            \n",
    "            # Train random forest model for after embeddings\n",
    "            model_after = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            model_after.fit(features_df.values, y_after)\n",
    "            after_importance += model_after.feature_importances_\n",
    "        \n",
    "        # Average importance scores across dimensions\n",
    "        before_importance /= n_dims\n",
    "        after_importance /= n_dims\n",
    "        \n",
    "        # Create DataFrame with results\n",
    "        results = pd.DataFrame({\n",
    "            'Feature': features_df.columns,\n",
    "            'Importance_Before': before_importance,\n",
    "            'Importance_After': after_importance,\n",
    "            'Change': after_importance - before_importance,\n",
    "            'Percent_Change': (after_importance - before_importance) / (before_importance + 1e-10) * 100\n",
    "        })\n",
    "        \n",
    "        # Sort by absolute change\n",
    "        results['Abs_Change'] = results['Change'].abs()\n",
    "        results = results.sort_values('Abs_Change', ascending=False)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def visualize_property_prediction(self, prediction_results):\n",
    "        \"\"\"Visualize property prediction performance\n",
    "        \n",
    "        Args:\n",
    "            prediction_results: DataFrame with prediction performance metrics\n",
    "            \n",
    "        Returns:\n",
    "            Path to saved figure\n",
    "        \"\"\"\n",
    "        print(\"Visualizing property prediction results...\")\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(12, 7))\n",
    "        \n",
    "        # Get data from results\n",
    "        properties = prediction_results['Property']\n",
    "        before_scores = prediction_results['Before_R2']\n",
    "        after_scores = prediction_results['After_R2']\n",
    "        changes = prediction_results['Change']\n",
    "        \n",
    "        # Set up bar positions\n",
    "        x = np.arange(len(properties))\n",
    "        width = 0.35\n",
    "        \n",
    "        # Create bars\n",
    "        rects1 = ax.bar(x - width/2, before_scores, width, \n",
    "                      label='Before Training', color=self.colors['before'], alpha=0.7)\n",
    "        rects2 = ax.bar(x + width/2, after_scores, width, \n",
    "                      label='After Training', color=self.colors['after'], alpha=0.7)\n",
    "        \n",
    "        # Add details\n",
    "        ax.set_xlabel('Molecular Property')\n",
    "        ax.set_ylabel('R² Score (Higher = Better)')\n",
    "        ax.set_title('How Well Embeddings Capture Molecular Properties')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(properties, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Add value labels and change indicators\n",
    "        for i, (before, after, change) in enumerate(zip(before_scores, after_scores, changes)):\n",
    "            # Add value labels\n",
    "            ax.annotate(f'{before:.2f}', xy=(i - width/2, before + 0.01), \n",
    "                      ha='center', va='bottom')\n",
    "            ax.annotate(f'{after:.2f}', xy=(i + width/2, after + 0.01), \n",
    "                      ha='center', va='bottom')\n",
    "            \n",
    "            # Add change arrow\n",
    "            if abs(change) > 0.01:  # Only show meaningful changes\n",
    "                color = 'green' if change > 0 else 'red'\n",
    "                style = 'solid' if change > 0 else 'dashed'\n",
    "                ax.annotate('', xy=(i + width/2, after), xytext=(i - width/2, before),\n",
    "                          arrowprops=dict(arrowstyle='->', color=color, linestyle=style))\n",
    "                # Add percentage change\n",
    "                percent = change * 100 / max(0.01, abs(before))  # Avoid division by zero\n",
    "                ax.annotate(f'{percent:+.1f}%', \n",
    "                          xy=(i, (before + after)/2 + 0.05),\n",
    "                          ha='center', va='bottom', color=color, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        fig_path = os.path.join(self.output_dir, 'property_prediction.png')\n",
    "        plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        return fig_path\n",
    "    \n",
    "    def visualize_embedding_sensitivity(self, sensitivity_results):\n",
    "        \"\"\"Visualize embedding sensitivity to properties\n",
    "        \n",
    "        Args:\n",
    "            sensitivity_results: DataFrame with sensitivity metrics\n",
    "            \n",
    "        Returns:\n",
    "            Path to saved figure\n",
    "        \"\"\"\n",
    "        print(\"Visualizing embedding sensitivity results...\")\n",
    "        \n",
    "        # Filter out NaN results\n",
    "        filtered_results = sensitivity_results.dropna(subset=['Sensitivity_Before', 'Sensitivity_After'])\n",
    "        \n",
    "        if len(filtered_results) == 0:\n",
    "            print(\"No valid sensitivity results to visualize\")\n",
    "            return None\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(12, 7))\n",
    "        \n",
    "        # Get data from results\n",
    "        properties = filtered_results['Property']\n",
    "        before_sens = filtered_results['Sensitivity_Before']\n",
    "        after_sens = filtered_results['Sensitivity_After']\n",
    "        changes = filtered_results['Change']\n",
    "        \n",
    "        # Set up bar positions\n",
    "        x = np.arange(len(properties))\n",
    "        width = 0.35\n",
    "        \n",
    "        # Create bars\n",
    "        rects1 = ax.bar(x - width/2, before_sens, width, \n",
    "                      label='Before Training', color=self.colors['before'], alpha=0.7)\n",
    "        rects2 = ax.bar(x + width/2, after_sens, width, \n",
    "                      label='After Training', color=self.colors['after'], alpha=0.7)\n",
    "        \n",
    "        # Add details\n",
    "        ax.set_xlabel('Molecular Property')\n",
    "        ax.set_ylabel('Sensitivity Ratio (Higher = Better Separation)')\n",
    "        ax.set_title('How Well Embeddings Separate Molecules by Properties')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(properties, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Add value labels and change indicators\n",
    "        for i, (before, after, change) in enumerate(zip(before_sens, after_sens, changes)):\n",
    "            # Add value labels\n",
    "            ax.annotate(f'{before:.2f}', xy=(i - width/2, before + 0.05), \n",
    "                      ha='center', va='bottom')\n",
    "            ax.annotate(f'{after:.2f}', xy=(i + width/2, after + 0.05), \n",
    "                      ha='center', va='bottom')\n",
    "            \n",
    "            # Add change arrow\n",
    "            if abs(change) > 0.01:  # Only show meaningful changes\n",
    "                color = 'green' if change > 0 else 'red'\n",
    "                style = 'solid' if change > 0 else 'dashed'\n",
    "                ax.annotate('', xy=(i + width/2, after), xytext=(i - width/2, before),\n",
    "                          arrowprops=dict(arrowstyle='->', color=color, linestyle=style))\n",
    "                # Add percentage change\n",
    "                percent = change * 100 / max(0.01, abs(before))  # Avoid division by zero\n",
    "                ax.annotate(f'{percent:+.1f}%', \n",
    "                          xy=(i, (before + after)/2 + 0.1),\n",
    "                          ha='center', va='bottom', color=color, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        fig_path = os.path.join(self.output_dir, 'embedding_sensitivity.png')\n",
    "        plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        return fig_path\n",
    "    \n",
    "    def visualize_feature_importance(self, importance_results, top_n=15):\n",
    "        \"\"\"Visualize feature importance for embeddings\n",
    "        \n",
    "        Args:\n",
    "            importance_results: DataFrame with feature importance metrics\n",
    "            top_n: Number of top features to show\n",
    "            \n",
    "        Returns:\n",
    "            Path to saved figure\n",
    "        \"\"\"\n",
    "        print(\"Visualizing feature importance results...\")\n",
    "        \n",
    "        # Get top N features by absolute change\n",
    "        top_features = importance_results.head(top_n).copy()\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        \n",
    "        # Sort by before importance\n",
    "        top_features = top_features.sort_values('Importance_Before')\n",
    "        \n",
    "        # Get data\n",
    "        features = top_features['Feature']\n",
    "        before_imp = top_features['Importance_Before']\n",
    "        after_imp = top_features['Importance_After']\n",
    "        changes = top_features['Change']\n",
    "        \n",
    "        # Set up bar positions\n",
    "        y = np.arange(len(features))\n",
    "        height = 0.35\n",
    "        \n",
    "        # Create bars\n",
    "        ax.barh(y - height/2, before_imp, height, \n",
    "              label='Before Training', color=self.colors['before'], alpha=0.7)\n",
    "        ax.barh(y + height/2, after_imp, height, \n",
    "              label='After Training', color=self.colors['after'], alpha=0.7)\n",
    "        \n",
    "        # Add details\n",
    "        ax.set_xlabel('Relative Importance')\n",
    "        ax.set_title('Importance of Molecular Features in Embedding Space')\n",
    "        ax.set_yticks(y)\n",
    "        ax.set_yticklabels(features)\n",
    "        ax.legend()\n",
    "        \n",
    "        # Add change indicators\n",
    "        for i, (before, after, change) in enumerate(zip(before_imp, after_imp, changes)):\n",
    "            # Add value labels\n",
    "            ax.annotate(f'{before:.3f}', xy=(before + 0.002, i - height/2), \n",
    "                      ha='left', va='center')\n",
    "            ax.annotate(f'{after:.3f}', xy=(after + 0.002, i + height/2), \n",
    "                      ha='left', va='center')\n",
    "            \n",
    "            # Add change arrow\n",
    "            if abs(change) > 0.005:  # Only show meaningful changes\n",
    "                color = 'green' if change > 0 else 'red'\n",
    "                style = 'solid' if change > 0 else 'dashed'\n",
    "                ax.annotate('', xy=(after, i + height/2), xytext=(before, i - height/2),\n",
    "                          arrowprops=dict(arrowstyle='->', color=color, linestyle=style))\n",
    "                \n",
    "                # Add percentage change\n",
    "                percent = change * 100 / max(0.01, before)  # Avoid division by zero\n",
    "                ax.annotate(f'{percent:+.1f}%', \n",
    "                          xy=((before + after)/2, i),\n",
    "                          ha='center', va='center', color=color, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        fig_path = os.path.join(self.output_dir, 'feature_importance.png')\n",
    "        plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        return fig_path\n",
    "    \n",
    "    def visualize_embedding_space(self, before_emb, after_emb, properties_df, \n",
    "                                prop_to_color='LogP'):\n",
    "        \"\"\"Visualize embedding spaces colored by property\n",
    "        \n",
    "        Args:\n",
    "            before_emb: Embeddings before training\n",
    "            after_emb: Embeddings after training\n",
    "            properties_df: DataFrame with properties\n",
    "            prop_to_color: Property to use for coloring points\n",
    "            \n",
    "        Returns:\n",
    "            Path to saved figure\n",
    "        \"\"\"\n",
    "        print(f\"Visualizing embedding spaces colored by {prop_to_color}...\")\n",
    "        \n",
    "        # Apply dimensionality reduction\n",
    "        pca = PCA(n_components=2)\n",
    "        before_pca = pca.fit_transform(before_emb)\n",
    "        pca = PCA(n_components=2)\n",
    "        after_pca = pca.fit_transform(after_emb)\n",
    "        \n",
    "        # Get property values\n",
    "        if prop_to_color in properties_df.columns:\n",
    "            colors = properties_df[prop_to_color].values\n",
    "            \n",
    "            # Create figure\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "            \n",
    "            # Before embedding\n",
    "            scatter1 = ax1.scatter(before_pca[:, 0], before_pca[:, 1], c=colors, \n",
    "                                 cmap='viridis', alpha=0.8, s=50)\n",
    "            ax1.set_title(f'Before Training Embedding Space\\nColored by {prop_to_color}')\n",
    "            ax1.set_xlabel('Principal Component 1')\n",
    "            ax1.set_ylabel('Principal Component 2')\n",
    "            \n",
    "            # After embedding\n",
    "            scatter2 = ax2.scatter(after_pca[:, 0], after_pca[:, 1], c=colors, \n",
    "                                 cmap='viridis', alpha=0.8, s=50)\n",
    "            ax2.set_title(f'After Training Embedding Space\\nColored by {prop_to_color}')\n",
    "            ax2.set_xlabel('Principal Component 1')\n",
    "            ax2.set_ylabel('Principal Component 2')\n",
    "            \n",
    "            # Add colorbar\n",
    "            cbar = plt.colorbar(scatter1, ax=[ax1, ax2])\n",
    "            cbar.set_label(prop_to_color)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save figure\n",
    "            fig_path = os.path.join(self.output_dir, f'embedding_space_{prop_to_color}.png')\n",
    "            plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            return fig_path\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def run_complete_analysis(self, before_path, after_path):\n",
    "        \"\"\"Run a complete analysis workflow on embedding files\n",
    "        \n",
    "        Args:\n",
    "            before_path: Path to before-training embeddings\n",
    "            after_path: Path to after-training embeddings\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with results and paths to saved figures\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Load embeddings\n",
    "        before_emb, before_smiles, after_emb, after_smiles = self.load_embeddings(before_path, after_path)\n",
    "        \n",
    "        # Make sure embeddings have the same molecules (by index) for proper comparison\n",
    "        # Get common SMILES\n",
    "        common_smiles = list(set(before_smiles).intersection(set(after_smiles)))\n",
    "        \n",
    "        if len(common_smiles) == 0:\n",
    "            print(\"No common molecules found in before and after embeddings!\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Found {len(common_smiles)} common molecules for analysis\")\n",
    "        \n",
    "        # Get indices for common SMILES\n",
    "        before_indices = [i for i, s in enumerate(before_smiles) if s in common_smiles]\n",
    "        after_indices = [i for i, s in enumerate(after_smiles) if s in common_smiles]\n",
    "        \n",
    "        # Filter embeddings\n",
    "        before_emb_common = before_emb[before_indices]\n",
    "        after_emb_common = after_emb[after_indices]\n",
    "        common_smiles = [before_smiles[i] for i in before_indices]  # Keep order consistent\n",
    "        \n",
    "        # Extract properties and features\n",
    "        properties_df = self.extract_molecular_properties(common_smiles)\n",
    "        functional_groups_df = self.extract_functional_groups(common_smiles)\n",
    "        structural_features_df = self.extract_structural_features(common_smiles)\n",
    "        \n",
    "        # Save the extracted data\n",
    "        properties_df.to_csv(os.path.join(self.output_dir, 'properties.csv'))\n",
    "        functional_groups_df.to_csv(os.path.join(self.output_dir, 'functional_groups.csv'))\n",
    "        structural_features_df.to_csv(os.path.join(self.output_dir, 'structural_features.csv'))\n",
    "        \n",
    "        # Run property prediction analysis\n",
    "        prop_prediction = self.property_prediction_analysis(before_emb_common, after_emb_common, properties_df)\n",
    "        prop_prediction.to_csv(os.path.join(self.output_dir, 'property_prediction_results.csv'))\n",
    "        results['property_prediction'] = prop_prediction\n",
    "        \n",
    "        # Run embedding sensitivity analysis\n",
    "        sensitivity = self.embedding_sensitivity_analysis(before_emb_common, after_emb_common, properties_df)\n",
    "        sensitivity.to_csv(os.path.join(self.output_dir, 'sensitivity_results.csv'))\n",
    "        results['sensitivity'] = sensitivity\n",
    "        \n",
    "        # Run feature importance analysis for functional groups\n",
    "        func_importance = self.feature_importance_analysis(before_emb_common, after_emb_common, functional_groups_df)\n",
    "        func_importance.to_csv(os.path.join(self.output_dir, 'functional_group_importance.csv'))\n",
    "        results['functional_importance'] = func_importance\n",
    "        \n",
    "        # Run feature importance analysis for structural features\n",
    "        struct_importance = self.feature_importance_analysis(before_emb_common, after_emb_common, structural_features_df)\n",
    "        struct_importance.to_csv(os.path.join(self.output_dir, 'structural_feature_importance.csv'))\n",
    "        results['structural_importance'] = struct_importance\n",
    "        \n",
    "        # Create visualizations\n",
    "        results['figures'] = {}\n",
    "        \n",
    "        # Property prediction visualization\n",
    "        prop_pred_fig = self.visualize_property_prediction(prop_prediction)\n",
    "        results['figures']['property_prediction'] = prop_pred_fig\n",
    "        \n",
    "        # Embedding sensitivity visualization\n",
    "        sensitivity_fig = self.visualize_embedding_sensitivity(sensitivity)\n",
    "        results['figures']['sensitivity'] = sensitivity_fig\n",
    "        \n",
    "        # Feature importance visualizations\n",
    "        func_imp_fig = self.visualize_feature_importance(func_importance, top_n=10)\n",
    "        results['figures']['functional_importance'] = func_imp_fig\n",
    "        \n",
    "        struct_imp_fig = self.visualize_feature_importance(struct_importance, top_n=10)\n",
    "        results['figures']['structural_importance'] = struct_imp_fig\n",
    "        \n",
    "        # Embedding space visualizations for key properties\n",
    "        for prop in ['LogP', 'MW', 'TPSA']:\n",
    "            if prop in properties_df.columns:\n",
    "                emb_space_fig = self.visualize_embedding_space(before_emb_common, after_emb_common, \n",
    "                                                              properties_df, prop)\n",
    "                results['figures'][f'embedding_space_{prop}'] = emb_space_fig\n",
    "        \n",
    "        print(\"Analysis complete! Results saved to:\", self.output_dir)\n",
    "        return results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to analyze embeddings\"\"\"\n",
    "    # Set file paths directly in the code\n",
    "    before_path = \"./embeddings/before_training_20250301_193202.npz\"\n",
    "    after_path = \"./embeddings/after_training_20250301_193202.npz\"\n",
    "    output_dir = \"./analysis_results\"\n",
    "    \n",
    "    print(f\"Analyzing embeddings:\")\n",
    "    print(f\"  - Before: {before_path}\")\n",
    "    print(f\"  - After: {after_path}\")\n",
    "    print(f\"  - Output: {output_dir}\")\n",
    "    \n",
    "    # Create analyzer and run analysis\n",
    "    analyzer = EmbeddingAnalyzer(output_dir=output_dir)\n",
    "    results = analyzer.run_complete_analysis(before_path, after_path)\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\nAnalysis Summary:\")\n",
    "        print(\"------------------\")\n",
    "        \n",
    "        # Print top improved properties\n",
    "        improved_props = results['property_prediction'][results['property_prediction']['Change'] > 0]\n",
    "        if not improved_props.empty:\n",
    "            print(\"\\nProperties better captured after training:\")\n",
    "            for idx, row in improved_props.iterrows():\n",
    "                print(f\"  - {row['Property']}: {row['Percent_Change']:.1f}% improvement in R²\")\n",
    "        \n",
    "        # Print top degraded properties\n",
    "        degraded_props = results['property_prediction'][results['property_prediction']['Change'] < 0]\n",
    "        if not degraded_props.empty:\n",
    "            print(\"\\nProperties less well captured after training:\")\n",
    "            for idx, row in degraded_props.iterrows():\n",
    "                print(f\"  - {row['Property']}: {row['Percent_Change']:.1f}% decrease in R²\")\n",
    "        \n",
    "        # Print top features with increased importance\n",
    "        increased_features = results['functional_importance'][results['functional_importance']['Change'] > 0].head(5)\n",
    "        if not increased_features.empty:\n",
    "            print(\"\\nFunctional groups with increased importance:\")\n",
    "            for idx, row in increased_features.iterrows():\n",
    "                print(f\"  - {row['Feature']}: {row['Percent_Change']:.1f}% increase in importance\")\n",
    "        \n",
    "        # Print top features with decreased importance\n",
    "        decreased_features = results['functional_importance'][results['functional_importance']['Change'] < 0].head(5)\n",
    "        if not decreased_features.empty:\n",
    "            print(\"\\nFunctional groups with decreased importance:\")\n",
    "            for idx, row in decreased_features.iterrows():\n",
    "                print(f\"  - {row['Feature']}: {row['Percent_Change']:.1f}% decrease in importance\")\n",
    "        \n",
    "        print(\"\\nVisualization files:\")\n",
    "        for name, path in results['figures'].items():\n",
    "            if path:\n",
    "                print(f\"  - {name}: {path}\")\n",
    "    else:\n",
    "        print(\"Analysis failed!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
