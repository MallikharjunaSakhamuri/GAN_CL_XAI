{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5379f427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Malli\\anaconda3\\envs\\baceenv\\lib\\site-packages\\torch_geometric\\typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "import random\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from rdkit import RDLogger\n",
    "from rdkit.Chem import RemoveHs\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import DataLoader\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, MessagePassing\n",
    "from typing import Tuple, List, Optional\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Suppress RDKit warnings\n",
    "RDLogger.DisableLog('rdApp.warning')\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de17bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.atom_list = list(range(1, 119))\n",
    "        self.chirality_list = [\n",
    "            Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "            Chem.rdchem.ChiralType.CHI_OTHER\n",
    "        ]\n",
    "        self.bond_list = [\n",
    "            Chem.rdchem.BondType.SINGLE,\n",
    "            Chem.rdchem.BondType.DOUBLE, \n",
    "            Chem.rdchem.BondType.TRIPLE,\n",
    "            Chem.rdchem.BondType.AROMATIC\n",
    "        ]\n",
    "        self.bonddir_list = [\n",
    "            Chem.rdchem.BondDir.NONE,\n",
    "            Chem.rdchem.BondDir.ENDUPRIGHT,\n",
    "            Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
    "        ]\n",
    "\n",
    "    def calc_atom_features(self, atom: Chem.Atom) -> Tuple[list, list]:\n",
    "        \"\"\"Calculate atom features with better error handling\"\"\"\n",
    "        try:\n",
    "            # Basic features\n",
    "            atom_feat = [\n",
    "                self.atom_list.index(atom.GetAtomicNum()),\n",
    "                self.chirality_list.index(atom.GetChiralTag())\n",
    "            ]\n",
    "\n",
    "            # Physical features with error handling\n",
    "            phys_feat = []\n",
    "            \n",
    "            # Molecular weight contribution\n",
    "            try:\n",
    "                contrib_mw = Descriptors.ExactMolWt(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
    "                phys_feat.append(contrib_mw)\n",
    "            except:\n",
    "                phys_feat.append(0.0)\n",
    "                \n",
    "            # LogP contribution    \n",
    "            try:\n",
    "                contrib_logp = Descriptors.MolLogP(Chem.MolFromSmiles(f'[{atom.GetSymbol()}]'))\n",
    "                phys_feat.append(contrib_logp)\n",
    "            except:\n",
    "                phys_feat.append(0.0)\n",
    "                \n",
    "            # Add other physical properties\n",
    "            phys_feat.extend([\n",
    "                atom.GetFormalCharge(),\n",
    "                int(atom.GetHybridization()),\n",
    "                int(atom.GetIsAromatic()),\n",
    "                atom.GetTotalNumHs(),\n",
    "                atom.GetTotalValence(),\n",
    "                atom.GetDegree()\n",
    "            ])\n",
    "            \n",
    "            return atom_feat, phys_feat\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating atom features: {e}\")\n",
    "            return [0, 0], [0.0] * 9\n",
    "\n",
    "    def get_atom_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Extract atom features for the whole molecule\"\"\"\n",
    "        atom_feats = []\n",
    "        phys_feats = []\n",
    "        \n",
    "        if mol is None:\n",
    "            return torch.tensor([[0, 0]], dtype=torch.long), torch.tensor([[0.0] * 9], dtype=torch.float)\n",
    "            \n",
    "        for atom in mol.GetAtoms():\n",
    "            atom_feat, phys_feat = self.calc_atom_features(atom)\n",
    "            atom_feats.append(atom_feat)\n",
    "            phys_feats.append(phys_feat)\n",
    "\n",
    "        x = torch.tensor(atom_feats, dtype=torch.long)\n",
    "        phys = torch.tensor(phys_feats, dtype=torch.float)\n",
    "        \n",
    "        return x, phys\n",
    "    \n",
    "    def remove_unbonded_hydrogens(mol):\n",
    "        params = Chem.RemoveHsParameters()\n",
    "        params.removeDegreeZero = True\n",
    "        mol = Chem.RemoveHs(mol, params)\n",
    "        return mol\n",
    "\n",
    "\n",
    "    def get_bond_features(self, mol: Chem.Mol) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Extract bond features with better error handling\"\"\"\n",
    "        if mol is None:\n",
    "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
    "            \n",
    "        row, col, edge_feat = [], [], []\n",
    "        \n",
    "        for bond in mol.GetBonds():\n",
    "            try:\n",
    "                start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "                \n",
    "                # Add edges in both directions\n",
    "                row += [start, end]\n",
    "                col += [end, start]\n",
    "                \n",
    "                # Bond features\n",
    "                bond_type = self.bond_list.index(bond.GetBondType())\n",
    "                bond_dir = self.bonddir_list.index(bond.GetBondDir())\n",
    "                \n",
    "                # Calculate additional properties\n",
    "                feat = [\n",
    "                    bond_type,\n",
    "                    bond_dir,\n",
    "                    int(bond.GetIsConjugated()),\n",
    "                    int(self._is_rotatable(bond)),\n",
    "                    self._get_bond_length(mol, start, end)\n",
    "                ]\n",
    "                \n",
    "                edge_feat.extend([feat, feat])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing bond: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not row:  # If no valid bonds were processed\n",
    "            return torch.tensor([[0], [0]], dtype=torch.long), torch.tensor([[0.0] * 5], dtype=torch.float)\n",
    "\n",
    "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "        edge_attr = torch.tensor(edge_feat, dtype=torch.float)\n",
    "        \n",
    "        return edge_index, edge_attr\n",
    "\n",
    "    def _is_rotatable(self, bond: Chem.Bond) -> bool:\n",
    "        \"\"\"Check if bond is rotatable\"\"\"\n",
    "        return (bond.GetBondType() == Chem.rdchem.BondType.SINGLE and \n",
    "                not bond.IsInRing() and\n",
    "                len(bond.GetBeginAtom().GetNeighbors()) > 1 and\n",
    "                len(bond.GetEndAtom().GetNeighbors()) > 1)\n",
    "\n",
    "    def _get_bond_length(self, mol: Chem.Mol, start: int, end: int) -> float:\n",
    "        \"\"\"Get bond length with error handling\"\"\"\n",
    "        try:\n",
    "            conf = mol.GetConformer()\n",
    "            if conf.Is3D():\n",
    "                return Chem.rdMolTransforms.GetBondLength(conf, start, end)\n",
    "        except:\n",
    "            pass\n",
    "        return 0.0\n",
    "\n",
    "    def process_molecule(self, smiles: str) -> Data:\n",
    "        \"\"\"Process SMILES string to graph data\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                print(f\"Invalid SMILES: {smiles}\")\n",
    "                return None  # Skip invalid molecules\n",
    "            mol = RemoveHs(mol)\n",
    "\n",
    "            # Add explicit hydrogens\n",
    "            mol = Chem.AddHs(mol, addCoords=True)\n",
    "\n",
    "            # Sanitize molecule\n",
    "            Chem.SanitizeMol(mol)\n",
    "\n",
    "            # Check if the molecule has atoms\n",
    "            if mol.GetNumAtoms() == 0:\n",
    "                print(\"Molecule has no atoms, skipping.\")\n",
    "                return None\n",
    "\n",
    "            # Generate 3D coordinates\n",
    "            if not mol.GetNumConformers():\n",
    "                status = AllChem.EmbedMolecule(mol, AllChem.ETKDG())\n",
    "                if status != 0:\n",
    "                    print(\"Failed to generate 3D conformer\")\n",
    "                    return None  # Skip failed molecules\n",
    "\n",
    "                # Try MMFF or UFF optimization\n",
    "                try:\n",
    "                    AllChem.MMFFOptimizeMolecule(mol)\n",
    "                except:\n",
    "                    AllChem.UFFOptimizeMolecule(mol)\n",
    "\n",
    "            # Extract features\n",
    "            x_cat, x_phys = self.get_atom_features(mol)\n",
    "            edge_index, edge_attr = self.get_bond_features(mol)\n",
    "\n",
    "            return Data(\n",
    "                x_cat=x_cat, \n",
    "                x_phys=x_phys,\n",
    "                edge_index=edge_index, \n",
    "                edge_attr=edge_attr,\n",
    "                num_nodes=x_cat.size(0)\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing molecule {smiles}: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a311ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryQueue:\n",
    "    \"\"\"Memory queue with temporal decay for contrastive learning\"\"\"\n",
    "    def __init__(self, size: int, dim: int, decay: float = 0.99999):\n",
    "        self.size = size\n",
    "        self.dim = dim\n",
    "        self.decay = decay\n",
    "        self.ptr = 0\n",
    "        self.full = False\n",
    "        \n",
    "        # Initialize queue\n",
    "        self.queue = nn.Parameter(F.normalize(torch.randn(size, dim), dim=1), requires_grad=False)\n",
    "        self.queue_age = nn.Parameter(torch.zeros(size), requires_grad=False)\n",
    "        \n",
    "#         self.register_buffer(\"queue\", torch.randn(size, dim))\n",
    "#         self.register_buffer(\"queue_age\", torch.zeros(size))  # Track age of each entry\n",
    "        self.queue = F.normalize(self.queue, dim=1)\n",
    "        \n",
    "    def update_queue(self, keys: torch.Tensor):\n",
    "        \"\"\"Update queue with new keys\"\"\"\n",
    "        batch_size = keys.shape[0]\n",
    "        \n",
    "        # Increment age of all entries\n",
    "        self.queue_age += 1\n",
    "        \n",
    "        # Add new keys\n",
    "        if self.ptr + batch_size <= self.size:\n",
    "            self.queue[self.ptr:self.ptr + batch_size] = keys\n",
    "            self.queue_age[self.ptr:self.ptr + batch_size] = 0\n",
    "        else:\n",
    "            # Handle overflow\n",
    "            rem = self.size - self.ptr\n",
    "            self.queue[self.ptr:] = keys[:rem]\n",
    "            self.queue[:batch_size-rem] = keys[rem:]\n",
    "            self.queue_age[self.ptr:] = 0\n",
    "            self.queue_age[:batch_size-rem] = 0\n",
    "            self.full = True\n",
    "            \n",
    "        self.ptr = (self.ptr + batch_size) % self.size\n",
    "        \n",
    "    def get_decay_weights(self) -> torch.Tensor:\n",
    "        \"\"\"Get temporal decay weights for queue entries\"\"\"\n",
    "        return self.decay ** self.queue_age\n",
    "        \n",
    "    def compute_contrastive_loss(self, query: torch.Tensor, positive_key: torch.Tensor, \n",
    "                                temperature: float = 0.07) -> torch.Tensor:\n",
    "        \"\"\"Compute contrastive loss with temporal decay\"\"\"\n",
    "        # Normalize embeddings\n",
    "        query = F.normalize(query, dim=1)\n",
    "        positive_key = F.normalize(positive_key, dim=1)\n",
    "        queue = F.normalize(self.queue, dim=1)\n",
    "        \n",
    "        # Compute logits\n",
    "        l_pos = torch.einsum('nc,nc->n', [query, positive_key]).unsqueeze(-1)\n",
    "        l_neg = torch.einsum('nc,ck->nk', [query, queue.T])\n",
    "        \n",
    "        # Apply temporal decay to negative samples\n",
    "        decay_weights = self.get_decay_weights()\n",
    "        l_neg = l_neg * decay_weights.unsqueeze(0)\n",
    "        \n",
    "        # Temperature scaling\n",
    "        logits = torch.cat([l_pos, l_neg], dim=1) / temperature\n",
    "        labels = torch.zeros(logits.shape[0], dtype=torch.long, device=query.device)\n",
    "        \n",
    "        return F.cross_entropy(logits, labels)\n",
    "\n",
    "class GraphGenerator(nn.Module):\n",
    "    \"\"\"Generator network with proper feature handling\"\"\"\n",
    "    def __init__(self, node_dim: int, edge_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Node feature processing\n",
    "        self.node_encoder = nn.Sequential(\n",
    "            nn.Linear(node_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Edge feature processing\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Importance prediction layers\n",
    "        self.node_importance = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.edge_importance = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def normalize_features(self, x_cat, x_phys):\n",
    "        \"\"\"Normalize categorical and physical features separately\"\"\"\n",
    "        # Convert categorical features to one-hot\n",
    "        x_cat = x_cat.float()\n",
    "        \n",
    "        # Normalize physical features\n",
    "        x_phys = x_phys.float()\n",
    "        if x_phys.size(0) > 1:  # Only normalize if we have more than one sample\n",
    "            x_phys = (x_phys - x_phys.mean(0)) / (x_phys.std(0) + 1e-5)\n",
    "            \n",
    "        return x_cat, x_phys\n",
    "        \n",
    "    def forward(self, data) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Normalize features\n",
    "        x_cat, x_phys = self.normalize_features(data.x_cat, data.x_phys)\n",
    "        \n",
    "        # Concatenate features\n",
    "        x = torch.cat([x_cat, x_phys], dim=-1)\n",
    "        \n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr.float()  # Ensure float type\n",
    "        \n",
    "        # Initial feature encoding\n",
    "        x = self.node_encoder(x)\n",
    "        edge_attr = self.edge_encoder(edge_attr)\n",
    "        \n",
    "        # Graph convolutions\n",
    "        x = F.relu(self.conv1(x, edge_index))  # Removed edge_attr from GCNConv\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # Predict importance scores\n",
    "        node_scores = self.node_importance(x)\n",
    "        \n",
    "        # Edge scores using both connected nodes\n",
    "        edge_features = torch.cat([\n",
    "            x[edge_index[0]], \n",
    "            x[edge_index[1]]\n",
    "        ], dim=-1)\n",
    "        edge_scores = self.edge_importance(edge_features)\n",
    "        \n",
    "        return node_scores, edge_scores\n",
    "\n",
    "def get_model_config(dataset):\n",
    "    \"\"\"Get model configuration based on dataset features\"\"\"\n",
    "    sample_data = dataset[0]\n",
    "    \n",
    "    # Calculate input dimensions\n",
    "    node_dim = sample_data.x_cat.shape[1] + sample_data.x_phys.shape[1]\n",
    "    edge_dim = sample_data.edge_attr.shape[1]\n",
    "    \n",
    "    config = GanClConfig(\n",
    "        node_dim=node_dim,\n",
    "        edge_dim=edge_dim,\n",
    "        hidden_dim=128,\n",
    "        output_dim=128,\n",
    "        queue_size=65536,\n",
    "        momentum=0.999,\n",
    "        temperature=0.07,\n",
    "        decay=0.99999,\n",
    "        dropout_ratio=0.25\n",
    "    )\n",
    "    \n",
    "    return config\n",
    "\n",
    "class GraphDiscriminator(nn.Module):\n",
    "    \"\"\"Discriminator/Encoder network\"\"\"\n",
    "    def __init__(self, node_dim: int, edge_dim: int, hidden_dim: int = 128, output_dim: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature encoding\n",
    "        self.node_encoder = nn.Sequential(\n",
    "            nn.Linear(node_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "        \n",
    "        # Projection head for contrastive learning\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def normalize_features(self, x_cat, x_phys):\n",
    "        \"\"\"Normalize categorical and physical features separately\"\"\"\n",
    "        # Convert categorical features to one-hot\n",
    "        x_cat = x_cat.float()\n",
    "        \n",
    "        # Normalize physical features\n",
    "        x_phys = x_phys.float()\n",
    "        if x_phys.size(0) > 1:  # Only normalize if we have more than one sample\n",
    "            x_phys = (x_phys - x_phys.mean(0)) / (x_phys.std(0) + 1e-5)\n",
    "            \n",
    "        return x_cat, x_phys \n",
    "        \n",
    "    def forward(self, data):\n",
    "        # Normalize features\n",
    "        x_cat, x_phys = self.normalize_features(data.x_cat, data.x_phys)\n",
    "        \n",
    "        # Concatenate features\n",
    "        x = torch.cat([x_cat, x_phys], dim=-1)\n",
    "        \n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr.float()  # Ensure float type\n",
    "        batch = data.batch\n",
    "        \n",
    "        # Initial feature encoding\n",
    "        x = self.node_encoder(x)\n",
    "        edge_attr = self.edge_encoder(edge_attr)\n",
    "        \n",
    "        # Graph convolutions\n",
    "        x = F.relu(self.conv1(x, edge_index))  # Removed edge_attr from GCNConv\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Projection\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GanClConfig:\n",
    "    \"\"\"Configuration for GAN-CL training\"\"\"\n",
    "    node_dim: int\n",
    "    edge_dim: int\n",
    "    hidden_dim: int = 128\n",
    "    output_dim: int = 128\n",
    "    queue_size: int = 65536\n",
    "    momentum: float = 0.999\n",
    "    temperature: float = 0.07\n",
    "    decay: float = 0.99999\n",
    "    dropout_ratio: float = 0.25\n",
    "\n",
    "class MolecularGANCL(nn.Module):\n",
    "    \"\"\"Combined GAN and Contrastive Learning framework\"\"\"\n",
    "    def __init__(self, config: GanClConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Add weight initialization\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.generator = GraphGenerator(\n",
    "            config.node_dim, \n",
    "            config.edge_dim, \n",
    "            config.hidden_dim * 2\n",
    "        )\n",
    "        \n",
    "        self.encoder = GraphDiscriminator(\n",
    "            config.node_dim,\n",
    "            config.edge_dim,\n",
    "            config.hidden_dim,\n",
    "            config.output_dim\n",
    "        )\n",
    "        self.encoder.apply(init_weights)\n",
    "        \n",
    "        # Modified loss weights\n",
    "        self.contrastive_weight = 1.0\n",
    "        self.adversarial_weight = 0.1  # Increased from 0.05\n",
    "        self.similarity_weight = 0.01  # Decreased from 0.1\n",
    "        \n",
    "        # Temperature annealing\n",
    "        self.initial_temperature = 0.1\n",
    "        self.min_temperature = 0.05        \n",
    "        \n",
    "        # Create momentum encoder\n",
    "        self.momentum_encoder = copy.deepcopy(self.encoder)\n",
    "        for param in self.momentum_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Initialize memory queue\n",
    "        self.memory_queue = MemoryQueue(\n",
    "            config.queue_size,\n",
    "            config.output_dim,\n",
    "            config.decay\n",
    "        )\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def _momentum_update(self):\n",
    "        \"\"\"Update momentum encoder\"\"\"\n",
    "        for param_q, param_k in zip(self.encoder.parameters(), \n",
    "                                  self.momentum_encoder.parameters()):\n",
    "            param_k.data = self.config.momentum * param_k.data + \\\n",
    "                          (1 - self.config.momentum) * param_q.data\n",
    "                          \n",
    "    def drop_graph_elements(self, data, node_scores: torch.Tensor, \n",
    "                          edge_scores: torch.Tensor) -> Data:\n",
    "        \"\"\"Apply dropout to graph based on importance scores\"\"\"\n",
    "        # Select elements to keep based on scores and dropout ratio\n",
    "#         node_mask = (node_scores < self.config.dropout_ratio).float()\n",
    "#         edge_mask = (edge_scores < self.config.dropout_ratio).float()\n",
    "\n",
    "        node_mask = (torch.rand_like(node_scores) > self.config.dropout_ratio).float()\n",
    "        edge_mask = (torch.rand_like(edge_scores) > self.config.dropout_ratio).float()\n",
    "        \n",
    "        # Apply masks\n",
    "        x_cat_new = data.x_cat * node_mask\n",
    "        x_phys_new = data.x_phys * node_mask\n",
    "        edge_attr_new = data.edge_attr * edge_mask\n",
    "        \n",
    "        # Create new graph data object\n",
    "        return Data(\n",
    "            x_cat=x_cat_new,\n",
    "            x_phys=x_phys_new,\n",
    "            edge_index=data.edge_index,\n",
    "            edge_attr=edge_attr_new,\n",
    "            batch=data.batch\n",
    "        )\n",
    "        \n",
    "    def get_temperature(self, epoch, total_epochs):\n",
    "        \"\"\"Anneal temperature during training\"\"\"\n",
    "        progress = epoch / total_epochs\n",
    "        return max(self.initial_temperature * (1 - progress), self.min_temperature)\n",
    "    \n",
    "    def forward(self, data, epoch=0, total_epochs=50):\n",
    "        # Get current temperature\n",
    "        temperature = self.get_temperature(epoch, total_epochs)\n",
    "        \n",
    "        # Get importance scores from generator\n",
    "        node_scores, edge_scores = self.generator(data)\n",
    "        \n",
    "        # Create perturbed graph\n",
    "        perturbed_data = self.drop_graph_elements(data, node_scores, edge_scores)\n",
    "        \n",
    "        # Get embeddings\n",
    "        query_emb = self.encoder(perturbed_data)\n",
    "        with torch.no_grad():\n",
    "            key_emb = self.momentum_encoder(data)\n",
    "            original_emb = self.encoder(data).detach()\n",
    "        \n",
    "        # Compute losses with modified weights\n",
    "        contrastive_loss = self.memory_queue.compute_contrastive_loss(\n",
    "            query_emb, key_emb, temperature\n",
    "        ) * self.contrastive_weight\n",
    "        \n",
    "        adversarial_loss = -F.mse_loss(query_emb, original_emb) * self.adversarial_weight\n",
    "        similarity_loss = F.mse_loss(query_emb, original_emb) * self.similarity_weight\n",
    "        \n",
    "        return contrastive_loss, adversarial_loss, similarity_loss\n",
    "    \n",
    "    def get_embeddings(self, data) -> torch.Tensor:\n",
    "        \"\"\"Get embeddings for downstream tasks\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.encoder(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb075ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_molecule_metadata(dataset):\n",
    "    \"\"\"Extract metadata from PyG graph data without relying on SMILES strings\"\"\"\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors\n",
    "    from tqdm import tqdm\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import networkx as nx\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    metadata = []\n",
    "    \n",
    "    for i, data in enumerate(tqdm(dataset, desc=\"Extracting molecule metadata\")):\n",
    "        # Set graph ID\n",
    "        mol_id = f\"molecule_{i}\"\n",
    "        \n",
    "        # Initialize empty dictionaries for metadata\n",
    "        properties = {}\n",
    "        features = {}\n",
    "        functional_groups = {}\n",
    "        ring_info = {\"ring_counts\": {}, \"ring_sizes\": {}}\n",
    "        \n",
    "        # Extract basic graph properties directly from the PyG data\n",
    "        if hasattr(data, 'num_nodes') and hasattr(data, 'edge_index'):\n",
    "            try:\n",
    "                # Convert to networkx graph for analysis\n",
    "                G = to_networkx(data)\n",
    "                \n",
    "                # Calculate graph-level properties\n",
    "                num_edges = data.edge_index.size(1) // 2  # Count unique edges\n",
    "                properties = {\n",
    "                    \"num_nodes\": data.num_nodes,\n",
    "                    \"num_edges\": num_edges,\n",
    "                    \"avg_node_degree\": 2 * num_edges / data.num_nodes if data.num_nodes > 0 else 0\n",
    "                }\n",
    "                \n",
    "                # Calculate average path length if graph is connected\n",
    "                if nx.is_connected(G):\n",
    "                    try:\n",
    "                        properties[\"avg_path_length\"] = nx.average_shortest_path_length(G)\n",
    "                    except:\n",
    "                        properties[\"avg_path_length\"] = 0.0\n",
    "                else:\n",
    "                    properties[\"avg_path_length\"] = 0.0\n",
    "                \n",
    "                # Add more sophisticated graph properties\n",
    "                try:\n",
    "                    properties[\"clustering_coefficient\"] = nx.average_clustering(G)\n",
    "                except:\n",
    "                    properties[\"clustering_coefficient\"] = 0.0\n",
    "                \n",
    "                try:\n",
    "                    properties[\"graph_diameter\"] = nx.diameter(G) if nx.is_connected(G) else 0\n",
    "                except:\n",
    "                    properties[\"graph_diameter\"] = 0\n",
    "\n",
    "                try:\n",
    "                    properties[\"assortativity\"] = nx.degree_assortativity_coefficient(G)\n",
    "                except:\n",
    "                    properties[\"assortativity\"] = 0.0\n",
    "                \n",
    "                # Graph features\n",
    "                features = {\n",
    "                    \"is_connected\": nx.is_connected(G),\n",
    "                    \"num_connected_components\": nx.number_connected_components(G),\n",
    "                    \"has_cycles\": not nx.is_tree(G),\n",
    "                    \"max_degree\": max(dict(G.degree()).values()) if G.number_of_nodes() > 0 else 0,\n",
    "                    \"density\": nx.density(G),\n",
    "                    \"is_bipartite\": nx.is_bipartite(G) if G.number_of_nodes() > 0 else False\n",
    "                }\n",
    "                \n",
    "                # Get centrality measures\n",
    "                if G.number_of_nodes() > 0:\n",
    "                    try:\n",
    "                        degree_centrality = nx.degree_centrality(G)\n",
    "                        features[\"max_centrality\"] = max(degree_centrality.values()) if degree_centrality else 0\n",
    "                        features[\"avg_centrality\"] = sum(degree_centrality.values()) / len(degree_centrality) if degree_centrality else 0\n",
    "                    except:\n",
    "                        features[\"max_centrality\"] = 0\n",
    "                        features[\"avg_centrality\"] = 0\n",
    "                else:\n",
    "                    features[\"max_centrality\"] = 0\n",
    "                    features[\"avg_centrality\"] = 0\n",
    "                \n",
    "                # Analyze node features if available\n",
    "                if hasattr(data, 'x_cat') and hasattr(data, 'x_phys'):\n",
    "                    # Atomic element distribution (from x_cat)\n",
    "                    atom_types = {}\n",
    "                    if data.x_cat.size(1) > 0:\n",
    "                        for i in range(data.num_nodes):\n",
    "                            atom_type = int(data.x_cat[i, 0].item())\n",
    "                            atom_types[atom_type] = atom_types.get(atom_type, 0) + 1\n",
    "                    \n",
    "                    features[\"atom_type_distribution\"] = atom_types\n",
    "                    \n",
    "                    # Physical property statistics (from x_phys)\n",
    "                    if data.x_phys.size(1) > 0:\n",
    "                        phys_means = data.x_phys.mean(dim=0).tolist() \n",
    "                        phys_stds = data.x_phys.std(dim=0).tolist()\n",
    "                        \n",
    "                        # Map indices to meaningful property names for the first few common properties\n",
    "                        phys_prop_names = ['contrib_mw', 'contrib_logp', 'formal_charge', \n",
    "                                        'hybridization', 'is_aromatic', 'num_h', 'valence', 'degree']\n",
    "                        \n",
    "                        for idx, name in enumerate(phys_prop_names):\n",
    "                            if idx < len(phys_means):\n",
    "                                properties[f\"avg_{name}\"] = phys_means[idx]\n",
    "                                properties[f\"std_{name}\"] = phys_stds[idx]\n",
    "                \n",
    "                # Cycle analysis\n",
    "                cycles = list(nx.cycle_basis(G))\n",
    "                cycle_count = len(cycles)\n",
    "                ring_info[\"ring_counts\"][\"total\"] = cycle_count\n",
    "                \n",
    "                # Count rings by size\n",
    "                ring_sizes = defaultdict(int)\n",
    "                for cycle in cycles:\n",
    "                    size = len(cycle)\n",
    "                    ring_sizes[str(size)] = ring_sizes.get(str(size), 0) + 1\n",
    "                \n",
    "                # Ensure we have entries for common ring sizes\n",
    "                for size in range(3, 11):\n",
    "                    if str(size) not in ring_sizes:\n",
    "                        ring_sizes[str(size)] = 0\n",
    "                \n",
    "                ring_info[\"ring_sizes\"] = dict(ring_sizes)\n",
    "                \n",
    "                # Estimate ring types\n",
    "                ring_info[\"ring_counts\"][\"single\"] = 0\n",
    "                ring_info[\"ring_counts\"][\"fused\"] = 0\n",
    "                \n",
    "                # Identify single vs fused rings by checking for shared nodes\n",
    "                if cycles:\n",
    "                    # Build a mapping of nodes to cycles they belong to\n",
    "                    node_to_cycles = defaultdict(list)\n",
    "                    for cycle_idx, cycle in enumerate(cycles):\n",
    "                        for node in cycle:\n",
    "                            node_to_cycles[node].append(cycle_idx)\n",
    "                    \n",
    "                    # Count single rings (no shared nodes with other rings)\n",
    "                    shared_cycles = set()\n",
    "                    for node, cycle_list in node_to_cycles.items():\n",
    "                        if len(cycle_list) > 1:\n",
    "                            for c in cycle_list:\n",
    "                                shared_cycles.add(c)\n",
    "                    \n",
    "                    ring_info[\"ring_counts\"][\"single\"] = cycle_count - len(shared_cycles)\n",
    "                    ring_info[\"ring_counts\"][\"fused\"] = len(shared_cycles)\n",
    "                \n",
    "                # Edge feature analysis if available\n",
    "                if hasattr(data, 'edge_attr') and data.edge_attr.size(0) > 0:\n",
    "                    # Analyze bond types (assuming first dimension is bond type)\n",
    "                    bond_types = {}\n",
    "                    for i in range(data.edge_attr.size(0)):\n",
    "                        if data.edge_attr.size(1) > 0:\n",
    "                            bond_type = int(data.edge_attr[i, 0].item())\n",
    "                            bond_types[bond_type] = bond_types.get(bond_type, 0) + 1\n",
    "                    \n",
    "                    # Divide by 2 since each bond is counted twice in undirected graph\n",
    "                    for bt in bond_types:\n",
    "                        bond_types[bt] = bond_types[bt] // 2\n",
    "                    \n",
    "                    functional_groups[\"bond_types\"] = bond_types\n",
    "                    \n",
    "                    # Count functional group proxies based on patterns in the graph\n",
    "                    # This is just an estimate since we don't have chemical information\n",
    "                    conjugated_bonds = 0\n",
    "                    for i in range(data.edge_attr.size(0)):\n",
    "                        if data.edge_attr.size(1) > 1 and data.edge_attr[i, 2].item() > 0:  # IsConjugated flag\n",
    "                            conjugated_bonds += 1\n",
    "                    \n",
    "                    functional_groups[\"conjugated_bonds\"] = conjugated_bonds // 2\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # If any error occurs during analysis, use minimal information\n",
    "                print(f\"Error analyzing graph {i}: {e}\")\n",
    "        \n",
    "        metadata.append({\n",
    "            \"graph_id\": mol_id,\n",
    "            \"properties\": properties,\n",
    "            \"features\": features,\n",
    "            \"functional_groups\": functional_groups,\n",
    "            \"ring_info\": ring_info\n",
    "        })\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def to_networkx(data):\n",
    "    \"\"\"Convert PyG data to networkx graph for analysis\"\"\"\n",
    "    import networkx as nx\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for i in range(data.num_nodes):\n",
    "        G.add_node(i)\n",
    "    \n",
    "    # Add edges (removing duplicates and self-loops)\n",
    "    edge_index = data.edge_index.cpu().numpy()\n",
    "    edges = set()\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        u, v = edge_index[0, i], edge_index[1, i]\n",
    "        if u != v and (u, v) not in edges and (v, u) not in edges:\n",
    "            G.add_edge(u, v)\n",
    "            edges.add((u, v))\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e90813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embedding_file(embeddings, molecule_indices, training_info, model_config, filepath):\n",
    "    \"\"\"Save embeddings with training metadata\"\"\"\n",
    "    data = {\n",
    "        \"embeddings\": embeddings,\n",
    "        \"molecule_indices\": molecule_indices,\n",
    "        \"training_info\": training_info,\n",
    "        \"model_config\": {k: v for k, v in model_config.__dict__.items() \n",
    "                         if not k.startswith('_') and not callable(v)}\n",
    "    }\n",
    "    \n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def save_embeddings(embeddings, labels, filepath):\n",
    "    \"\"\"Save embeddings and corresponding labels\"\"\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'embeddings': embeddings,\n",
    "            'labels': labels\n",
    "        }, f)\n",
    "\n",
    "def save_encoder(encoder, save_path, info=None):\n",
    "    \"\"\"Save encoder model for downstream tasks\"\"\"\n",
    "    save_dict = {\n",
    "        'encoder_state_dict': encoder.state_dict(),\n",
    "        'model_info': info or {}\n",
    "    }\n",
    "    torch.save(save_dict, save_path)\n",
    "\n",
    "def load_encoder(model_path, device='cpu'):\n",
    "    \"\"\"Load saved encoder model\"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    encoder = GraphDiscriminator(\n",
    "        node_dim=checkpoint['model_info'].get('node_dim'),\n",
    "        edge_dim=checkpoint['model_info'].get('edge_dim'),\n",
    "        hidden_dim=checkpoint['model_info'].get('hidden_dim', 128),\n",
    "        output_dim=checkpoint['model_info'].get('output_dim', 128)\n",
    "    )\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    return encoder        \n",
    "        \n",
    "def train_gan_cl(train_loader, config, dataset, device='cuda', \n",
    "                save_dir='./checkpoints', \n",
    "                embedding_dir='./embeddings'):\n",
    "    \"\"\"Main training function for GAN-CL with embedding storage for bias analysis\"\"\"\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(embedding_dir, exist_ok=True)\n",
    "    metadata_dir = os.path.join(embedding_dir, 'metadata')\n",
    "    os.makedirs(metadata_dir, exist_ok=True)\n",
    "    encoder_dir = os.path.join(save_dir, 'encoders')\n",
    "    os.makedirs(encoder_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract and save molecule metadata (once, before training)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    metadata = extract_molecule_metadata(dataset)\n",
    "    with open(os.path.join(metadata_dir, f'molecule_metadata_{timestamp}.pkl'), 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    \n",
    "    # Save molecule indices for consistent order\n",
    "    molecule_indices = list(range(len(metadata)))\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MolecularGANCL(config).to(device)\n",
    "    \n",
    "    # Get pre-training embeddings before any training\n",
    "    print(\"Extracting pre-training embeddings...\")\n",
    "    model.eval()\n",
    "    pre_training_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(train_loader, desc=\"Pre-training embeddings\"):\n",
    "            batch = batch.to(device)\n",
    "            embeddings = model.get_embeddings(batch)\n",
    "            pre_training_embeddings.append(embeddings.cpu())\n",
    "    \n",
    "    pre_training_embeddings = torch.cat(pre_training_embeddings, dim=0).numpy()\n",
    "    \n",
    "    # Save pre-training embeddings\n",
    "    pre_training_info = {\n",
    "        \"stage\": \"pre\",\n",
    "        \"epoch\": 0,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"loss_values\": {\"contrastive\": 0, \"adversarial\": 0, \"similarity\": 0, \"total\": 0}\n",
    "    }\n",
    "    \n",
    "    save_embedding_file(\n",
    "        pre_training_embeddings, \n",
    "        molecule_indices,\n",
    "        pre_training_info, \n",
    "        config, \n",
    "        os.path.join(embedding_dir, f'pre_training_embeddings_{timestamp}.pkl')\n",
    "    )\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Rest of your training code remains the same...\n",
    "    # Initialize optimizers\n",
    "    optimizer_encoder = torch.optim.Adam(model.encoder.parameters(), lr=3e-4)\n",
    "    optimizer_generator = torch.optim.Adam(model.generator.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Save initial model info\n",
    "    model_info = {\n",
    "        'node_dim': config.node_dim,\n",
    "        'edge_dim': config.edge_dim,\n",
    "        'hidden_dim': config.hidden_dim,\n",
    "        'output_dim': config.output_dim,\n",
    "        'training_config': config.__dict__\n",
    "    }\n",
    "    \n",
    "    # Training phases as before...\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    # Training metrics\n",
    "    metrics = {\n",
    "        'contrastive_losses': [],\n",
    "        'adversarial_losses': [],\n",
    "        'similarity_losses': [],\n",
    "        'total_losses': []\n",
    "    }\n",
    "    \n",
    "    # Training phases\n",
    "    print(\"Phase 1: Pretraining Contrastive Learning...\")\n",
    "    pretrain_epochs = 10\n",
    "    for epoch in range(pretrain_epochs):\n",
    "        contrastive_epoch_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f'Pretrain Epoch {epoch+1}/{pretrain_epochs}'):\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Forward pass (without generator)\n",
    "            query_emb = model.encoder(batch)\n",
    "            with torch.no_grad():\n",
    "                key_emb = model.momentum_encoder(batch)\n",
    "            \n",
    "            # Compute contrastive loss\n",
    "            contrastive_loss = model.memory_queue.compute_contrastive_loss(\n",
    "                query_emb, key_emb, model.config.temperature\n",
    "            )\n",
    "            \n",
    "            # Update encoder\n",
    "            optimizer_encoder.zero_grad()\n",
    "            contrastive_loss.backward()\n",
    "            optimizer_encoder.step()\n",
    "            \n",
    "            # Update momentum encoder\n",
    "            model._momentum_update()\n",
    "            \n",
    "            # Update memory queue\n",
    "            model.memory_queue.update_queue(key_emb.detach())\n",
    "            \n",
    "            contrastive_epoch_loss += contrastive_loss.item()\n",
    "            \n",
    "        avg_loss = contrastive_epoch_loss / len(train_loader)\n",
    "        metrics['contrastive_losses'].append(avg_loss)\n",
    "        print(f'Pretrain Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        # Save pretrained checkpoint\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "            }, os.path.join(save_dir, f'pretrain_checkpoint_{epoch+1}.pt'))\n",
    "    \n",
    "    print(\"\\nPhase 2: Training GAN-CL...\")\n",
    "    train_epochs = 50\n",
    "#     train_epochs = 10\n",
    "    for epoch in range(train_epochs):\n",
    "        epoch_losses = {\n",
    "            'contrastive': 0,\n",
    "            'adversarial': 0,\n",
    "            'similarity': 0,\n",
    "            'total': 0\n",
    "        }\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f'Train Epoch {epoch+1}/{train_epochs}'):\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Step 1: Train Encoder\n",
    "            optimizer_encoder.zero_grad()\n",
    "\n",
    "            # Get importance scores from generator\n",
    "            with torch.no_grad():\n",
    "                node_scores, edge_scores = model.generator(batch)\n",
    "\n",
    "            # Create perturbed graph\n",
    "            perturbed_data = model.drop_graph_elements(batch, node_scores, edge_scores)\n",
    "\n",
    "            # Get embeddings\n",
    "            query_emb = model.encoder(perturbed_data)\n",
    "            with torch.no_grad():\n",
    "                key_emb = model.momentum_encoder(batch)\n",
    "                original_emb = model.encoder(batch).detach()\n",
    "\n",
    "            # Compute losses for encoder\n",
    "            contrastive_loss = model.memory_queue.compute_contrastive_loss(\n",
    "                query_emb, key_emb, model.config.temperature\n",
    "            )\n",
    "            similarity_loss = F.mse_loss(query_emb, original_emb)\n",
    "\n",
    "            # Total loss for encoder\n",
    "            encoder_loss = contrastive_loss + 0.1 * similarity_loss\n",
    "\n",
    "            # Update encoder\n",
    "            encoder_loss.backward()\n",
    "            optimizer_encoder.step()\n",
    "\n",
    "            # Update momentum encoder\n",
    "            model._momentum_update()\n",
    "\n",
    "            # Step 2: Train Generator\n",
    "            optimizer_generator.zero_grad()\n",
    "\n",
    "            # Get new embeddings for adversarial loss\n",
    "            node_scores, edge_scores = model.generator(batch)\n",
    "            perturbed_data = model.drop_graph_elements(batch, node_scores, edge_scores)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                original_emb = model.encoder(batch)\n",
    "            perturbed_emb = model.encoder(perturbed_data)\n",
    "\n",
    "            # Compute adversarial loss\n",
    "            adversarial_loss = -F.mse_loss(perturbed_emb, original_emb)\n",
    "\n",
    "            # Update generator\n",
    "            adversarial_loss.backward()\n",
    "            optimizer_generator.step()\n",
    "\n",
    "            # Update memory queue\n",
    "            model.memory_queue.update_queue(key_emb.detach())\n",
    "\n",
    "            # Update metrics\n",
    "            epoch_losses['contrastive'] += contrastive_loss.item()\n",
    "            epoch_losses['adversarial'] += adversarial_loss.item()\n",
    "            epoch_losses['similarity'] += similarity_loss.item()\n",
    "            epoch_losses['total'] += encoder_loss.item()\n",
    "\n",
    "        # Average losses\n",
    "        for k in epoch_losses:\n",
    "            epoch_losses[k] /= len(train_loader)\n",
    "            metrics[f'{k}_losses'].append(epoch_losses[k])\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Losses: {epoch_losses}')\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
    "                'optimizer_generator_state_dict': optimizer_generator.state_dict(),\n",
    "                'losses': epoch_losses,\n",
    "            }, os.path.join(save_dir, f'gan_cl_checkpoint_{epoch+1}.pt'))            \n",
    "    \n",
    "    # Extract and save embeddings periodically\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        checkpoint_embeddings = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in train_loader:\n",
    "                batch = batch.to(device)\n",
    "                embeddings = model.get_embeddings(batch)\n",
    "                checkpoint_embeddings.append(embeddings.cpu())\n",
    "        \n",
    "        checkpoint_embeddings = torch.cat(checkpoint_embeddings, dim=0).numpy()\n",
    "        \n",
    "        # Save checkpoint embeddings with training info\n",
    "        checkpoint_info = {\n",
    "            \"stage\": f\"epoch_{epoch+1}\",\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"timestamp\": datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "            \"loss_values\": epoch_losses\n",
    "        }\n",
    "        \n",
    "        save_embedding_file(\n",
    "            checkpoint_embeddings,\n",
    "            molecule_indices,\n",
    "            checkpoint_info,\n",
    "            config,\n",
    "            os.path.join(embedding_dir, f'epoch_{epoch+1}_embeddings_{timestamp}.pkl')\n",
    "        )\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        model.train()\n",
    "    \n",
    "    # Extract and save post-training embeddings at the end\n",
    "    print(\"Extracting post-training embeddings...\")\n",
    "    model.eval()\n",
    "    post_training_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(train_loader, desc=\"Post-training embeddings\"):\n",
    "            batch = batch.to(device)\n",
    "            embeddings = model.get_embeddings(batch)\n",
    "            post_training_embeddings.append(embeddings.cpu())\n",
    "        \n",
    "    post_training_embeddings = torch.cat(post_training_embeddings, dim=0).numpy()\n",
    "    \n",
    "    # Save post-training embeddings\n",
    "    post_training_info = {\n",
    "        \"stage\": \"post\",\n",
    "        \"epoch\": train_epochs,\n",
    "        \"timestamp\": datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "        \"loss_values\": epoch_losses\n",
    "    }\n",
    "    \n",
    "    save_embedding_file(\n",
    "        post_training_embeddings,\n",
    "        molecule_indices,\n",
    "        post_training_info,\n",
    "        config,\n",
    "        os.path.join(embedding_dir, f'post_training_embeddings_{timestamp}.pkl')\n",
    "    )\n",
    "    \n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dbc0517",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data loading...\n",
      "1. Loaded dataset with 41 graphs.\n",
      "2. Failed SMILES count: 0\n",
      "3. Created DataLoader with 41 graphs\n",
      "4. Using device: cpu\n",
      "5. Starting GAN-CL training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Malli\\anaconda3\\envs\\baceenv\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "Extracting molecule metadata: 100%|| 41/41 [00:00<00:00, 154.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting pre-training embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-training embeddings: 100%|| 2/2 [00:00<00:00, 87.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1: Pretraining Contrastive Learning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 1/10: 100%|| 2/2 [00:00<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 1, Avg Loss: 1.7262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 2/10: 100%|| 2/2 [00:00<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 2, Avg Loss: 3.8650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 3/10: 100%|| 2/2 [00:00<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 3, Avg Loss: 4.3900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 4/10: 100%|| 2/2 [00:00<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 4, Avg Loss: 4.5528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 5/10: 100%|| 2/2 [00:00<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 5, Avg Loss: 4.8037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 6/10: 100%|| 2/2 [00:00<00:00,  8.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 6, Avg Loss: 4.9904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 7/10: 100%|| 2/2 [00:00<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 7, Avg Loss: 5.0852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 8/10: 100%|| 2/2 [00:00<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 8, Avg Loss: 5.2395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 9/10: 100%|| 2/2 [00:00<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 9, Avg Loss: 5.4276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 10/10: 100%|| 2/2 [00:00<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch 10, Avg Loss: 5.4803\n",
      "\n",
      "Phase 2: Training GAN-CL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/50: 100%|| 2/2 [00:00<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Losses: {'contrastive': 5.568895578384399, 'adversarial': -0.0004835611762246117, 'similarity': 0.00042209863022435457, 'total': 5.5689377784729}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2/50: 100%|| 2/2 [00:00<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Losses: {'contrastive': 5.624008893966675, 'adversarial': -0.0005867302534170449, 'similarity': 0.00031574812601320446, 'total': 5.624040365219116}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3/50: 100%|| 2/2 [00:00<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Losses: {'contrastive': 5.74617338180542, 'adversarial': -0.0004692150978371501, 'similarity': 0.0007399976602755487, 'total': 5.746247291564941}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4/50: 100%|| 2/2 [00:00<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Losses: {'contrastive': 5.879963397979736, 'adversarial': -0.0006502189789898694, 'similarity': 0.0004583373956847936, 'total': 5.880009174346924}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5/50: 100%|| 2/2 [00:00<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Losses: {'contrastive': 5.836902856826782, 'adversarial': -0.0004875833401456475, 'similarity': 0.0004612123884726316, 'total': 5.836949110031128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 6/50: 100%|| 2/2 [00:00<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Losses: {'contrastive': 5.863351345062256, 'adversarial': -0.0003791887720581144, 'similarity': 0.00043034319241996855, 'total': 5.863394260406494}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7/50: 100%|| 2/2 [00:00<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Losses: {'contrastive': 5.9450037479400635, 'adversarial': -0.00048267732199747115, 'similarity': 0.0004108881257707253, 'total': 5.945044755935669}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 8/50: 100%|| 2/2 [00:00<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Losses: {'contrastive': 6.02864933013916, 'adversarial': -0.0006302129622781649, 'similarity': 0.0004832061822526157, 'total': 6.028697729110718}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 9/50: 100%|| 2/2 [00:00<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Losses: {'contrastive': 6.006911516189575, 'adversarial': -0.0005460973479785025, 'similarity': 0.0006343293935060501, 'total': 6.006974935531616}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 10/50: 100%|| 2/2 [00:00<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Losses: {'contrastive': 6.037991762161255, 'adversarial': -0.0006658931670244783, 'similarity': 0.0005739877233281732, 'total': 6.038048982620239}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 11/50: 100%|| 2/2 [00:00<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Losses: {'contrastive': 6.04201340675354, 'adversarial': -0.0004923422675346956, 'similarity': 0.0004852450074395165, 'total': 6.042062044143677}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 12/50: 100%|| 2/2 [00:00<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Losses: {'contrastive': 6.1500372886657715, 'adversarial': -0.0006447480991482735, 'similarity': 0.00071715097874403, 'total': 6.150109052658081}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 13/50: 100%|| 2/2 [00:00<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Losses: {'contrastive': 6.1158530712127686, 'adversarial': -0.00043904643098358065, 'similarity': 0.00042245996883139014, 'total': 6.1158952713012695}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 14/50: 100%|| 2/2 [00:00<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Losses: {'contrastive': 6.2128119468688965, 'adversarial': -0.000566862232517451, 'similarity': 0.0006136484735179693, 'total': 6.212873220443726}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 15/50: 100%|| 2/2 [00:00<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Losses: {'contrastive': 6.248115062713623, 'adversarial': -0.0004796717257704586, 'similarity': 0.0005966777971480042, 'total': 6.248174667358398}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 16/50: 100%|| 2/2 [00:00<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Losses: {'contrastive': 6.238865375518799, 'adversarial': -0.0006384352454915643, 'similarity': 0.0005865262064617127, 'total': 6.238924026489258}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 17/50: 100%|| 2/2 [00:00<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Losses: {'contrastive': 6.1768434047698975, 'adversarial': -0.00042477673559915274, 'similarity': 0.0004916784673696384, 'total': 6.176892518997192}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 18/50: 100%|| 2/2 [00:00<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Losses: {'contrastive': 6.18988823890686, 'adversarial': -0.0004615634970832616, 'similarity': 0.0006931962125236169, 'total': 6.189957618713379}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 19/50: 100%|| 2/2 [00:00<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Losses: {'contrastive': 6.366357088088989, 'adversarial': -0.0007115011103451252, 'similarity': 0.0004920346837025136, 'total': 6.366406202316284}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 20/50: 100%|| 2/2 [00:00<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Losses: {'contrastive': 6.4833924770355225, 'adversarial': -0.0006950148963369429, 'similarity': 0.0006037329440005124, 'total': 6.483452796936035}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 21/50: 100%|| 2/2 [00:00<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Losses: {'contrastive': 6.4130470752716064, 'adversarial': -0.0007345461053773761, 'similarity': 0.000689105101628229, 'total': 6.413115978240967}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 22/50: 100%|| 2/2 [00:00<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Losses: {'contrastive': 6.477242708206177, 'adversarial': -0.00048466623411513865, 'similarity': 0.0005902819975744933, 'total': 6.477301597595215}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 23/50: 100%|| 2/2 [00:00<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Losses: {'contrastive': 6.439223527908325, 'adversarial': -0.0005964302690699697, 'similarity': 0.0005656966241076589, 'total': 6.439280033111572}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 24/50: 100%|| 2/2 [00:00<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Losses: {'contrastive': 6.482683181762695, 'adversarial': -0.0006926057976670563, 'similarity': 0.0005887255247216672, 'total': 6.482742071151733}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 25/50: 100%|| 2/2 [00:00<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Losses: {'contrastive': 6.585399150848389, 'adversarial': -0.0005640167219098657, 'similarity': 0.0007192011980805546, 'total': 6.585471153259277}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 26/50: 100%|| 2/2 [00:00<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Losses: {'contrastive': 6.426996231079102, 'adversarial': -0.0005145814211573452, 'similarity': 0.00048582990712020546, 'total': 6.427044868469238}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 27/50: 100%|| 2/2 [00:00<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Losses: {'contrastive': 6.567430734634399, 'adversarial': -0.0005467765731737018, 'similarity': 0.0005489958130056038, 'total': 6.567485809326172}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 28/50: 100%|| 2/2 [00:00<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Losses: {'contrastive': 6.463615417480469, 'adversarial': -0.0007383793999906629, 'similarity': 0.0006087599904276431, 'total': 6.463676452636719}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 29/50: 100%|| 2/2 [00:00<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Losses: {'contrastive': 6.605386257171631, 'adversarial': -0.0005994949897285551, 'similarity': 0.0006709489389322698, 'total': 6.605453252792358}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 30/50: 100%|| 2/2 [00:00<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Losses: {'contrastive': 6.533696413040161, 'adversarial': -0.0004880764026893303, 'similarity': 0.0005747574032284319, 'total': 6.533753871917725}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 31/50: 100%|| 2/2 [00:00<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Losses: {'contrastive': 6.7119598388671875, 'adversarial': -0.00047501017979811877, 'similarity': 0.0006712072645314038, 'total': 6.712027072906494}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 32/50: 100%|| 2/2 [00:00<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Losses: {'contrastive': 6.680181264877319, 'adversarial': -0.0005406907730503008, 'similarity': 0.0006477886636275798, 'total': 6.680245876312256}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 33/50: 100%|| 2/2 [00:00<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Losses: {'contrastive': 6.707647800445557, 'adversarial': -0.0006253420433495194, 'similarity': 0.0006330031901597977, 'total': 6.707711219787598}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 34/50: 100%|| 2/2 [00:00<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Losses: {'contrastive': 6.701786756515503, 'adversarial': -0.0004901026259176433, 'similarity': 0.0006836779357399791, 'total': 6.701855182647705}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 35/50: 100%|| 2/2 [00:00<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Losses: {'contrastive': 6.700112342834473, 'adversarial': -0.0004856068844674155, 'similarity': 0.0005898848467040807, 'total': 6.70017147064209}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 36/50: 100%|| 2/2 [00:00<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Losses: {'contrastive': 6.7739245891571045, 'adversarial': -0.0006427053303923458, 'similarity': 0.0007721323054283857, 'total': 6.774001836776733}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 37/50: 100%|| 2/2 [00:00<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Losses: {'contrastive': 6.785456418991089, 'adversarial': -0.0006853173254057765, 'similarity': 0.0006000671710353345, 'total': 6.7855165004730225}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 38/50: 100%|| 2/2 [00:00<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Losses: {'contrastive': 6.688919305801392, 'adversarial': -0.0005946753371972591, 'similarity': 0.0007388136291410774, 'total': 6.688993215560913}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 39/50: 100%|| 2/2 [00:00<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Losses: {'contrastive': 6.769747257232666, 'adversarial': -0.0005594748072326183, 'similarity': 0.000578015620703809, 'total': 6.769804954528809}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 40/50: 100%|| 2/2 [00:00<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Losses: {'contrastive': 6.856609344482422, 'adversarial': -0.0005147706979187205, 'similarity': 0.0006374001386575401, 'total': 6.856673002243042}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 41/50: 100%|| 2/2 [00:00<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Losses: {'contrastive': 6.773037433624268, 'adversarial': -0.0006841940630692989, 'similarity': 0.0006602714129257947, 'total': 6.773103475570679}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 42/50: 100%|| 2/2 [00:00<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Losses: {'contrastive': 6.76062536239624, 'adversarial': -0.00044564990093931556, 'similarity': 0.0005078965768916532, 'total': 6.760676145553589}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 43/50: 100%|| 2/2 [00:00<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, Losses: {'contrastive': 6.7661683559417725, 'adversarial': -0.0004569964512484148, 'similarity': 0.0005188503710087389, 'total': 6.766220331192017}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 44/50: 100%|| 2/2 [00:00<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, Losses: {'contrastive': 6.792860507965088, 'adversarial': -0.0004584005946526304, 'similarity': 0.0005127161566633731, 'total': 6.792911767959595}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 45/50: 100%|| 2/2 [00:00<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Losses: {'contrastive': 6.905829906463623, 'adversarial': -0.0006359552498906851, 'similarity': 0.0005696466250810772, 'total': 6.905886888504028}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 46/50: 100%|| 2/2 [00:00<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Losses: {'contrastive': 6.809175252914429, 'adversarial': -0.0005371884908527136, 'similarity': 0.0007244061562232673, 'total': 6.809247732162476}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 47/50: 100%|| 2/2 [00:00<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Losses: {'contrastive': 6.821249961853027, 'adversarial': -0.00044609814358409494, 'similarity': 0.000614759890595451, 'total': 6.8213114738464355}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 48/50: 100%|| 2/2 [00:00<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Losses: {'contrastive': 6.822723150253296, 'adversarial': -0.0006341566913761199, 'similarity': 0.0006010508514009416, 'total': 6.822783470153809}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 49/50: 100%|| 2/2 [00:00<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Losses: {'contrastive': 6.793233156204224, 'adversarial': -0.0005557907570619136, 'similarity': 0.00043278219527564943, 'total': 6.79327654838562}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 50/50: 100%|| 2/2 [00:00<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Losses: {'contrastive': 6.882603883743286, 'adversarial': -0.000535395898623392, 'similarity': 0.0007659027178306133, 'total': 6.882680654525757}\n",
      "Extracting post-training embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Post-training embeddings: 100%|| 2/2 [00:00<00:00, 171.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. Training completed!\n",
      "7. Extracting final embeddings for XAI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|| 2/2 [00:00<00:00, 143.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8. Final embeddings saved to ./embeddings/final_embeddings_20250303_112446.pkl\n",
      "9. Encoders saved in ./checkpoints/encoders/:\n",
      "   - Best encoder: best_encoder.pt\n",
      "   - Final encoder: final_encoder.pt\n",
      "   - Periodic encoders: encoder_epoch_*.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "     # Enable anomaly detection during development\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    # Your existing data loading code here\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(\"Starting data loading...\")\n",
    "    extractor = MolecularFeatureExtractor()\n",
    "    smiles_file = \"D:\\\\PhD\\\\Chapter3\\\\Unsupervised_GAN_Code\\\\pubchem-41-clean.txt\"\n",
    "#     smiles_file = \"D:\\\\PhD\\\\Chapter3\\\\Unsupervised_GAN_Code\\\\pubchem-10m-clean_test50k.txt\"\n",
    "    \n",
    "    dataset = []\n",
    "    failed_smiles = []\n",
    "    \n",
    "    with open(smiles_file, 'r') as f:\n",
    "        for line in f:\n",
    "            smiles = line.strip()\n",
    "            data = extractor.process_molecule(smiles)\n",
    "            if data is not None:\n",
    "                dataset.append(data)\n",
    "            else:\n",
    "                failed_smiles.append(smiles)\n",
    "    \n",
    "    print(f\"1. Loaded dataset with {len(dataset)} graphs.\")\n",
    "    print(f\"2. Failed SMILES count: {len(failed_smiles)}\")\n",
    "    \n",
    "    if not dataset:\n",
    "        print(\"No valid graphs generated.\")\n",
    "        return None\n",
    "        \n",
    "    # Setup training\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    print(f\"3. Created DataLoader with {len(train_loader.dataset)} graphs\")\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"4. Using device: {device}\")\n",
    "    \n",
    "    # Get configuration based on dataset\n",
    "    config = get_model_config(dataset)   \n",
    "   \n",
    "    # Train model\n",
    "    print(\"5. Starting GAN-CL training...\")\n",
    "    model, metrics = train_gan_cl(\n",
    "        train_loader, \n",
    "        config,\n",
    "        dataset,  # Pass the dataset for metadata extraction\n",
    "        device=device,\n",
    "        save_dir='./checkpoints',\n",
    "        embedding_dir='./embeddings'\n",
    "    )    \n",
    "    \n",
    "    \n",
    "    print(\"6. Training completed!\")\n",
    "    \n",
    "    # Extract embeddings for XAI\n",
    "    print(\"7. Extracting final embeddings for XAI...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_embeddings = []\n",
    "        all_graphs = []\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=\"Extracting embeddings\"):\n",
    "            batch = batch.to(device)\n",
    "            embeddings = model.get_embeddings(batch)\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_graphs.extend([data for data in batch])\n",
    "            \n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0).numpy()\n",
    "    \n",
    "    # Save final embeddings and graphs\n",
    "#     final_embedding_path = './embeddings/final_embeddings.pkl'\n",
    "    final_embedding_path = f'./embeddings/final_embeddings_{timestamp}.pkl'\n",
    "    save_embeddings(all_embeddings, all_graphs, final_embedding_path)\n",
    "    print(f\"8. Final embeddings saved to {final_embedding_path}\")\n",
    "    \n",
    "    # Print encoder locations\n",
    "    print(f\"9. Encoders saved in ./checkpoints/encoders/:\")\n",
    "    print(f\"   - Best encoder: best_encoder.pt\")\n",
    "    print(f\"   - Final encoder: final_encoder.pt\")\n",
    "    print(f\"   - Periodic encoders: encoder_epoch_*.pt\")\n",
    "    \n",
    "    return model, metrics, all_embeddings, all_graphs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, metrics, embeddings, graphs = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d928ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
