{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "612e8015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import umap\n",
    "\n",
    "class EmbeddingAnalyzer:\n",
    "    \"\"\"Analyze molecular embedding spaces for biases and representation quality\"\"\"\n",
    "    \n",
    "    def __init__(self, metadata_path, embedding_files):\n",
    "        \"\"\"\n",
    "        Initialize analyzer with specific file paths\n",
    "        \n",
    "        Args:\n",
    "            metadata_path: Path to molecule metadata pickle file\n",
    "            embedding_files: Dictionary mapping embedding types to file paths\n",
    "        \"\"\"\n",
    "        self.metadata_path = metadata_path\n",
    "        self.embedding_files = embedding_files\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(metadata_path, 'rb') as f:\n",
    "            self.metadata = pickle.load(f)\n",
    "        \n",
    "        # Initialize dictionaries to store embeddings\n",
    "        self.embedding_data = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def load_embeddings(self):\n",
    "        \"\"\"Load embedding files from provided paths\"\"\"\n",
    "        print(\"Loading embedding files...\")\n",
    "        \n",
    "        for emb_type, filepath in self.embedding_files.items():\n",
    "            if not os.path.exists(filepath):\n",
    "                print(f\"Warning: File not found: {filepath}\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                with open(filepath, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                \n",
    "                # Handle different embedding file formats\n",
    "                if 'embeddings' in data:\n",
    "                    embeddings = data['embeddings']\n",
    "                elif isinstance(data, dict) and 'labels' in data:\n",
    "                    # Format from original save_embeddings function\n",
    "                    embeddings = data['embeddings']\n",
    "                else:\n",
    "                    print(f\"Warning: Couldn't extract embeddings from {filepath}\")\n",
    "                    continue\n",
    "                \n",
    "                self.embedding_data[emb_type] = {\n",
    "                    'embeddings': embeddings,\n",
    "                    'metadata': data\n",
    "                }\n",
    "                print(f\"Loaded {emb_type} embeddings: {embeddings.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filepath}: {e}\")\n",
    "        \n",
    "        if not self.embedding_data:\n",
    "            raise ValueError(\"No embedding files loaded!\")\n",
    "    \n",
    "    def prepare_property_data(self):\n",
    "        \"\"\"Extract properties from metadata into pandas DataFrame\"\"\"\n",
    "        print(\"Preparing property data from metadata...\")\n",
    "\n",
    "        # Initialize lists to collect property data\n",
    "        property_data = []\n",
    "\n",
    "        for mol_data in self.metadata:\n",
    "            # Extract properties\n",
    "            props = mol_data['properties']\n",
    "            feats = mol_data['features']\n",
    "            funcs = mol_data['functional_groups']\n",
    "            rings = mol_data['ring_info']\n",
    "\n",
    "            # Combine all properties into a single dict\n",
    "            mol_props = {\n",
    "                'graph_id': mol_data.get('graph_id', 'unknown')\n",
    "            }\n",
    "\n",
    "            # Add molecular properties\n",
    "            for k, v in props.items():\n",
    "                if isinstance(v, (int, float, bool)):\n",
    "                    mol_props[f'prop_{k}'] = v\n",
    "\n",
    "            # Add structural features\n",
    "            for k, v in feats.items():\n",
    "                if isinstance(v, (int, float, bool)):\n",
    "                    mol_props[f'feat_{k}'] = v\n",
    "\n",
    "            # Add functional groups\n",
    "            for k, v in funcs.items():\n",
    "                if isinstance(v, (int, float, bool)):\n",
    "                    mol_props[f'func_{k}'] = v\n",
    "\n",
    "            # Add ring information\n",
    "            for category, counts in rings.items():\n",
    "                if isinstance(counts, dict):\n",
    "                    for k, v in counts.items():\n",
    "                        if isinstance(v, (int, float)):\n",
    "                            mol_props[f'ring_{category}_{k}'] = v\n",
    "\n",
    "            property_data.append(mol_props)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        self.prop_df = pd.DataFrame(property_data)\n",
    "\n",
    "        # Make sure we have some numeric columns\n",
    "        numeric_cols = self.prop_df.select_dtypes(include=np.number).columns\n",
    "        if len(numeric_cols) == 0:\n",
    "            print(\"Warning: No numeric properties found in metadata!\")\n",
    "            # Add a dummy numeric column to prevent errors\n",
    "            self.prop_df['dummy'] = 0\n",
    "\n",
    "        print(f\"Prepared DataFrame with {len(self.prop_df)} molecules and {self.prop_df.shape[1]} properties\")\n",
    "\n",
    "        # Filter out columns with too many missing values or zero variance\n",
    "        self._clean_property_dataframe()\n",
    "    \n",
    "    def _clean_property_dataframe(self):\n",
    "        \"\"\"Clean property DataFrame by removing low-information columns\"\"\"\n",
    "        # Exclude non-numeric columns from variance calculation\n",
    "        numeric_cols = self.prop_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "        # Remove columns with too many missing values\n",
    "        missing_thresh = 0.5\n",
    "        missing_cols = [col for col in numeric_cols \n",
    "                        if self.prop_df[col].isna().mean() > missing_thresh]\n",
    "\n",
    "        # Remove columns with zero variance\n",
    "        var_thresh = 0.0\n",
    "        var_cols = [col for col in numeric_cols \n",
    "                    if col not in missing_cols and self.prop_df[col].var() <= var_thresh]\n",
    "\n",
    "        # Remove identified columns\n",
    "        drop_cols = missing_cols + var_cols\n",
    "        if drop_cols:\n",
    "            self.prop_df = self.prop_df.drop(columns=drop_cols)\n",
    "            print(f\"Removed {len(drop_cols)} low-information columns\")\n",
    "\n",
    "        # Fill remaining missing values\n",
    "        numeric_cols = self.prop_df.select_dtypes(include=np.number).columns\n",
    "        self.prop_df[numeric_cols] = self.prop_df[numeric_cols].fillna(0)\n",
    "    \n",
    "    def analyze_property_prediction(self, properties=None, cv=5):\n",
    "        \"\"\"\n",
    "        Analyze how well embeddings predict molecular properties\n",
    "\n",
    "        Args:\n",
    "            properties: List of property column names to analyze (default: analyze all numeric)\n",
    "            cv: Number of cross-validation folds\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Property Prediction Analysis ===\")\n",
    "\n",
    "        # Select properties to analyze\n",
    "        if properties is None:\n",
    "            # Only consider numeric columns, and exclude graph_id\n",
    "            numeric_cols = self.prop_df.select_dtypes(include=np.number).columns\n",
    "            properties = list(numeric_cols)\n",
    "\n",
    "        if len(properties) == 0:\n",
    "            print(\"No numeric properties available for prediction analysis.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Initialize results storage\n",
    "        property_results = {'property': [], 'embedding_type': [], 'r2_score': [], 'model': []}\n",
    "\n",
    "        # Iterate through available embedding types\n",
    "        for emb_type, emb_data in self.embedding_data.items():\n",
    "            print(f\"\\nAnalyzing {emb_type} embeddings...\")\n",
    "            embeddings = emb_data['embeddings']\n",
    "\n",
    "            # Analyze each property\n",
    "            for prop in properties:\n",
    "                y = self.prop_df[prop].values\n",
    "\n",
    "                # Skip if property has too little variance\n",
    "                if np.var(y) < 1e-6:\n",
    "                    continue\n",
    "\n",
    "                # Try both linear and non-linear models\n",
    "                for model_type, model in [('Linear', Ridge(alpha=1.0)), \n",
    "                                         ('RandomForest', RandomForestRegressor(n_estimators=50))]:\n",
    "                    # Perform cross-validation\n",
    "                    try:\n",
    "                        cv_scores = cross_val_score(model, embeddings, y, \n",
    "                                                  cv=KFold(n_splits=min(cv, len(y)), shuffle=True, random_state=42), \n",
    "                                                  scoring='r2')\n",
    "\n",
    "                        # Store results\n",
    "                        mean_r2 = np.mean(cv_scores)\n",
    "                        property_results['property'].append(prop)\n",
    "                        property_results['embedding_type'].append(emb_type)\n",
    "                        property_results['r2_score'].append(mean_r2)\n",
    "                        property_results['model'].append(model_type)\n",
    "\n",
    "                        print(f\"{prop} - {model_type}: R² = {mean_r2:.4f}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error analyzing {prop}: {e}\")\n",
    "\n",
    "        # Convert to DataFrame and store\n",
    "        results_df = pd.DataFrame(property_results)\n",
    "        self.results['property_prediction'] = results_df\n",
    "\n",
    "        return results_df\n",
    "    \n",
    "    def analyze_embedding_sensitivity(self, properties=None, percentile_threshold=10):\n",
    "        \"\"\"\n",
    "        Analyze embedding sensitivity to properties\n",
    "        \n",
    "        Args:\n",
    "            properties: List of property column names to analyze (default: analyze all numeric)\n",
    "            percentile_threshold: Percentile threshold for defining similar/different molecules\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Embedding Sensitivity Analysis ===\")\n",
    "        \n",
    "        # Select properties to analyze\n",
    "        if properties is None:\n",
    "            numeric_cols = self.prop_df.select_dtypes(include=np.number).columns\n",
    "            properties = [col for col in numeric_cols if col != 'graph_id']\n",
    "        \n",
    "        # Initialize results storage\n",
    "        sensitivity_results = {\n",
    "            'property': [], \n",
    "            'embedding_type': [], \n",
    "            'avg_dist_similar': [],\n",
    "            'avg_dist_different': [],\n",
    "            'sensitivity_ratio': []\n",
    "        }\n",
    "        \n",
    "        # Iterate through available embedding types\n",
    "        for emb_type, emb_data in self.embedding_data.items():\n",
    "            print(f\"\\nAnalyzing {emb_type} embeddings...\")\n",
    "            embeddings = emb_data['embeddings']\n",
    "            \n",
    "            # Analyze each property\n",
    "            for prop in properties:\n",
    "                y = self.prop_df[prop].values\n",
    "                \n",
    "                # Skip if property has too little variance\n",
    "                if np.var(y) < 1e-6:\n",
    "                    continue\n",
    "                \n",
    "                # Compute low and high thresholds for the property\n",
    "                low_thresh = np.percentile(y, percentile_threshold)\n",
    "                high_thresh = np.percentile(y, 100 - percentile_threshold)\n",
    "                \n",
    "                if low_thresh == high_thresh:\n",
    "                    print(f\"Skipping {prop} - insufficient variance\")\n",
    "                    continue\n",
    "                \n",
    "                # Define similar and different molecule pairs\n",
    "                low_indices = np.where(y <= low_thresh)[0]\n",
    "                high_indices = np.where(y >= high_thresh)[0]\n",
    "                \n",
    "                if len(low_indices) < 5 or len(high_indices) < 5:\n",
    "                    print(f\"Skipping {prop} - insufficient samples in partition\")\n",
    "                    continue\n",
    "                \n",
    "                # Calculate distances between similar molecules\n",
    "                similar_dists = []\n",
    "                \n",
    "                # Sample pairs from low group\n",
    "                np.random.seed(42)\n",
    "                low_pairs = np.random.choice(low_indices, size=(min(1000, len(low_indices) * (len(low_indices) - 1) // 2), 2), replace=True)\n",
    "                for i, j in low_pairs:\n",
    "                    if i != j:\n",
    "                        similar_dists.append(np.linalg.norm(embeddings[i] - embeddings[j]))\n",
    "                \n",
    "                # Sample pairs from high group\n",
    "                high_pairs = np.random.choice(high_indices, size=(min(1000, len(high_indices) * (len(high_indices) - 1) // 2), 2), replace=True)\n",
    "                for i, j in high_pairs:\n",
    "                    if i != j:\n",
    "                        similar_dists.append(np.linalg.norm(embeddings[i] - embeddings[j]))\n",
    "                \n",
    "                # Calculate distances between different molecules\n",
    "                different_dists = []\n",
    "                \n",
    "                # Sample pairs between low and high groups\n",
    "                for _ in range(min(2000, len(low_indices) * len(high_indices))):\n",
    "                    i = np.random.choice(low_indices)\n",
    "                    j = np.random.choice(high_indices)\n",
    "                    different_dists.append(np.linalg.norm(embeddings[i] - embeddings[j]))\n",
    "                \n",
    "                # Compute average distances and sensitivity ratio\n",
    "                avg_similar = np.mean(similar_dists)\n",
    "                avg_different = np.mean(different_dists)\n",
    "                sensitivity_ratio = avg_different / avg_similar if avg_similar > 0 else 0\n",
    "                \n",
    "                # Store results\n",
    "                sensitivity_results['property'].append(prop)\n",
    "                sensitivity_results['embedding_type'].append(emb_type)\n",
    "                sensitivity_results['avg_dist_similar'].append(avg_similar)\n",
    "                sensitivity_results['avg_dist_different'].append(avg_different)\n",
    "                sensitivity_results['sensitivity_ratio'].append(sensitivity_ratio)\n",
    "                \n",
    "                print(f\"{prop}: Ratio = {sensitivity_ratio:.4f} (Similar: {avg_similar:.4f}, Different: {avg_different:.4f})\")\n",
    "        \n",
    "        # Convert to DataFrame and store\n",
    "        results_df = pd.DataFrame(sensitivity_results)\n",
    "        self.results['embedding_sensitivity'] = results_df\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def analyze_feature_importance(self, embedding_types=None):\n",
    "        \"\"\"\n",
    "        Analyze importance of molecular features in determining embedding structure\n",
    "        \n",
    "        Args:\n",
    "            embedding_types: List of embedding types to analyze (default: analyze all)\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Feature Importance Analysis ===\")\n",
    "        \n",
    "        # Select embedding types to analyze\n",
    "        if embedding_types is None:\n",
    "            embedding_types = list(self.embedding_data.keys())\n",
    "        \n",
    "        # Initialize results storage\n",
    "        feature_results = {'feature': [], 'embedding_type': [], 'importance': []}\n",
    "        \n",
    "        # Get feature columns (exclude graph_id and target properties)\n",
    "        feature_cols = [col for col in self.prop_df.columns if col != 'graph_id']\n",
    "        X = self.prop_df[feature_cols].values\n",
    "        \n",
    "        # Iterate through embedding types\n",
    "        for emb_type in embedding_types:\n",
    "            if emb_type not in self.embedding_data:\n",
    "                print(f\"Warning: {emb_type} not found in embeddings. Skipping.\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nAnalyzing {emb_type} embeddings...\")\n",
    "            embeddings = self.embedding_data[emb_type]['embeddings']\n",
    "                \n",
    "            # Train a RandomForest model for each embedding dimension\n",
    "            emb_dim = embeddings.shape[1]\n",
    "            importance_matrix = np.zeros((len(feature_cols), emb_dim))\n",
    "            \n",
    "            for dim in range(emb_dim):\n",
    "                # Extract target embedding dimension\n",
    "                y = embeddings[:, dim]\n",
    "                \n",
    "                # Train model\n",
    "                model = RandomForestRegressor(n_estimators=50, max_depth=6, random_state=42)\n",
    "                model.fit(X, y)\n",
    "                \n",
    "                # Store feature importances\n",
    "                importance_matrix[:, dim] = model.feature_importances_\n",
    "            \n",
    "            # Aggregate importance across dimensions (mean importance)\n",
    "            mean_importance = np.mean(importance_matrix, axis=1)\n",
    "            \n",
    "            # Store results for each feature\n",
    "            for i, feature in enumerate(feature_cols):\n",
    "                feature_results['feature'].append(feature)\n",
    "                feature_results['embedding_type'].append(emb_type)\n",
    "                feature_results['importance'].append(mean_importance[i])\n",
    "        \n",
    "        # Convert to DataFrame and store\n",
    "        results_df = pd.DataFrame(feature_results)\n",
    "        self.results['feature_importance'] = results_df\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def visualize_embeddings(self, properties=None, method='umap', save_dir='./figures'):\n",
    "        \"\"\"\n",
    "        Create visualizations of embedding spaces colored by properties\n",
    "        \n",
    "        Args:\n",
    "            properties: List of properties to color points by (default: top 4 most predictable)\n",
    "            method: Dimension reduction method ('pca', 'tsne', or 'umap')\n",
    "            save_dir: Directory to save visualization figures\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Embedding Visualization ===\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # If no properties specified, use top 4 from property prediction results\n",
    "        if properties is None and 'property_prediction' in self.results:\n",
    "            top_props = (self.results['property_prediction']\n",
    "                         .groupby('property')['r2_score']\n",
    "                         .max()\n",
    "                         .sort_values(ascending=False)\n",
    "                         .head(4)\n",
    "                         .index.tolist())\n",
    "            properties = top_props\n",
    "        elif properties is None:\n",
    "            # Use a few common properties if available\n",
    "            potential_props = ['prop_num_nodes', 'prop_avg_node_degree', \n",
    "                              'prop_clustering_coefficient', 'feat_is_connected']\n",
    "            properties = [p for p in potential_props if p in self.prop_df.columns][:4]\n",
    "        \n",
    "        print(f\"Visualizing with properties: {properties}\")\n",
    "        \n",
    "        # Iterate through embedding types\n",
    "        for emb_type, emb_data in self.embedding_data.items():\n",
    "            print(f\"\\nVisualizing {emb_type} embeddings...\")\n",
    "            embeddings = emb_data['embeddings']\n",
    "            \n",
    "            # Apply dimension reduction\n",
    "            if method == 'pca':\n",
    "                reducer = PCA(n_components=2, random_state=42)\n",
    "                emb_2d = reducer.fit_transform(embeddings)\n",
    "                method_name = 'PCA'\n",
    "            elif method == 'tsne':\n",
    "                reducer = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "                emb_2d = reducer.fit_transform(embeddings)\n",
    "                method_name = 't-SNE'\n",
    "            else:  # umap\n",
    "                reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "                emb_2d = reducer.fit_transform(embeddings)\n",
    "                method_name = 'UMAP'\n",
    "            \n",
    "            # Create DataFrame for visualization\n",
    "            vis_df = pd.DataFrame({\n",
    "                'x': emb_2d[:, 0],\n",
    "                'y': emb_2d[:, 1]\n",
    "            })\n",
    "            \n",
    "            # Create plots for each property\n",
    "            for prop in properties:\n",
    "                if prop not in self.prop_df.columns:\n",
    "                    print(f\"Warning: Property {prop} not found in data. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                # Add property values\n",
    "                vis_df['property'] = self.prop_df[prop].values\n",
    "                \n",
    "                # Create figure\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                \n",
    "                # Create scatter plot colored by property\n",
    "                if prop.startswith('feat_') and np.all(np.isin(vis_df['property'].unique(), [0, 1])):\n",
    "                    # Categorical (boolean) property\n",
    "                    sns.scatterplot(data=vis_df, x='x', y='y', hue='property', \n",
    "                                   palette='viridis', alpha=0.7)\n",
    "                else:\n",
    "                    # Continuous property\n",
    "                    plt.scatter(vis_df['x'], vis_df['y'], c=vis_df['property'], \n",
    "                               cmap='viridis', alpha=0.7)\n",
    "                    plt.colorbar(label=prop)\n",
    "                \n",
    "                # Set labels and title\n",
    "                plt.xlabel(f'{method_name} Dimension 1')\n",
    "                plt.ylabel(f'{method_name} Dimension 2')\n",
    "                plt.title(f'{emb_type} Embeddings - Colored by {prop}')\n",
    "                \n",
    "                # Save figure\n",
    "                plt.tight_layout()\n",
    "                filename = f'{emb_type}_{method}_{prop}.png'\n",
    "                plt.savefig(os.path.join(save_dir, filename), dpi=300)\n",
    "                plt.close()\n",
    "                \n",
    "        print(f\"Saved visualization figures to {save_dir}\")\n",
    "    \n",
    "    def compare_property_prediction(self):\n",
    "        \"\"\"Compare property prediction quality between pre and post training\"\"\"\n",
    "        if 'property_prediction' not in self.results:\n",
    "            print(\"Run analyze_property_prediction first!\")\n",
    "            return\n",
    "        \n",
    "        results = self.results['property_prediction']\n",
    "        \n",
    "        # Filter to just pre and post training (not intermediate epochs)\n",
    "        compare_df = results[results['embedding_type'].isin(['pre_training', 'post_training'])]\n",
    "        \n",
    "        # Pivot to get pre-post comparison\n",
    "        pivot_df = compare_df.pivot_table(\n",
    "            index=['property', 'model'], \n",
    "            columns='embedding_type', \n",
    "            values='r2_score'\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Calculate improvement\n",
    "        if 'pre_training' in pivot_df.columns and 'post_training' in pivot_df.columns:\n",
    "            pivot_df['improvement'] = pivot_df['post_training'] - pivot_df['pre_training']\n",
    "            pivot_df['relative_improvement'] = pivot_df['improvement'] / (pivot_df['pre_training'].abs() + 1e-6)\n",
    "            \n",
    "            # Sort by absolute improvement\n",
    "            pivot_df = pivot_df.sort_values('improvement', ascending=False)\n",
    "            \n",
    "            # Print results\n",
    "            print(\"\\n=== Property Prediction Comparison ===\")\n",
    "            print(pivot_df)\n",
    "            \n",
    "            # Visualize improvements\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            # Filter to just RandomForest results for better visualization\n",
    "            rf_df = pivot_df[pivot_df['model'] == 'RandomForest']\n",
    "            \n",
    "            # Get top 15 by absolute change for plotting\n",
    "            plot_df = rf_df.head(15)\n",
    "            \n",
    "            # Create bar chart\n",
    "            bars = plt.barh(plot_df['property'], plot_df['improvement'], color='skyblue')\n",
    "            \n",
    "            # Highlight negative improvements in red\n",
    "            for i, imp in enumerate(plot_df['improvement']):\n",
    "                if imp < 0:\n",
    "                    bars[i].set_color('salmon')\n",
    "            \n",
    "            plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "            plt.xlabel('Improvement in R² Score (Post - Pre)')\n",
    "            plt.title('Changes in Property Predictability After Training')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save figure\n",
    "            plt.savefig('property_prediction_improvement.png', dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            return pivot_df\n",
    "        else:\n",
    "            print(\"Missing either pre-training or post-training embeddings\")\n",
    "            return None\n",
    "    \n",
    "    def compare_feature_importance(self):\n",
    "        \"\"\"Compare feature importance between pre and post training\"\"\"\n",
    "        if 'feature_importance' not in self.results:\n",
    "            print(\"Run analyze_feature_importance first!\")\n",
    "            return\n",
    "        \n",
    "        results = self.results['feature_importance']\n",
    "        \n",
    "        # Filter to just pre and post training (not intermediate epochs)\n",
    "        compare_df = results[results['embedding_type'].isin(['pre_training', 'post_training'])]\n",
    "        \n",
    "        # Pivot to get pre-post comparison\n",
    "        pivot_df = compare_df.pivot_table(\n",
    "            index='feature', \n",
    "            columns='embedding_type', \n",
    "            values='importance'\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Calculate changes\n",
    "        if 'pre_training' in pivot_df.columns and 'post_training' in pivot_df.columns:\n",
    "            pivot_df['abs_change'] = pivot_df['post_training'] - pivot_df['pre_training']\n",
    "            pivot_df['rel_change'] = pivot_df['abs_change'] / (pivot_df['pre_training'] + 1e-6)\n",
    "            \n",
    "            # Sort by absolute change\n",
    "            pivot_df = pivot_df.sort_values('abs_change', ascending=False)\n",
    "            \n",
    "            # Print results\n",
    "            print(\"\\n=== Feature Importance Comparison ===\")\n",
    "            print(pivot_df)\n",
    "            \n",
    "            # Visualize changes\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            \n",
    "            # Get top and bottom 10 by absolute change for plotting\n",
    "            top_df = pivot_df.head(10)\n",
    "            bottom_df = pivot_df.tail(10)\n",
    "            plot_df = pd.concat([top_df, bottom_df])\n",
    "            \n",
    "            # Create bar chart\n",
    "            bars = plt.barh(plot_df['feature'], plot_df['abs_change'], color='skyblue')\n",
    "            \n",
    "            # Highlight negative changes in red\n",
    "            for i, change in enumerate(plot_df['abs_change']):\n",
    "                if change < 0:\n",
    "                    bars[i].set_color('salmon')\n",
    "            \n",
    "            plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "            plt.xlabel('Change in Feature Importance (Post - Pre)')\n",
    "            plt.title('Changes in Feature Importance After Training')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save figure\n",
    "            plt.savefig('feature_importance_change.png', dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            return pivot_df\n",
    "        else:\n",
    "            print(\"Missing either pre-training or post-training embeddings\")\n",
    "            return None\n",
    "\n",
    "    def compare_sensitivity(self):\n",
    "        \"\"\"Compare embedding sensitivity between pre and post training\"\"\"\n",
    "        if 'embedding_sensitivity' not in self.results:\n",
    "            print(\"Run analyze_embedding_sensitivity first!\")\n",
    "            return\n",
    "        \n",
    "        results = self.results['embedding_sensitivity']\n",
    "        \n",
    "        # Filter to just pre and post training (not intermediate epochs)\n",
    "        compare_df = results[results['embedding_type'].isin(['pre_training', 'post_training'])]\n",
    "        \n",
    "        # Pivot to get pre-post comparison for sensitivity ratio\n",
    "        pivot_df = compare_df.pivot_table(\n",
    "            index='property', \n",
    "            columns='embedding_type', \n",
    "            values='sensitivity_ratio'\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Calculate changes\n",
    "        if 'pre_training' in pivot_df.columns and 'post_training' in pivot_df.columns:\n",
    "            pivot_df['abs_change'] = pivot_df['post_training'] - pivot_df['pre_training']\n",
    "            pivot_df['rel_change'] = pivot_df['abs_change'] / (pivot_df['pre_training'] + 1e-6)\n",
    "            \n",
    "            # Sort by absolute change\n",
    "            pivot_df = pivot_df.sort_values('abs_change', ascending=False)\n",
    "            \n",
    "            # Print results\n",
    "            print(\"\\n=== Sensitivity Ratio Comparison ===\")\n",
    "            print(pivot_df)\n",
    "            \n",
    "            # Visualize changes\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            # Get top 15 by absolute change for plotting\n",
    "            plot_df = pivot_df.head(15)\n",
    "            \n",
    "            # Create bar chart\n",
    "            bars = plt.barh(plot_df['property'], plot_df['abs_change'], color='skyblue')\n",
    "            \n",
    "            # Highlight negative changes in red\n",
    "            for i, change in enumerate(plot_df['abs_change']):\n",
    "                if change < 0:\n",
    "                    bars[i].set_color('salmon')\n",
    "            \n",
    "            plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "            plt.xlabel('Change in Sensitivity Ratio (Post - Pre)')\n",
    "            plt.title('Changes in Embedding Sensitivity After Training')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save figure\n",
    "            plt.savefig('sensitivity_change.png', dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            return pivot_df\n",
    "        else:\n",
    "            print(\"Missing either pre-training or post-training embeddings\")\n",
    "            return None\n",
    "            \n",
    "    def run_all_analyses(self):\n",
    "        \"\"\"Run all analysis methods and generate a comprehensive report\"\"\"\n",
    "        print(\"Starting comprehensive embedding analysis...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        self.prepare_property_data()\n",
    "        \n",
    "        # Run all analyses\n",
    "        self.analyze_property_prediction()\n",
    "        self.analyze_embedding_sensitivity()\n",
    "        self.analyze_feature_importance()\n",
    "        \n",
    "        # Generate comparison reports\n",
    "        self.compare_property_prediction()\n",
    "        self.compare_feature_importance()\n",
    "        self.compare_sensitivity()\n",
    "        \n",
    "        # Create visualizations\n",
    "        self.visualize_embeddings()\n",
    "        \n",
    "        print(\"\\nAnalysis complete! Results and visualizations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4f1e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_visualizations(analyzer, output_dir='./enhanced_figures'):\n",
    "    \"\"\"Create enhanced visualizations for better analysis communication\"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # ---- 1. Property Prediction Power Chart ----\n",
    "    if 'property_prediction' in analyzer.results:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Filter to important properties and organize by category\n",
    "        pred_df = analyzer.results['property_prediction']\n",
    "        \n",
    "        # Filter to RandomForest results for better visualization\n",
    "        rf_df = pred_df[pred_df['model'] == 'RandomForest']\n",
    "        \n",
    "        # Create pivot table for pre vs post\n",
    "        pivot_df = rf_df.pivot_table(\n",
    "            index='property', \n",
    "            columns='embedding_type', \n",
    "            values='r2_score'\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add improvement column\n",
    "        if 'pre_training' in pivot_df.columns and 'post_training' in pivot_df.columns:\n",
    "            pivot_df['improvement'] = pivot_df['post_training'] - pivot_df['pre_training']\n",
    "            \n",
    "            # Sort by improvement\n",
    "            pivot_df = pivot_df.sort_values('improvement', ascending=False)\n",
    "            \n",
    "            # Create a categorical column for property type\n",
    "            def categorize_property(prop):\n",
    "                if 'mw' in prop.lower() or 'weight' in prop.lower():\n",
    "                    return 'Molecular Weight'\n",
    "                elif 'logp' in prop.lower():\n",
    "                    return 'Lipophilicity'\n",
    "                elif 'ring' in prop.lower():\n",
    "                    return 'Ring Structure'\n",
    "                elif 'aromatic' in prop.lower():\n",
    "                    return 'Aromaticity'\n",
    "                elif 'func_' in prop.lower():\n",
    "                    return 'Functional Group'\n",
    "                elif 'contrib' in prop.lower():\n",
    "                    return 'Atom Contribution'\n",
    "                else:\n",
    "                    return 'Other Properties'\n",
    "            \n",
    "            pivot_df['category'] = pivot_df['property'].apply(categorize_property)\n",
    "            \n",
    "            # Filter to top 20 properties by absolute improvement\n",
    "            plot_df = pivot_df.copy()\n",
    "            plot_df['abs_improvement'] = plot_df['improvement'].abs()\n",
    "            plot_df = plot_df.sort_values('abs_improvement', ascending=False).head(20)\n",
    "            \n",
    "            # Create plot\n",
    "            fig, ax = plt.subplots(figsize=(12, 10))\n",
    "            \n",
    "            # Plot bars\n",
    "            bar_width = 0.35\n",
    "            x = np.arange(len(plot_df))\n",
    "            \n",
    "            # Pre-training bars\n",
    "            rects1 = ax.barh(x - bar_width/2, plot_df['pre_training'], bar_width, \n",
    "                             label='Pre-training', color='skyblue', alpha=0.7)\n",
    "            \n",
    "            # Post-training bars\n",
    "            rects2 = ax.barh(x + bar_width/2, plot_df['post_training'], bar_width,\n",
    "                             label='Post-training', color='orange', alpha=0.7)\n",
    "            \n",
    "            # Add property names\n",
    "            property_labels = [p.replace('prop_', '').replace('func_', '').replace('ring_', '') \n",
    "                              for p in plot_df['property']]\n",
    "            ax.set_yticks(x)\n",
    "            ax.set_yticklabels(property_labels)\n",
    "            \n",
    "            # Add a line at R²=0\n",
    "            ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "            \n",
    "            # Add labels and title\n",
    "            ax.set_xlabel('R² Score (Higher is Better)')\n",
    "            ax.set_title('Property Prediction Power: Pre vs. Post Training', fontsize=14)\n",
    "            ax.legend()\n",
    "            \n",
    "            # Add color-coded categories (FIXED: the indexing error was here)\n",
    "            categories = plot_df['category'].unique()\n",
    "            for i, cat in enumerate(categories):\n",
    "                # Get indices in the x array where the category matches\n",
    "                idx_in_plot_df = plot_df[plot_df['category'] == cat].index\n",
    "                # Map these indices to positions in the x array\n",
    "                positions_in_x = [j for j, idx in enumerate(plot_df.index) if idx in idx_in_plot_df]\n",
    "                \n",
    "                if positions_in_x:  # If we have positions for this category\n",
    "                    min_idx = min(positions_in_x)\n",
    "                    max_idx = max(positions_in_x)\n",
    "                    ax.axhspan(min_idx - 0.5, max_idx + 0.5, alpha=0.1, color=plt.cm.tab10(i))\n",
    "                    ax.text(ax.get_xlim()[0], (min_idx + max_idx) / 2, cat, \n",
    "                            ha='left', va='center', fontsize=9, fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, 'property_prediction_power.png'), dpi=300)\n",
    "            plt.close()\n",
    "    \n",
    "    # ---- 2. Property Sensitivity Heatmap ----\n",
    "    if 'embedding_sensitivity' in analyzer.results:\n",
    "        sens_df = analyzer.results['embedding_sensitivity']\n",
    "        \n",
    "        # Pivot to get property x embedding type matrix of sensitivity ratios\n",
    "        pivot_sens = sens_df.pivot_table(\n",
    "            index='property', \n",
    "            columns='embedding_type', \n",
    "            values='sensitivity_ratio'\n",
    "        )\n",
    "        \n",
    "        # Add a delta column (percentage change)\n",
    "        if 'pre_training' in pivot_sens.columns and 'post_training' in pivot_sens.columns:\n",
    "            delta = ((pivot_sens['post_training'] - pivot_sens['pre_training']) / \n",
    "                    pivot_sens['pre_training'] * 100)\n",
    "            pivot_sens['delta_pct'] = delta\n",
    "            \n",
    "            # Sort by delta\n",
    "            pivot_sens = pivot_sens.sort_values('delta_pct', ascending=False)\n",
    "            \n",
    "            # Select top properties by absolute change percentage\n",
    "            plot_sens = pivot_sens.head(20)\n",
    "            \n",
    "            # Create heatmap\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            \n",
    "            # Format for better display\n",
    "            heatmap_df = plot_sens[['pre_training', 'post_training']].copy()\n",
    "            # Clean property names for display\n",
    "            heatmap_df.index = [idx.replace('prop_', '').replace('func_', '').replace('ring_', '') \n",
    "                              for idx in heatmap_df.index]\n",
    "            \n",
    "            # Create diverging colormap centered at 1.0\n",
    "            cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "            \n",
    "            # Plot heatmap\n",
    "            ax = sns.heatmap(heatmap_df, annot=True, fmt=\".2f\", cmap=cmap, center=1.0,\n",
    "                           linewidths=0.5, cbar_kws={\"label\": \"Sensitivity Ratio\"})\n",
    "            \n",
    "            # Add delta values\n",
    "            for i, idx in enumerate(plot_sens.index):\n",
    "                delta_val = plot_sens.loc[idx, 'delta_pct']\n",
    "                color = 'green' if delta_val > 0 else 'red'\n",
    "                plt.text(2.5, i + 0.5, f\"{delta_val:.1f}%\", \n",
    "                        ha='center', va='center', fontweight='bold', color=color)\n",
    "            \n",
    "            plt.title('Property Sensitivity: Pre vs. Post Training', fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, 'property_sensitivity_heatmap.png'), dpi=300)\n",
    "            plt.close()\n",
    "    \n",
    "    # ---- 3. Feature Importance Radar Chart ----\n",
    "    if 'feature_importance' in analyzer.results:\n",
    "        feat_df = analyzer.results['feature_importance']\n",
    "        \n",
    "        # Filter to pre and post training\n",
    "        radar_df = feat_df[feat_df['embedding_type'].isin(['pre_training', 'post_training'])]\n",
    "        \n",
    "        # Group features by category\n",
    "        def group_feature(feat):\n",
    "            if 'ring' in feat:\n",
    "                return 'Ring Structure'\n",
    "            elif 'aromatic' in feat:\n",
    "                return 'Aromaticity'\n",
    "            elif 'func_' in feat:\n",
    "                return 'Functional Group'\n",
    "            elif any(term in feat for term in ['hybridization', 'valence', 'charge']):\n",
    "                return 'Electronic Properties'\n",
    "            elif any(term in feat for term in ['path', 'diameter', 'degree']):\n",
    "                return 'Topological Properties'\n",
    "            else:\n",
    "                return 'Other'\n",
    "                \n",
    "        radar_df['category'] = radar_df['feature'].apply(group_feature)\n",
    "        \n",
    "        # Calculate average importance by category\n",
    "        radar_pivot = radar_df.pivot_table(\n",
    "            index='category',\n",
    "            columns='embedding_type',\n",
    "            values='importance',\n",
    "            aggfunc='mean'\n",
    "        ).reset_index()\n",
    "        \n",
    "        if not radar_pivot.empty and 'pre_training' in radar_pivot.columns and 'post_training' in radar_pivot.columns:\n",
    "            # Create radar chart\n",
    "            categories = radar_pivot['category']\n",
    "            pre_values = radar_pivot['pre_training']\n",
    "            post_values = radar_pivot['post_training']\n",
    "            \n",
    "            # Compute angles for the radar chart\n",
    "            N = len(categories)\n",
    "            if N > 0:  # Only proceed if we have categories\n",
    "                angles = np.linspace(0, 2*np.pi, N, endpoint=False).tolist()\n",
    "                angles += angles[:1]  # Close the loop\n",
    "                \n",
    "                # Add the values to complete the loop\n",
    "                pre_values = pre_values.tolist()\n",
    "                pre_values += pre_values[:1]\n",
    "                post_values = post_values.tolist()\n",
    "                post_values += post_values[:1]\n",
    "                \n",
    "                # Create figure\n",
    "                fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))\n",
    "                \n",
    "                # Plot pre-training values\n",
    "                ax.plot(angles, pre_values, 'o-', linewidth=2, label='Pre-training', color='blue', alpha=0.7)\n",
    "                ax.fill(angles, pre_values, alpha=0.1, color='blue')\n",
    "                \n",
    "                # Plot post-training values\n",
    "                ax.plot(angles, post_values, 'o-', linewidth=2, label='Post-training', color='orange', alpha=0.7)\n",
    "                ax.fill(angles, post_values, alpha=0.1, color='orange')\n",
    "                \n",
    "                # Set category labels\n",
    "                categories = categories.tolist()\n",
    "                categories += categories[:1]  # Complete the loop\n",
    "                ax.set_xticks(angles)\n",
    "                ax.set_xticklabels(categories, fontsize=10, fontweight='bold')\n",
    "                \n",
    "                # Add legend and title\n",
    "                ax.legend(loc='upper right')\n",
    "                plt.title('Feature Importance by Category: Pre vs. Post Training', fontsize=14)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_dir, 'feature_importance_radar.png'), dpi=300)\n",
    "                plt.close()\n",
    "    \n",
    "    # ---- 4. Ring Structure Analysis ----\n",
    "    # Extract ring-related properties\n",
    "    ring_props = [prop for prop in analyzer.prop_df.columns if 'ring' in prop.lower()]\n",
    "    \n",
    "    if ring_props and 'embedding_sensitivity' in analyzer.results:\n",
    "        # Filter sensitivity results to ring properties\n",
    "        ring_sens = analyzer.results['embedding_sensitivity']\n",
    "        ring_sens = ring_sens[ring_sens['property'].isin(ring_props)]\n",
    "        \n",
    "        # Create a comparison of ring sensitivity\n",
    "        ring_pivot = ring_sens.pivot_table(\n",
    "            index='property',\n",
    "            columns='embedding_type',\n",
    "            values='sensitivity_ratio'\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add improvement column\n",
    "        if 'pre_training' in ring_pivot.columns and 'post_training' in ring_pivot.columns:\n",
    "            ring_pivot['pct_change'] = ((ring_pivot['post_training'] - ring_pivot['pre_training']) / \n",
    "                                      ring_pivot['pre_training'] * 100)\n",
    "            \n",
    "            # Categorize by ring type\n",
    "            def ring_category(prop):\n",
    "                if 'single' in prop:\n",
    "                    return 'Single Rings'\n",
    "                elif 'fused' in prop:\n",
    "                    return 'Fused Rings'\n",
    "                elif 'spiro' in prop:\n",
    "                    return 'Spiro Rings'\n",
    "                elif 'bridged' in prop:\n",
    "                    return 'Bridged Rings'\n",
    "                elif 'sizes' in prop:\n",
    "                    # Extract ring size\n",
    "                    size = prop.split('_')[-1]\n",
    "                    return f'Ring Size {size}'\n",
    "                else:\n",
    "                    return 'General Ring Properties'\n",
    "                    \n",
    "            ring_pivot['category'] = ring_pivot['property'].apply(ring_category)\n",
    "            \n",
    "            if not ring_pivot.empty:  # Only proceed if we have data\n",
    "                # Create grouped bar chart\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                \n",
    "                # Group by category and sort\n",
    "                ring_pivot = ring_pivot.sort_values(['category', 'pct_change'])\n",
    "                \n",
    "                # Set positions and width\n",
    "                pos = np.arange(len(ring_pivot))\n",
    "                width = 0.35\n",
    "                \n",
    "                # Create bars\n",
    "                fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                \n",
    "                # Pre-training bars\n",
    "                pre_bars = ax.barh(pos - width/2, ring_pivot['pre_training'], width,\n",
    "                                  label='Pre-training', color='skyblue')\n",
    "                \n",
    "                # Post-training bars\n",
    "                post_bars = ax.barh(pos + width/2, ring_pivot['post_training'], width,\n",
    "                                   label='Post-training', color='orange')\n",
    "                \n",
    "                # Add property labels\n",
    "                ax.set_yticks(pos)\n",
    "                # Clean up property names for display\n",
    "                ax.set_yticklabels([p.replace('ring_ring_', '') for p in ring_pivot['property']])\n",
    "                \n",
    "                # Add a reference line at 1.0\n",
    "                ax.axvline(x=1.0, color='gray', linestyle='--', alpha=0.7)\n",
    "                \n",
    "                # Add labels and title\n",
    "                ax.set_xlabel('Sensitivity Ratio (Higher = Better Separation)')\n",
    "                ax.set_title('Ring Structure Sensitivity: Pre vs. Post Training', fontsize=14)\n",
    "                ax.legend()\n",
    "                \n",
    "                # Add percentage change annotations\n",
    "                for i, (_, row) in enumerate(ring_pivot.iterrows()):\n",
    "                    pct = row['pct_change']\n",
    "                    color = 'green' if pct > 0 else 'red'\n",
    "                    ax.text(max(row['pre_training'], row['post_training']) + 0.05, i, \n",
    "                           f\"{pct:.1f}%\", va='center', color=color, fontweight='bold')\n",
    "                \n",
    "                # Highlight categories with background colors\n",
    "                categories = ring_pivot['category'].unique()\n",
    "                for i, cat in enumerate(categories):\n",
    "                    # Get indices in the current plot\n",
    "                    positions = [j for j, (_, row) in enumerate(ring_pivot.iterrows()) if row['category'] == cat]\n",
    "                    \n",
    "                    if positions:  # If we have positions for this category\n",
    "                        min_pos = min(positions)\n",
    "                        max_pos = max(positions)\n",
    "                        ax.axhspan(min_pos - 0.5, max_pos + 0.5, color=plt.cm.Pastel1(i), alpha=0.3)\n",
    "                        # Add category label\n",
    "                        ax.text(ax.get_xlim()[0] - 0.1, (min_pos + max_pos) / 2, cat,\n",
    "                               ha='right', va='center', fontweight='bold', fontsize=10)\n",
    "                        \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_dir, 'ring_structure_sensitivity.png'), dpi=300)\n",
    "                plt.close()\n",
    "    \n",
    "    # ---- 5. Functional Group Impact ----\n",
    "    # Extract functional group related properties\n",
    "    func_props = [prop for prop in analyzer.prop_df.columns if 'func_' in prop]\n",
    "    \n",
    "    if func_props and 'embedding_sensitivity' in analyzer.results and 'feature_importance' in analyzer.results:\n",
    "        # Get sensitivities for functional groups\n",
    "        func_sens = analyzer.results['embedding_sensitivity']\n",
    "        func_sens = func_sens[func_sens['property'].isin(func_props)]\n",
    "        \n",
    "        # Get importance for functional groups\n",
    "        func_imp = analyzer.results['feature_importance']\n",
    "        func_imp = func_imp[func_imp['feature'].isin(func_props)]\n",
    "        \n",
    "        # Create a combined dataframe\n",
    "        func_data = []\n",
    "        \n",
    "        for prop in func_props:\n",
    "            # Get sensitivity data\n",
    "            sens_pre = func_sens[(func_sens['property'] == prop) & \n",
    "                                (func_sens['embedding_type'] == 'pre_training')]['sensitivity_ratio'].values\n",
    "            sens_post = func_sens[(func_sens['property'] == prop) & \n",
    "                                 (func_sens['embedding_type'] == 'post_training')]['sensitivity_ratio'].values\n",
    "            \n",
    "            # Get importance data\n",
    "            imp_pre = func_imp[(func_imp['feature'] == prop) & \n",
    "                              (func_imp['embedding_type'] == 'pre_training')]['importance'].values\n",
    "            imp_post = func_imp[(func_imp['feature'] == prop) & \n",
    "                               (func_imp['embedding_type'] == 'post_training')]['importance'].values\n",
    "            \n",
    "            # Only add if we have all data\n",
    "            if len(sens_pre) > 0 and len(sens_post) > 0 and len(imp_pre) > 0 and len(imp_post) > 0:\n",
    "                func_data.append({\n",
    "                    'property': prop,\n",
    "                    'sensitivity_pre': sens_pre[0],\n",
    "                    'sensitivity_post': sens_post[0],\n",
    "                    'importance_pre': imp_pre[0],\n",
    "                    'importance_post': imp_post[0],\n",
    "                    'sensitivity_change': (sens_post[0] - sens_pre[0]) / sens_pre[0] * 100,\n",
    "                    'importance_change': (imp_post[0] - imp_pre[0]) / imp_pre[0] * 100\n",
    "                })\n",
    "                \n",
    "        # Create dataframe\n",
    "        if func_data:\n",
    "            func_df = pd.DataFrame(func_data)\n",
    "            \n",
    "            if not func_df.empty:  # Only proceed if we have data\n",
    "                # Create a quad chart (importance vs sensitivity)\n",
    "                fig, ax = plt.subplots(figsize=(10, 8))\n",
    "                \n",
    "                # Clean property names\n",
    "                func_df['display_name'] = func_df['property'].str.replace('func_', '')\n",
    "                \n",
    "                # Create arrows from pre to post\n",
    "                for _, row in func_df.iterrows():\n",
    "                    ax.arrow(row['importance_pre'], row['sensitivity_pre'],\n",
    "                            row['importance_post'] - row['importance_pre'],\n",
    "                            row['sensitivity_post'] - row['sensitivity_pre'],\n",
    "                            head_width=0.01, head_length=0.02, fc='black', ec='black', length_includes_head=True)\n",
    "                    \n",
    "                    # Add property label at midpoint\n",
    "                    mid_x = (row['importance_pre'] + row['importance_post']) / 2\n",
    "                    mid_y = (row['sensitivity_pre'] + row['sensitivity_post']) / 2\n",
    "                    ax.text(mid_x, mid_y, row['display_name'], fontsize=9, \n",
    "                           ha='center', va='center', bbox=dict(facecolor='white', alpha=0.7, boxstyle='round'))\n",
    "                \n",
    "                # Add quadrant lines\n",
    "                imp_min = min(func_df[['importance_pre', 'importance_post']].values.flatten())\n",
    "                imp_max = max(func_df[['importance_pre', 'importance_post']].values.flatten())\n",
    "                sens_min = min(func_df[['sensitivity_pre', 'sensitivity_post']].values.flatten())\n",
    "                sens_max = max(func_df[['sensitivity_pre', 'sensitivity_post']].values.flatten())\n",
    "                \n",
    "                imp_mid = (imp_min + imp_max) / 2\n",
    "                sens_mid = (sens_min + sens_max) / 2\n",
    "                \n",
    "                ax.axhline(y=sens_mid, color='gray', linestyle='--', alpha=0.5)\n",
    "                ax.axvline(x=imp_mid, color='gray', linestyle='--', alpha=0.5)\n",
    "                \n",
    "                # Set axis limits with some padding\n",
    "                ax.set_xlim(imp_min - 0.01, imp_max + 0.01)\n",
    "                ax.set_ylim(sens_min - 0.05, sens_max + 0.05)\n",
    "                \n",
    "                # Add quadrant labels\n",
    "                ax.text(imp_max * 0.9, sens_max * 0.9, \n",
    "                       \"High Importance\\nHigh Sensitivity\", ha='center', va='center', \n",
    "                       bbox=dict(facecolor='lightyellow', alpha=0.7))\n",
    "                \n",
    "                ax.text(imp_min * 1.1, sens_max * 0.9, \n",
    "                       \"Low Importance\\nHigh Sensitivity\", ha='center', va='center',\n",
    "                       bbox=dict(facecolor='lightblue', alpha=0.7))\n",
    "                \n",
    "                ax.text(imp_max * 0.9, sens_min * 1.1, \n",
    "                       \"High Importance\\nLow Sensitivity\", ha='center', va='center',\n",
    "                       bbox=dict(facecolor='lightgreen', alpha=0.7))\n",
    "                \n",
    "                ax.text(imp_min * 1.1, sens_min * 1.1, \n",
    "                       \"Low Importance\\nLow Sensitivity\", ha='center', va='center',\n",
    "                       bbox=dict(facecolor='lightgray', alpha=0.7))\n",
    "                \n",
    "                # Add labels and title\n",
    "                ax.set_xlabel('Feature Importance')\n",
    "                ax.set_ylabel('Sensitivity Ratio')\n",
    "                ax.set_title('Functional Group Representation: Pre vs. Post Training', fontsize=14)\n",
    "                \n",
    "                # Add legend\n",
    "                import matplotlib.lines as mlines\n",
    "                arrow = mlines.Line2D([], [], color='black', marker='>', linestyle='-',\n",
    "                                     markersize=10, label='Pre → Post')\n",
    "                ax.legend(handles=[arrow], loc='upper left')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_dir, 'functional_group_impact.png'), dpi=300)\n",
    "                plt.close()\n",
    "    \n",
    "    print(f\"Enhanced visualizations saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a8ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_additional_visualizations(analyzer, output_dir='./enhanced_figures'):\n",
    "    \"\"\"Create additional visualizations focusing on property distribution, \n",
    "    feature correlations, functional group clustering, and ring structure mapping\"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.decomposition import PCA\n",
    "    from matplotlib.colors import Normalize\n",
    "    import networkx as nx\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    from scipy.spatial import ConvexHull\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # ---- Visualization 3: Property Distribution in Embedding Space ----\n",
    "    if any(emb_type in analyzer.embedding_data for emb_type in ['pre_training', 'post_training']):\n",
    "        # Get pre and post training embeddings\n",
    "        pre_emb = analyzer.embedding_data.get('pre_training', {}).get('embeddings')\n",
    "        post_emb = analyzer.embedding_data.get('post_training', {}).get('embeddings')\n",
    "        \n",
    "        if pre_emb is not None and post_emb is not None:\n",
    "            # Select a few key properties\n",
    "            key_properties = [\n",
    "                # Try to get molecular weight and LogP related properties\n",
    "                next((c for c in analyzer.prop_df.columns if 'mw' in c.lower()), None),\n",
    "                next((c for c in analyzer.prop_df.columns if 'logp' in c.lower()), None),\n",
    "                # Try other common properties\n",
    "                next((c for c in analyzer.prop_df.columns if 'aromatic' in c.lower()), None),\n",
    "                next((c for c in analyzer.prop_df.columns if 'ring' in c.lower()), None)\n",
    "            ]\n",
    "            \n",
    "            # Filter out None values\n",
    "            key_properties = [p for p in key_properties if p is not None]\n",
    "            \n",
    "            # If we don't have any of the target properties, select the first few numeric ones\n",
    "            if not key_properties:\n",
    "                numeric_cols = analyzer.prop_df.select_dtypes(include=np.number).columns\n",
    "                key_properties = list(numeric_cols)[:4]  # Take up to 4 properties\n",
    "            \n",
    "            if key_properties:  # Only proceed if we have properties\n",
    "                # Calculate PCA for embeddings\n",
    "                pca_pre = PCA(n_components=2)\n",
    "                pca_post = PCA(n_components=2)\n",
    "                \n",
    "                pre_2d = pca_pre.fit_transform(pre_emb)\n",
    "                post_2d = pca_post.fit_transform(post_emb)\n",
    "                \n",
    "                # Create grid of plots\n",
    "                n_props = len(key_properties)\n",
    "                fig, axes = plt.subplots(n_props, 2, figsize=(14, 4 * n_props))\n",
    "                \n",
    "                # If only one property, wrap axes in list\n",
    "                if n_props == 1:\n",
    "                    axes = [axes]\n",
    "                \n",
    "                for i, prop in enumerate(key_properties):\n",
    "                    # Get property values\n",
    "                    prop_values = analyzer.prop_df[prop].values\n",
    "                    \n",
    "                    # Create color map\n",
    "                    norm = Normalize(vmin=np.min(prop_values), vmax=np.max(prop_values))\n",
    "                    \n",
    "                    # Plot pre-training\n",
    "                    ax_pre = axes[i][0]\n",
    "                    sc_pre = ax_pre.scatter(pre_2d[:, 0], pre_2d[:, 1], \n",
    "                                         c=prop_values, cmap='viridis', \n",
    "                                         alpha=0.8, norm=norm)\n",
    "                    ax_pre.set_title(f'Pre-training: {prop}')\n",
    "                    ax_pre.set_xlabel('PC1')\n",
    "                    ax_pre.set_ylabel('PC2')\n",
    "                    \n",
    "                    # Add contour lines if we have enough points\n",
    "                    if len(pre_2d) > 10:\n",
    "                        try:\n",
    "                            x = pre_2d[:, 0]\n",
    "                            y = pre_2d[:, 1]\n",
    "                            \n",
    "                            # Try to create a 2D histogram and then contour\n",
    "                            hist, x_edges, y_edges = np.histogram2d(x, y, bins=10)\n",
    "                            x_centers = (x_edges[:-1] + x_edges[1:]) / 2\n",
    "                            y_centers = (y_edges[:-1] + y_edges[1:]) / 2\n",
    "                            \n",
    "                            X, Y = np.meshgrid(x_centers, y_centers)\n",
    "                            ax_pre.contour(X, Y, hist.T, colors='black', alpha=0.3, levels=3)\n",
    "                        except:\n",
    "                            pass  # Skip contours if they fail\n",
    "                    \n",
    "                    # Plot post-training\n",
    "                    ax_post = axes[i][1]\n",
    "                    sc_post = ax_post.scatter(post_2d[:, 0], post_2d[:, 1], \n",
    "                                           c=prop_values, cmap='viridis', \n",
    "                                           alpha=0.8, norm=norm)\n",
    "                    ax_post.set_title(f'Post-training: {prop}')\n",
    "                    ax_post.set_xlabel('PC1')\n",
    "                    ax_post.set_ylabel('PC2')\n",
    "                    \n",
    "                    # Add contour lines if we have enough points\n",
    "                    if len(post_2d) > 10:\n",
    "                        try:\n",
    "                            x = post_2d[:, 0]\n",
    "                            y = post_2d[:, 1]\n",
    "                            \n",
    "                            # Try to create a 2D histogram and then contour\n",
    "                            hist, x_edges, y_edges = np.histogram2d(x, y, bins=10)\n",
    "                            x_centers = (x_edges[:-1] + x_edges[1:]) / 2\n",
    "                            y_centers = (y_edges[:-1] + y_edges[1:]) / 2\n",
    "                            \n",
    "                            X, Y = np.meshgrid(x_centers, y_centers)\n",
    "                            ax_post.contour(X, Y, hist.T, colors='black', alpha=0.3, levels=3)\n",
    "                        except:\n",
    "                            pass  # Skip contours if they fail\n",
    "                    \n",
    "                    # Add colorbar\n",
    "                    cbar = fig.colorbar(sc_post, ax=[ax_pre, ax_post], orientation='horizontal')\n",
    "                    cbar.set_label(prop)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_dir, 'property_distribution.png'), dpi=300)\n",
    "                plt.close()\n",
    "    \n",
    "    # ---- Visualization 5: Feature Correlation Network ----\n",
    "    if 'feature_importance' in analyzer.results:\n",
    "        # Get feature importance data\n",
    "        feat_df = analyzer.results['feature_importance']\n",
    "        \n",
    "        # Filter to relevant features and embedding types\n",
    "        pre_feats = feat_df[feat_df['embedding_type'] == 'pre_training']\n",
    "        post_feats = feat_df[feat_df['embedding_type'] == 'post_training']\n",
    "        \n",
    "        if not pre_feats.empty and not post_feats.empty:\n",
    "            # Get top features by importance\n",
    "            top_n = 10  # Number of top features to include\n",
    "            \n",
    "            pre_top = pre_feats.sort_values('importance', ascending=False).head(top_n)\n",
    "            post_top = post_feats.sort_values('importance', ascending=False).head(top_n)\n",
    "            \n",
    "            # Combine unique features from both sets\n",
    "            all_features = set(pre_top['feature']).union(set(post_top['feature']))\n",
    "            \n",
    "            if analyzer.prop_df is not None and len(all_features) > 1:\n",
    "                # Calculate correlations between these features\n",
    "                feature_corr = analyzer.prop_df[[f for f in all_features if f in analyzer.prop_df.columns]].corr()\n",
    "                \n",
    "                # Create two network plots\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "                \n",
    "                # Function to create network\n",
    "                def create_correlation_network(ax, features, importances_df, title):\n",
    "                    # Create graph\n",
    "                    G = nx.Graph()\n",
    "                    \n",
    "                    # Map feature to importance\n",
    "                    feat_imp = {f: i for f, i in zip(importances_df['feature'], importances_df['importance'])}\n",
    "                    \n",
    "                    # Add nodes\n",
    "                    for feature in feature_corr.columns:\n",
    "                        if feature in feat_imp:\n",
    "                            # Scale node size by importance\n",
    "                            size = feat_imp[feature] * 3000  # Scale factor\n",
    "                            G.add_node(feature, size=size)\n",
    "                    \n",
    "                    # Add edges for correlations\n",
    "                    for i, feat1 in enumerate(feature_corr.columns):\n",
    "                        for j, feat2 in enumerate(feature_corr.columns):\n",
    "                            if i < j and feat1 in feat_imp and feat2 in feat_imp:\n",
    "                                corr = abs(feature_corr.loc[feat1, feat2])\n",
    "                                if corr > 0.3:  # Only show stronger correlations\n",
    "                                    G.add_edge(feat1, feat2, weight=corr)\n",
    "                    \n",
    "                    # Draw network\n",
    "                    if len(G.nodes) > 1:  # Only draw if we have at least 2 nodes\n",
    "                        pos = nx.spring_layout(G, seed=42)\n",
    "                        \n",
    "                        # Draw nodes\n",
    "                        node_sizes = [G.nodes[n]['size'] for n in G.nodes]\n",
    "                        nx.draw_networkx_nodes(G, pos, ax=ax, node_size=node_sizes, \n",
    "                                               node_color='skyblue', alpha=0.8)\n",
    "                        \n",
    "                        # Draw edges with weights\n",
    "                        edge_weights = [G[u][v]['weight'] * 3 for u, v in G.edges]\n",
    "                        nx.draw_networkx_edges(G, pos, ax=ax, width=edge_weights, \n",
    "                                               alpha=0.5, edge_color='gray')\n",
    "                        \n",
    "                        # Draw labels\n",
    "                        labels = {n: n.replace('prop_', '').replace('func_', '').replace('ring_', '') \n",
    "                                 for n in G.nodes}\n",
    "                        nx.draw_networkx_labels(G, pos, ax=ax, labels=labels, font_size=8)\n",
    "                        \n",
    "                        # Set title\n",
    "                        ax.set_title(title)\n",
    "                        ax.axis('off')\n",
    "                \n",
    "                # Create networks\n",
    "                create_correlation_network(ax1, pre_top, pre_top, \"Pre-training Feature Correlation Network\")\n",
    "                create_correlation_network(ax2, post_top, post_top, \"Post-training Feature Correlation Network\")\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_dir, 'feature_correlation_network.png'), dpi=300)\n",
    "                plt.close()\n",
    "    \n",
    "    # ---- Visualization 7: Functional Group Clustering ----\n",
    "    func_props = [prop for prop in analyzer.prop_df.columns if 'func_' in prop]\n",
    "    \n",
    "    if func_props and 'pre_training' in analyzer.embedding_data and 'post_training' in analyzer.embedding_data:\n",
    "        pre_emb = analyzer.embedding_data['pre_training']['embeddings']\n",
    "        post_emb = analyzer.embedding_data['post_training']['embeddings']\n",
    "        \n",
    "        # Get top 4 functional groups\n",
    "        top_func_groups = []\n",
    "        for prop in func_props:\n",
    "            # Check if the property has any True values\n",
    "            if analyzer.prop_df[prop].sum() > 0:\n",
    "                top_func_groups.append(prop)\n",
    "                if len(top_func_groups) >= 4:\n",
    "                    break\n",
    "        \n",
    "        if top_func_groups:  # Only proceed if we have functional groups\n",
    "            # Create PCA projections\n",
    "            pca_pre = PCA(n_components=2).fit_transform(pre_emb)\n",
    "            pca_post = PCA(n_components=2).fit_transform(post_emb)\n",
    "            \n",
    "            # Create grid of plots\n",
    "            fig, axes = plt.subplots(len(top_func_groups), 2, figsize=(12, 4 * len(top_func_groups)))\n",
    "            \n",
    "            # If only one functional group, wrap axes in list\n",
    "            if len(top_func_groups) == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            for i, func in enumerate(top_func_groups):\n",
    "                # Get molecules with this functional group\n",
    "                has_func = analyzer.prop_df[func] > 0\n",
    "                \n",
    "                # Plot pre-training\n",
    "                ax_pre = axes[i][0]\n",
    "                \n",
    "                # Plot all points\n",
    "                ax_pre.scatter(pca_pre[:, 0], pca_pre[:, 1], color='gray', alpha=0.3)\n",
    "                \n",
    "                # Highlight molecules with the functional group\n",
    "                if has_func.any():\n",
    "                    ax_pre.scatter(pca_pre[has_func, 0], pca_pre[has_func, 1], \n",
    "                                  color='red', label=f'Has {func}')\n",
    "                    \n",
    "                    # Try to add convex hull\n",
    "                    try:\n",
    "                        points = pca_pre[has_func]\n",
    "                        if len(points) >= 3:  # Need at least 3 points for convex hull\n",
    "                            hull = ConvexHull(points)\n",
    "                            for simplex in hull.simplices:\n",
    "                                ax_pre.plot(points[simplex, 0], points[simplex, 1], 'r-', alpha=0.5)\n",
    "                    except:\n",
    "                        pass  # Skip hull if it fails\n",
    "                \n",
    "                ax_pre.set_title(f'Pre-training: {func}')\n",
    "                ax_pre.legend()\n",
    "                \n",
    "                # Plot post-training\n",
    "                ax_post = axes[i][1]\n",
    "                \n",
    "                # Plot all points\n",
    "                ax_post.scatter(pca_post[:, 0], pca_post[:, 1], color='gray', alpha=0.3)\n",
    "                \n",
    "                # Highlight molecules with the functional group\n",
    "                if has_func.any():\n",
    "                    ax_post.scatter(pca_post[has_func, 0], pca_post[has_func, 1], \n",
    "                                   color='red', label=f'Has {func}')\n",
    "                    \n",
    "                    # Try to add convex hull\n",
    "                    try:\n",
    "                        points = pca_post[has_func]\n",
    "                        if len(points) >= 3:  # Need at least 3 points for convex hull\n",
    "                            hull = ConvexHull(points)\n",
    "                            for simplex in hull.simplices:\n",
    "                                ax_post.plot(points[simplex, 0], points[simplex, 1], 'r-', alpha=0.5)\n",
    "                    except:\n",
    "                        pass  # Skip hull if it fails\n",
    "                    \n",
    "                    # Calculate silhouette score if possible\n",
    "                    try:\n",
    "                        if sum(has_func) >= 2 and sum(~has_func) >= 2:  # Need at least 2 points in each class\n",
    "                            pre_score = silhouette_score(pca_pre, has_func)\n",
    "                            post_score = silhouette_score(pca_post, has_func)\n",
    "                            \n",
    "                            # Add silhouette scores to plot\n",
    "                            ax_pre.text(0.05, 0.95, f'Silhouette: {pre_score:.3f}', \n",
    "                                       transform=ax_pre.transAxes, fontsize=9,\n",
    "                                       verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "                            \n",
    "                            ax_post.text(0.05, 0.95, f'Silhouette: {post_score:.3f}', \n",
    "                                        transform=ax_post.transAxes, fontsize=9,\n",
    "                                        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "                    except:\n",
    "                        pass  # Skip silhouette score if it fails\n",
    "                \n",
    "                ax_post.set_title(f'Post-training: {func}')\n",
    "                ax_post.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, 'functional_group_clustering.png'), dpi=300)\n",
    "            plt.close()\n",
    "    \n",
    "    # ---- Visualization 9: Ring Structure Embedding Map ----\n",
    "    ring_props = [prop for prop in analyzer.prop_df.columns if 'ring' in prop.lower()]\n",
    "    \n",
    "    if ring_props and 'pre_training' in analyzer.embedding_data and 'post_training' in analyzer.embedding_data:\n",
    "        pre_emb = analyzer.embedding_data['pre_training']['embeddings']\n",
    "        post_emb = analyzer.embedding_data['post_training']['embeddings']\n",
    "        \n",
    "        # Calculate PCA projections\n",
    "        pca = PCA(n_components=2)\n",
    "        # Use the combined embeddings to get consistent components\n",
    "        combined_emb = np.vstack([pre_emb, post_emb])\n",
    "        pca_combined = pca.fit_transform(combined_emb)\n",
    "        \n",
    "        # Split back into pre and post\n",
    "        pre_2d = pca_combined[:len(pre_emb)]\n",
    "        post_2d = pca_combined[len(pre_emb):]\n",
    "        \n",
    "        # Categorize ring types\n",
    "        ring_categories = {\n",
    "            'single': next((p for p in ring_props if 'single' in p), None),\n",
    "            'fused': next((p for p in ring_props if 'fused' in p), None),\n",
    "            'bridged': next((p for p in ring_props if 'bridged' in p), None),\n",
    "            'spiro': next((p for p in ring_props if 'spiro' in p), None),\n",
    "            'size_5': next((p for p in ring_props if 'size' in p and '5' in p), None),\n",
    "            'size_6': next((p for p in ring_props if 'size' in p and '6' in p), None)\n",
    "        }\n",
    "        \n",
    "        # Filter out None values\n",
    "        ring_categories = {k: v for k, v in ring_categories.items() if v is not None}\n",
    "        \n",
    "        if ring_categories:  # Only proceed if we have ring categories\n",
    "            # Get total ring count if available\n",
    "            total_rings = next((p for p in ring_props if 'total' in p), None)\n",
    "            \n",
    "            # Create figure\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "            \n",
    "            # Function to plot rings\n",
    "            def plot_ring_structures(ax, embeddings_2d, title):\n",
    "                # Plot all molecules as background\n",
    "                ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], color='lightgray', alpha=0.3)\n",
    "                \n",
    "                # Plot each ring category with different marker\n",
    "                markers = ['o', 's', '^', 'D', 'p', '*']\n",
    "                colors = ['red', 'blue', 'green', 'purple', 'orange', 'cyan']\n",
    "                \n",
    "                for i, (category, prop) in enumerate(ring_categories.items()):\n",
    "                    # Get molecules with this ring type\n",
    "                    has_ring = analyzer.prop_df[prop] > 0\n",
    "                    \n",
    "                    if has_ring.any():\n",
    "                        # Size by ring count if available\n",
    "                        if total_rings is not None:\n",
    "                            # Use total ring count as size, but ensure minimum size\n",
    "                            sizes = analyzer.prop_df[total_rings].values * 20 + 30\n",
    "                        else:\n",
    "                            sizes = 50  # Default size\n",
    "                        \n",
    "                        # Plot points with this ring type\n",
    "                        ax.scatter(embeddings_2d[has_ring, 0], embeddings_2d[has_ring, 1],\n",
    "                                  marker=markers[i % len(markers)], \n",
    "                                  s=sizes if isinstance(sizes, int) else sizes[has_ring],\n",
    "                                  color=colors[i % len(colors)], \n",
    "                                  alpha=0.7, label=category)\n",
    "                        \n",
    "                        # Try to add convex hull\n",
    "                        try:\n",
    "                            points = embeddings_2d[has_ring]\n",
    "                            if len(points) >= 3:  # Need at least 3 points for convex hull\n",
    "                                hull = ConvexHull(points)\n",
    "                                for simplex in hull.simplices:\n",
    "                                    ax.plot(points[simplex, 0], points[simplex, 1], \n",
    "                                           color=colors[i % len(colors)], alpha=0.3)\n",
    "                        except:\n",
    "                            pass  # Skip hull if it fails\n",
    "                \n",
    "                ax.set_title(title)\n",
    "                ax.legend(loc='best')\n",
    "            \n",
    "            # Plot pre and post training embeddings\n",
    "            plot_ring_structures(ax1, pre_2d, \"Pre-training Ring Structures\")\n",
    "            plot_ring_structures(ax2, post_2d, \"Post-training Ring Structures\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, 'ring_structure_embedding_map.png'), dpi=300)\n",
    "            plt.close()\n",
    "    \n",
    "    print(f\"Additional visualizations saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "334856c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding files...\n",
      "Loaded pre_training embeddings: (41, 128)\n",
      "Loaded post_training embeddings: (41, 128)\n",
      "Loaded epoch_50 embeddings: (41, 128)\n",
      "Loaded final embeddings: (41, 128)\n",
      "Starting comprehensive embedding analysis...\n",
      "Preparing property data from metadata...\n",
      "Prepared DataFrame with 41 molecules and 44 properties\n",
      "Removed 7 low-information columns\n",
      "\n",
      "=== Property Prediction Analysis ===\n",
      "\n",
      "Analyzing pre_training embeddings...\n",
      "prop_num_nodes - Linear: R² = -0.0511\n",
      "prop_num_nodes - RandomForest: R² = -0.2447\n",
      "prop_num_edges - Linear: R² = -0.0568\n",
      "prop_num_edges - RandomForest: R² = -0.4844\n",
      "prop_avg_node_degree - Linear: R² = -0.2087\n",
      "prop_avg_node_degree - RandomForest: R² = -0.9227\n",
      "prop_avg_path_length - Linear: R² = -0.0835\n",
      "prop_avg_path_length - RandomForest: R² = -0.5899\n",
      "prop_clustering_coefficient - Linear: R² = -0.0244\n",
      "prop_clustering_coefficient - RandomForest: R² = -0.0881\n",
      "prop_graph_diameter - Linear: R² = -0.1670\n",
      "prop_graph_diameter - RandomForest: R² = -0.7115\n",
      "prop_assortativity - Linear: R² = -0.1946\n",
      "prop_assortativity - RandomForest: R² = -0.5153\n",
      "prop_avg_contrib_mw - Linear: R² = -0.1316\n",
      "prop_avg_contrib_mw - RandomForest: R² = -0.8783\n",
      "prop_std_contrib_mw - Linear: R² = -0.1736\n",
      "prop_std_contrib_mw - RandomForest: R² = -1.7340\n",
      "prop_avg_contrib_logp - Linear: R² = -0.1445\n",
      "prop_avg_contrib_logp - RandomForest: R² = -0.3566\n",
      "prop_std_contrib_logp - Linear: R² = -0.1186\n",
      "prop_std_contrib_logp - RandomForest: R² = -0.2993\n",
      "prop_avg_formal_charge - Linear: R² = -0.1413\n",
      "prop_avg_formal_charge - RandomForest: R² = -0.8893\n",
      "prop_std_formal_charge - Linear: R² = -0.1997\n",
      "prop_std_formal_charge - RandomForest: R² = -0.7772\n",
      "prop_avg_hybridization - Linear: R² = -0.1890\n",
      "prop_avg_hybridization - RandomForest: R² = -0.9276\n",
      "prop_std_hybridization - Linear: R² = -0.1279\n",
      "prop_std_hybridization - RandomForest: R² = -0.5733\n",
      "prop_avg_is_aromatic - Linear: R² = -0.1131\n",
      "prop_avg_is_aromatic - RandomForest: R² = -0.5333\n",
      "prop_std_is_aromatic - Linear: R² = -0.1596\n",
      "prop_std_is_aromatic - RandomForest: R² = -1.2932\n",
      "prop_avg_valence - Linear: R² = -0.1139\n",
      "prop_avg_valence - RandomForest: R² = -0.6154\n",
      "prop_std_valence - Linear: R² = -0.7803\n",
      "prop_std_valence - RandomForest: R² = -0.8009\n",
      "prop_avg_degree - Linear: R² = -0.2087\n",
      "prop_avg_degree - RandomForest: R² = -0.8863\n",
      "prop_std_degree - Linear: R² = -0.1406\n",
      "prop_std_degree - RandomForest: R² = -0.6716\n",
      "feat_density - Linear: R² = 0.0252\n",
      "feat_density - RandomForest: R² = -0.3902\n",
      "feat_max_centrality - Linear: R² = 0.0192\n",
      "feat_max_centrality - RandomForest: R² = -0.3336\n",
      "feat_avg_centrality - Linear: R² = 0.0252\n",
      "feat_avg_centrality - RandomForest: R² = -0.4249\n",
      "func_conjugated_bonds - Linear: R² = -0.2559\n",
      "func_conjugated_bonds - RandomForest: R² = -1.0213\n",
      "ring_ring_counts_total - Linear: R² = -0.1336\n",
      "ring_ring_counts_total - RandomForest: R² = -0.6327\n",
      "ring_ring_counts_single - Linear: R² = -0.0489\n",
      "ring_ring_counts_single - RandomForest: R² = -0.4716\n",
      "ring_ring_counts_fused - Linear: R² = -0.0870\n",
      "ring_ring_counts_fused - RandomForest: R² = -0.2743\n",
      "ring_ring_sizes_6 - Linear: R² = -0.1210\n",
      "ring_ring_sizes_6 - RandomForest: R² = -0.4982\n",
      "ring_ring_sizes_3 - Linear: R² = -0.0247\n",
      "ring_ring_sizes_3 - RandomForest: R² = -0.1168\n",
      "ring_ring_sizes_5 - Linear: R² = -0.1071\n",
      "ring_ring_sizes_5 - RandomForest: R² = -0.1453\n",
      "ring_ring_sizes_7 - Linear: R² = -0.0250\n",
      "ring_ring_sizes_7 - RandomForest: R² = 0.1750\n",
      "ring_ring_sizes_8 - Linear: R² = -0.0286\n",
      "ring_ring_sizes_8 - RandomForest: R² = 0.1714\n",
      "\n",
      "Analyzing post_training embeddings...\n",
      "prop_num_nodes - Linear: R² = -0.1004\n",
      "prop_num_nodes - RandomForest: R² = -1.0128\n",
      "prop_num_edges - Linear: R² = -0.1143\n",
      "prop_num_edges - RandomForest: R² = -0.7296\n",
      "prop_avg_node_degree - Linear: R² = -0.2504\n",
      "prop_avg_node_degree - RandomForest: R² = -0.2546\n",
      "prop_avg_path_length - Linear: R² = -0.0403\n",
      "prop_avg_path_length - RandomForest: R² = -0.8540\n",
      "prop_clustering_coefficient - Linear: R² = -0.0226\n",
      "prop_clustering_coefficient - RandomForest: R² = -0.1361\n",
      "prop_graph_diameter - Linear: R² = -0.0245\n",
      "prop_graph_diameter - RandomForest: R² = -0.7680\n",
      "prop_assortativity - Linear: R² = -0.1990\n",
      "prop_assortativity - RandomForest: R² = -0.4726\n",
      "prop_avg_contrib_mw - Linear: R² = -0.1380\n",
      "prop_avg_contrib_mw - RandomForest: R² = -0.5025\n",
      "prop_std_contrib_mw - Linear: R² = -0.2399\n",
      "prop_std_contrib_mw - RandomForest: R² = -0.7616\n",
      "prop_avg_contrib_logp - Linear: R² = -0.0886\n",
      "prop_avg_contrib_logp - RandomForest: R² = -0.4452\n",
      "prop_std_contrib_logp - Linear: R² = -0.1798\n",
      "prop_std_contrib_logp - RandomForest: R² = -0.6053\n",
      "prop_avg_formal_charge - Linear: R² = -0.1961\n",
      "prop_avg_formal_charge - RandomForest: R² = -0.5896\n",
      "prop_std_formal_charge - Linear: R² = -0.2390\n",
      "prop_std_formal_charge - RandomForest: R² = -0.0277\n",
      "prop_avg_hybridization - Linear: R² = -0.2120\n",
      "prop_avg_hybridization - RandomForest: R² = -0.5188\n",
      "prop_std_hybridization - Linear: R² = 0.1012\n",
      "prop_std_hybridization - RandomForest: R² = -0.0480\n",
      "prop_avg_is_aromatic - Linear: R² = 0.0149\n",
      "prop_avg_is_aromatic - RandomForest: R² = -0.3144\n",
      "prop_std_is_aromatic - Linear: R² = 0.0351\n",
      "prop_std_is_aromatic - RandomForest: R² = -0.1809\n",
      "prop_avg_valence - Linear: R² = 0.0082\n",
      "prop_avg_valence - RandomForest: R² = -0.1445\n",
      "prop_std_valence - Linear: R² = -0.6777\n",
      "prop_std_valence - RandomForest: R² = -0.8357\n",
      "prop_avg_degree - Linear: R² = -0.2504\n",
      "prop_avg_degree - RandomForest: R² = -0.2322\n",
      "prop_std_degree - Linear: R² = 0.0321\n",
      "prop_std_degree - RandomForest: R² = -0.2270\n",
      "feat_density - Linear: R² = -0.0335\n",
      "feat_density - RandomForest: R² = -0.6327\n",
      "feat_max_centrality - Linear: R² = -0.0459\n",
      "feat_max_centrality - RandomForest: R² = -0.9352\n",
      "feat_avg_centrality - Linear: R² = -0.0335\n",
      "feat_avg_centrality - RandomForest: R² = -0.8222\n",
      "func_conjugated_bonds - Linear: R² = -0.2312\n",
      "func_conjugated_bonds - RandomForest: R² = -0.2170\n",
      "ring_ring_counts_total - Linear: R² = -0.1951\n",
      "ring_ring_counts_total - RandomForest: R² = -0.3126\n",
      "ring_ring_counts_single - Linear: R² = -0.2259\n",
      "ring_ring_counts_single - RandomForest: R² = -0.7995\n",
      "ring_ring_counts_fused - Linear: R² = -0.2648\n",
      "ring_ring_counts_fused - RandomForest: R² = -0.8882\n",
      "ring_ring_sizes_6 - Linear: R² = -0.1148\n",
      "ring_ring_sizes_6 - RandomForest: R² = -0.1217\n",
      "ring_ring_sizes_3 - Linear: R² = -0.0228\n",
      "ring_ring_sizes_3 - RandomForest: R² = -0.0740\n",
      "ring_ring_sizes_5 - Linear: R² = -0.0879\n",
      "ring_ring_sizes_5 - RandomForest: R² = -0.2302\n",
      "ring_ring_sizes_7 - Linear: R² = -0.0250\n",
      "ring_ring_sizes_7 - RandomForest: R² = -0.0250\n",
      "ring_ring_sizes_8 - Linear: R² = -0.0286\n",
      "ring_ring_sizes_8 - RandomForest: R² = -0.0286\n",
      "\n",
      "Analyzing epoch_50 embeddings...\n",
      "prop_num_nodes - Linear: R² = -0.1985\n",
      "prop_num_nodes - RandomForest: R² = -0.5550\n",
      "prop_num_edges - Linear: R² = -0.2192\n",
      "prop_num_edges - RandomForest: R² = -0.4577\n",
      "prop_avg_node_degree - Linear: R² = -0.2831\n",
      "prop_avg_node_degree - RandomForest: R² = -0.3740\n",
      "prop_avg_path_length - Linear: R² = -0.1641\n",
      "prop_avg_path_length - RandomForest: R² = -0.5475\n",
      "prop_clustering_coefficient - Linear: R² = -0.0260\n",
      "prop_clustering_coefficient - RandomForest: R² = -0.0571\n",
      "prop_graph_diameter - Linear: R² = -0.1569\n",
      "prop_graph_diameter - RandomForest: R² = -0.4785\n",
      "prop_assortativity - Linear: R² = -0.3009\n",
      "prop_assortativity - RandomForest: R² = -0.6974\n",
      "prop_avg_contrib_mw - Linear: R² = -0.0082\n",
      "prop_avg_contrib_mw - RandomForest: R² = -0.3734\n",
      "prop_std_contrib_mw - Linear: R² = -0.2108\n",
      "prop_std_contrib_mw - RandomForest: R² = -1.9547\n",
      "prop_avg_contrib_logp - Linear: R² = -0.1823\n",
      "prop_avg_contrib_logp - RandomForest: R² = -0.2787\n",
      "prop_std_contrib_logp - Linear: R² = -0.1362\n",
      "prop_std_contrib_logp - RandomForest: R² = -0.3045\n",
      "prop_avg_formal_charge - Linear: R² = -0.2370\n",
      "prop_avg_formal_charge - RandomForest: R² = -0.3014\n",
      "prop_std_formal_charge - Linear: R² = -0.2433\n",
      "prop_std_formal_charge - RandomForest: R² = -0.2195\n",
      "prop_avg_hybridization - Linear: R² = -0.0982\n",
      "prop_avg_hybridization - RandomForest: R² = -0.3109\n",
      "prop_std_hybridization - Linear: R² = -0.0444\n",
      "prop_std_hybridization - RandomForest: R² = -0.3464\n",
      "prop_avg_is_aromatic - Linear: R² = -0.0783\n",
      "prop_avg_is_aromatic - RandomForest: R² = -0.4106\n",
      "prop_std_is_aromatic - Linear: R² = -0.1109\n",
      "prop_std_is_aromatic - RandomForest: R² = -0.7312\n",
      "prop_avg_valence - Linear: R² = -0.0638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prop_avg_valence - RandomForest: R² = -0.2457\n",
      "prop_std_valence - Linear: R² = -0.9572\n",
      "prop_std_valence - RandomForest: R² = -1.4485\n",
      "prop_avg_degree - Linear: R² = -0.2831\n",
      "prop_avg_degree - RandomForest: R² = -0.5675\n",
      "prop_std_degree - Linear: R² = -0.0854\n",
      "prop_std_degree - RandomForest: R² = -0.4401\n",
      "feat_density - Linear: R² = -0.0992\n",
      "feat_density - RandomForest: R² = -0.2238\n",
      "feat_max_centrality - Linear: R² = -0.1254\n",
      "feat_max_centrality - RandomForest: R² = -0.1497\n",
      "feat_avg_centrality - Linear: R² = -0.0992\n",
      "feat_avg_centrality - RandomForest: R² = -0.3090\n",
      "func_conjugated_bonds - Linear: R² = -0.3013\n",
      "func_conjugated_bonds - RandomForest: R² = -0.7953\n",
      "ring_ring_counts_total - Linear: R² = -0.2567\n",
      "ring_ring_counts_total - RandomForest: R² = -0.2485\n",
      "ring_ring_counts_single - Linear: R² = -0.2149\n",
      "ring_ring_counts_single - RandomForest: R² = -0.3441\n",
      "ring_ring_counts_fused - Linear: R² = -0.2166\n",
      "ring_ring_counts_fused - RandomForest: R² = -0.4065\n",
      "ring_ring_sizes_6 - Linear: R² = -0.2334\n",
      "ring_ring_sizes_6 - RandomForest: R² = -0.3630\n",
      "ring_ring_sizes_3 - Linear: R² = -0.0261\n",
      "ring_ring_sizes_3 - RandomForest: R² = -0.0673\n",
      "ring_ring_sizes_5 - Linear: R² = -0.3614\n",
      "ring_ring_sizes_5 - RandomForest: R² = -0.4921\n",
      "ring_ring_sizes_7 - Linear: R² = -0.0250\n",
      "ring_ring_sizes_7 - RandomForest: R² = -0.0250\n",
      "ring_ring_sizes_8 - Linear: R² = -0.0286\n",
      "ring_ring_sizes_8 - RandomForest: R² = -0.0286\n",
      "\n",
      "Analyzing final embeddings...\n",
      "prop_num_nodes - Linear: R² = -0.0911\n",
      "prop_num_nodes - RandomForest: R² = -0.2122\n",
      "prop_num_edges - Linear: R² = -0.1195\n",
      "prop_num_edges - RandomForest: R² = -0.2968\n",
      "prop_avg_node_degree - Linear: R² = -0.3156\n",
      "prop_avg_node_degree - RandomForest: R² = -0.7013\n",
      "prop_avg_path_length - Linear: R² = -0.2745\n",
      "prop_avg_path_length - RandomForest: R² = -0.6817\n",
      "prop_clustering_coefficient - Linear: R² = -0.0977\n",
      "prop_clustering_coefficient - RandomForest: R² = -0.0717\n",
      "prop_graph_diameter - Linear: R² = -0.3018\n",
      "prop_graph_diameter - RandomForest: R² = -0.5531\n",
      "prop_assortativity - Linear: R² = -0.2593\n",
      "prop_assortativity - RandomForest: R² = -0.3287\n",
      "prop_avg_contrib_mw - Linear: R² = -0.1315\n",
      "prop_avg_contrib_mw - RandomForest: R² = -0.4551\n",
      "prop_std_contrib_mw - Linear: R² = -0.1243\n",
      "prop_std_contrib_mw - RandomForest: R² = -0.8547\n",
      "prop_avg_contrib_logp - Linear: R² = -0.1905\n",
      "prop_avg_contrib_logp - RandomForest: R² = -0.1444\n",
      "prop_std_contrib_logp - Linear: R² = -0.0732\n",
      "prop_std_contrib_logp - RandomForest: R² = -0.4383\n",
      "prop_avg_formal_charge - Linear: R² = -0.1366\n",
      "prop_avg_formal_charge - RandomForest: R² = -0.4465\n",
      "prop_std_formal_charge - Linear: R² = 0.1129\n",
      "prop_std_formal_charge - RandomForest: R² = -0.0500\n",
      "prop_avg_hybridization - Linear: R² = -0.1802\n",
      "prop_avg_hybridization - RandomForest: R² = -0.5284\n",
      "prop_std_hybridization - Linear: R² = -0.1327\n",
      "prop_std_hybridization - RandomForest: R² = -0.3900\n",
      "prop_avg_is_aromatic - Linear: R² = -0.1124\n",
      "prop_avg_is_aromatic - RandomForest: R² = -0.3264\n",
      "prop_std_is_aromatic - Linear: R² = -0.2531\n",
      "prop_std_is_aromatic - RandomForest: R² = -2.7936\n",
      "prop_avg_valence - Linear: R² = -0.1064\n",
      "prop_avg_valence - RandomForest: R² = -0.4613\n",
      "prop_std_valence - Linear: R² = -0.6796\n",
      "prop_std_valence - RandomForest: R² = -0.7548\n",
      "prop_avg_degree - Linear: R² = -0.3156\n",
      "prop_avg_degree - RandomForest: R² = -0.5730\n",
      "prop_std_degree - Linear: R² = -0.1136\n",
      "prop_std_degree - RandomForest: R² = -0.4427\n",
      "feat_density - Linear: R² = -0.1363\n",
      "feat_density - RandomForest: R² = -0.3258\n",
      "feat_max_centrality - Linear: R² = -0.1558\n",
      "feat_max_centrality - RandomForest: R² = -0.3794\n",
      "feat_avg_centrality - Linear: R² = -0.1363\n",
      "feat_avg_centrality - RandomForest: R² = -0.2823\n",
      "func_conjugated_bonds - Linear: R² = -0.3337\n",
      "func_conjugated_bonds - RandomForest: R² = -0.9383\n",
      "ring_ring_counts_total - Linear: R² = -0.3402\n",
      "ring_ring_counts_total - RandomForest: R² = -0.5521\n",
      "ring_ring_counts_single - Linear: R² = -0.1712\n",
      "ring_ring_counts_single - RandomForest: R² = -0.8163\n",
      "ring_ring_counts_fused - Linear: R² = -0.1188\n",
      "ring_ring_counts_fused - RandomForest: R² = -0.3382\n",
      "ring_ring_sizes_6 - Linear: R² = -0.1441\n",
      "ring_ring_sizes_6 - RandomForest: R² = -0.3827\n",
      "ring_ring_sizes_3 - Linear: R² = -0.0969\n",
      "ring_ring_sizes_3 - RandomForest: R² = -0.0668\n",
      "ring_ring_sizes_5 - Linear: R² = -0.2068\n",
      "ring_ring_sizes_5 - RandomForest: R² = -0.1399\n",
      "ring_ring_sizes_7 - Linear: R² = -0.0250\n",
      "ring_ring_sizes_7 - RandomForest: R² = -0.0250\n",
      "ring_ring_sizes_8 - Linear: R² = -0.0286\n",
      "ring_ring_sizes_8 - RandomForest: R² = -0.0286\n",
      "\n",
      "=== Embedding Sensitivity Analysis ===\n",
      "\n",
      "Analyzing pre_training embeddings...\n",
      "prop_num_nodes: Ratio = 1.2964 (Similar: 0.2547, Different: 0.3302)\n",
      "prop_num_edges: Ratio = 1.0980 (Similar: 0.2758, Different: 0.3028)\n",
      "prop_avg_node_degree: Ratio = 0.9424 (Similar: 0.3143, Different: 0.2962)\n",
      "prop_avg_path_length: Ratio = 1.6351 (Similar: 0.2355, Different: 0.3850)\n",
      "Skipping prop_clustering_coefficient - insufficient variance\n",
      "prop_graph_diameter: Ratio = 0.9585 (Similar: 0.2684, Different: 0.2573)\n",
      "prop_assortativity: Ratio = 0.7249 (Similar: 0.3449, Different: 0.2500)\n",
      "prop_avg_contrib_mw: Ratio = 1.0692 (Similar: 0.3179, Different: 0.3399)\n",
      "prop_std_contrib_mw: Ratio = 1.0241 (Similar: 0.2834, Different: 0.2903)\n",
      "prop_avg_contrib_logp: Ratio = 1.1744 (Similar: 0.2836, Different: 0.3331)\n",
      "prop_std_contrib_logp: Ratio = 1.3195 (Similar: 0.2239, Different: 0.2954)\n",
      "prop_avg_formal_charge: Ratio = 0.9579 (Similar: 0.2785, Different: 0.2668)\n",
      "prop_std_formal_charge: Ratio = 0.9411 (Similar: 0.2930, Different: 0.2758)\n",
      "prop_avg_hybridization: Ratio = 0.9954 (Similar: 0.2793, Different: 0.2780)\n",
      "prop_std_hybridization: Ratio = 0.9758 (Similar: 0.2652, Different: 0.2588)\n",
      "prop_avg_is_aromatic: Ratio = 0.9187 (Similar: 0.3111, Different: 0.2858)\n",
      "prop_std_is_aromatic: Ratio = 1.0492 (Similar: 0.2837, Different: 0.2977)\n",
      "prop_avg_valence: Ratio = 1.0954 (Similar: 0.2883, Different: 0.3158)\n",
      "prop_std_valence: Ratio = 1.0597 (Similar: 0.2297, Different: 0.2434)\n",
      "prop_avg_degree: Ratio = 0.9424 (Similar: 0.3143, Different: 0.2962)\n",
      "prop_std_degree: Ratio = 1.0077 (Similar: 0.2791, Different: 0.2813)\n",
      "feat_density: Ratio = 1.0021 (Similar: 0.3079, Different: 0.3085)\n",
      "feat_max_centrality: Ratio = 1.2412 (Similar: 0.2540, Different: 0.3153)\n",
      "feat_avg_centrality: Ratio = 1.0021 (Similar: 0.3079, Different: 0.3085)\n",
      "func_conjugated_bonds: Ratio = 1.3729 (Similar: 0.2820, Different: 0.3872)\n",
      "ring_ring_counts_total: Ratio = 1.1490 (Similar: 0.3625, Different: 0.4165)\n",
      "ring_ring_counts_single: Ratio = 1.2605 (Similar: 0.2397, Different: 0.3021)\n",
      "ring_ring_counts_fused: Ratio = 1.0831 (Similar: 0.2803, Different: 0.3036)\n",
      "ring_ring_sizes_6: Ratio = 0.9177 (Similar: 0.3080, Different: 0.2827)\n",
      "Skipping ring_ring_sizes_3 - insufficient variance\n",
      "ring_ring_sizes_5: Ratio = 0.9446 (Similar: 0.2932, Different: 0.2769)\n",
      "Skipping ring_ring_sizes_7 - insufficient variance\n",
      "Skipping ring_ring_sizes_8 - insufficient variance\n",
      "\n",
      "Analyzing post_training embeddings...\n",
      "prop_num_nodes: Ratio = 0.9352 (Similar: 0.5860, Different: 0.5480)\n",
      "prop_num_edges: Ratio = 0.9812 (Similar: 0.5207, Different: 0.5109)\n",
      "prop_avg_node_degree: Ratio = 0.8520 (Similar: 0.6459, Different: 0.5503)\n",
      "prop_avg_path_length: Ratio = 0.9734 (Similar: 0.6401, Different: 0.6231)\n",
      "Skipping prop_clustering_coefficient - insufficient variance\n",
      "prop_graph_diameter: Ratio = 0.9747 (Similar: 0.6061, Different: 0.5908)\n",
      "prop_assortativity: Ratio = 1.1214 (Similar: 0.5002, Different: 0.5609)\n",
      "prop_avg_contrib_mw: Ratio = 0.9991 (Similar: 0.5211, Different: 0.5206)\n",
      "prop_std_contrib_mw: Ratio = 1.0108 (Similar: 0.5630, Different: 0.5690)\n",
      "prop_avg_contrib_logp: Ratio = 0.8074 (Similar: 0.5078, Different: 0.4101)\n",
      "prop_std_contrib_logp: Ratio = 0.9096 (Similar: 0.5202, Different: 0.4732)\n",
      "prop_avg_formal_charge: Ratio = 1.1897 (Similar: 0.4714, Different: 0.5608)\n",
      "prop_std_formal_charge: Ratio = 1.0271 (Similar: 0.4638, Different: 0.4764)\n",
      "prop_avg_hybridization: Ratio = 0.9703 (Similar: 0.5059, Different: 0.4908)\n",
      "prop_std_hybridization: Ratio = 1.2891 (Similar: 0.4634, Different: 0.5974)\n",
      "prop_avg_is_aromatic: Ratio = 0.9800 (Similar: 0.5289, Different: 0.5184)\n",
      "prop_std_is_aromatic: Ratio = 0.9982 (Similar: 0.5069, Different: 0.5060)\n",
      "prop_avg_valence: Ratio = 1.1434 (Similar: 0.4929, Different: 0.5636)\n",
      "prop_std_valence: Ratio = 1.0073 (Similar: 0.4915, Different: 0.4951)\n",
      "prop_avg_degree: Ratio = 0.8520 (Similar: 0.6459, Different: 0.5503)\n",
      "prop_std_degree: Ratio = 1.2162 (Similar: 0.4603, Different: 0.5598)\n",
      "feat_density: Ratio = 0.9993 (Similar: 0.5522, Different: 0.5518)\n",
      "feat_max_centrality: Ratio = 0.8266 (Similar: 0.6185, Different: 0.5112)\n",
      "feat_avg_centrality: Ratio = 0.9993 (Similar: 0.5522, Different: 0.5518)\n",
      "func_conjugated_bonds: Ratio = 1.6015 (Similar: 0.3077, Different: 0.4927)\n",
      "ring_ring_counts_total: Ratio = 1.2655 (Similar: 0.4579, Different: 0.5795)\n",
      "ring_ring_counts_single: Ratio = 0.9846 (Similar: 0.5400, Different: 0.5317)\n",
      "ring_ring_counts_fused: Ratio = 0.9439 (Similar: 0.5393, Different: 0.5091)\n",
      "ring_ring_sizes_6: Ratio = 1.0576 (Similar: 0.4994, Different: 0.5282)\n",
      "Skipping ring_ring_sizes_3 - insufficient variance\n",
      "ring_ring_sizes_5: Ratio = 0.9888 (Similar: 0.5264, Different: 0.5205)\n",
      "Skipping ring_ring_sizes_7 - insufficient variance\n",
      "Skipping ring_ring_sizes_8 - insufficient variance\n",
      "\n",
      "Analyzing epoch_50 embeddings...\n",
      "prop_num_nodes: Ratio = 0.9892 (Similar: 0.5287, Different: 0.5230)\n",
      "prop_num_edges: Ratio = 1.0375 (Similar: 0.5708, Different: 0.5921)\n",
      "prop_avg_node_degree: Ratio = 0.9689 (Similar: 0.4946, Different: 0.4792)\n",
      "prop_avg_path_length: Ratio = 0.9468 (Similar: 0.5282, Different: 0.5001)\n",
      "Skipping prop_clustering_coefficient - insufficient variance\n",
      "prop_graph_diameter: Ratio = 0.8786 (Similar: 0.5801, Different: 0.5097)\n",
      "prop_assortativity: Ratio = 0.7234 (Similar: 0.6443, Different: 0.4661)\n",
      "prop_avg_contrib_mw: Ratio = 0.8845 (Similar: 0.5239, Different: 0.4634)\n",
      "prop_std_contrib_mw: Ratio = 1.1134 (Similar: 0.4490, Different: 0.4999)\n",
      "prop_avg_contrib_logp: Ratio = 1.0257 (Similar: 0.4955, Different: 0.5082)\n",
      "prop_std_contrib_logp: Ratio = 0.9820 (Similar: 0.4784, Different: 0.4698)\n",
      "prop_avg_formal_charge: Ratio = 1.0684 (Similar: 0.4935, Different: 0.5272)\n",
      "prop_std_formal_charge: Ratio = 1.0336 (Similar: 0.4933, Different: 0.5098)\n",
      "prop_avg_hybridization: Ratio = 1.0688 (Similar: 0.5551, Different: 0.5933)\n",
      "prop_std_hybridization: Ratio = 0.9306 (Similar: 0.6182, Different: 0.5753)\n",
      "prop_avg_is_aromatic: Ratio = 1.0627 (Similar: 0.5285, Different: 0.5616)\n",
      "prop_std_is_aromatic: Ratio = 1.0475 (Similar: 0.5320, Different: 0.5573)\n",
      "prop_avg_valence: Ratio = 1.1901 (Similar: 0.5055, Different: 0.6016)\n",
      "prop_std_valence: Ratio = 1.1231 (Similar: 0.4545, Different: 0.5105)\n",
      "prop_avg_degree: Ratio = 0.9689 (Similar: 0.4946, Different: 0.4792)\n",
      "prop_std_degree: Ratio = 1.0548 (Similar: 0.5395, Different: 0.5690)\n",
      "feat_density: Ratio = 0.9218 (Similar: 0.4966, Different: 0.4578)\n",
      "feat_max_centrality: Ratio = 1.0483 (Similar: 0.4812, Different: 0.5045)\n",
      "feat_avg_centrality: Ratio = 0.9218 (Similar: 0.4966, Different: 0.4578)\n",
      "func_conjugated_bonds: Ratio = 1.1187 (Similar: 0.5488, Different: 0.6139)\n",
      "ring_ring_counts_total: Ratio = 1.0105 (Similar: 0.5436, Different: 0.5493)\n",
      "ring_ring_counts_single: Ratio = 0.9767 (Similar: 0.5479, Different: 0.5351)\n",
      "ring_ring_counts_fused: Ratio = 1.0348 (Similar: 0.5225, Different: 0.5407)\n",
      "ring_ring_sizes_6: Ratio = 0.9276 (Similar: 0.5451, Different: 0.5056)\n",
      "Skipping ring_ring_sizes_3 - insufficient variance\n",
      "ring_ring_sizes_5: Ratio = 0.9772 (Similar: 0.5272, Different: 0.5152)\n",
      "Skipping ring_ring_sizes_7 - insufficient variance\n",
      "Skipping ring_ring_sizes_8 - insufficient variance\n",
      "\n",
      "Analyzing final embeddings...\n",
      "prop_num_nodes: Ratio = 1.1135 (Similar: 0.5498, Different: 0.6122)\n",
      "prop_num_edges: Ratio = 0.9891 (Similar: 0.4675, Different: 0.4623)\n",
      "prop_avg_node_degree: Ratio = 0.9591 (Similar: 0.5766, Different: 0.5531)\n",
      "prop_avg_path_length: Ratio = 0.9424 (Similar: 0.5596, Different: 0.5273)\n",
      "Skipping prop_clustering_coefficient - insufficient variance\n",
      "prop_graph_diameter: Ratio = 1.0801 (Similar: 0.3830, Different: 0.4137)\n",
      "prop_assortativity: Ratio = 0.7934 (Similar: 0.6839, Different: 0.5427)\n",
      "prop_avg_contrib_mw: Ratio = 1.0141 (Similar: 0.6275, Different: 0.6364)\n",
      "prop_std_contrib_mw: Ratio = 0.8654 (Similar: 0.6619, Different: 0.5728)\n",
      "prop_avg_contrib_logp: Ratio = 0.9810 (Similar: 0.5485, Different: 0.5381)\n",
      "prop_std_contrib_logp: Ratio = 1.3140 (Similar: 0.4982, Different: 0.6546)\n",
      "prop_avg_formal_charge: Ratio = 0.9775 (Similar: 0.5133, Different: 0.5018)\n",
      "prop_std_formal_charge: Ratio = 1.1353 (Similar: 0.5273, Different: 0.5986)\n",
      "prop_avg_hybridization: Ratio = 0.9047 (Similar: 0.5969, Different: 0.5401)\n",
      "prop_std_hybridization: Ratio = 0.9228 (Similar: 0.4948, Different: 0.4566)\n",
      "prop_avg_is_aromatic: Ratio = 1.4114 (Similar: 0.4389, Different: 0.6195)\n",
      "prop_std_is_aromatic: Ratio = 1.0878 (Similar: 0.5804, Different: 0.6313)\n",
      "prop_avg_valence: Ratio = 1.4195 (Similar: 0.4673, Different: 0.6633)\n",
      "prop_std_valence: Ratio = 1.0284 (Similar: 0.5909, Different: 0.6077)\n",
      "prop_avg_degree: Ratio = 0.9591 (Similar: 0.5766, Different: 0.5531)\n",
      "prop_std_degree: Ratio = 0.8631 (Similar: 0.5502, Different: 0.4749)\n",
      "feat_density: Ratio = 0.9578 (Similar: 0.5903, Different: 0.5654)\n",
      "feat_max_centrality: Ratio = 0.9819 (Similar: 0.5883, Different: 0.5777)\n",
      "feat_avg_centrality: Ratio = 0.9578 (Similar: 0.5903, Different: 0.5654)\n",
      "func_conjugated_bonds: Ratio = 0.7806 (Similar: 0.5050, Different: 0.3942)\n",
      "ring_ring_counts_total: Ratio = 0.9364 (Similar: 0.5613, Different: 0.5256)\n",
      "ring_ring_counts_single: Ratio = 0.9865 (Similar: 0.5294, Different: 0.5222)\n",
      "ring_ring_counts_fused: Ratio = 1.1212 (Similar: 0.5073, Different: 0.5688)\n",
      "ring_ring_sizes_6: Ratio = 0.9547 (Similar: 0.5536, Different: 0.5286)\n",
      "Skipping ring_ring_sizes_3 - insufficient variance\n",
      "ring_ring_sizes_5: Ratio = 1.0051 (Similar: 0.5301, Different: 0.5328)\n",
      "Skipping ring_ring_sizes_7 - insufficient variance\n",
      "Skipping ring_ring_sizes_8 - insufficient variance\n",
      "\n",
      "=== Feature Importance Analysis ===\n",
      "\n",
      "Analyzing pre_training embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing post_training embeddings...\n",
      "\n",
      "Analyzing epoch_50 embeddings...\n",
      "\n",
      "Analyzing final embeddings...\n",
      "\n",
      "=== Property Prediction Comparison ===\n",
      "embedding_type                 property         model  post_training  \\\n",
      "47                 prop_std_is_aromatic  RandomForest      -0.180945   \n",
      "39                  prop_std_contrib_mw  RandomForest      -0.761629   \n",
      "7                 func_conjugated_bonds  RandomForest      -0.217024   \n",
      "43               prop_std_formal_charge  RandomForest      -0.027703   \n",
      "23                 prop_avg_node_degree  RandomForest      -0.254616   \n",
      "..                                  ...           ...            ...   \n",
      "53              ring_ring_counts_single  RandomForest      -0.799493   \n",
      "1                   feat_avg_centrality  RandomForest      -0.822173   \n",
      "5                   feat_max_centrality  RandomForest      -0.935173   \n",
      "51               ring_ring_counts_fused  RandomForest      -0.888237   \n",
      "35                       prop_num_nodes  RandomForest      -1.012760   \n",
      "\n",
      "embedding_type  pre_training  improvement  relative_improvement  \n",
      "47                 -1.293206     1.112261              0.860080  \n",
      "39                 -1.734022     0.972394              0.560773  \n",
      "7                  -1.021330     0.804305              0.787507  \n",
      "43                 -0.777250     0.749547              0.964357  \n",
      "23                 -0.922691     0.668075              0.724050  \n",
      "..                       ...          ...                   ...  \n",
      "53                 -0.471600    -0.327893             -0.695276  \n",
      "1                  -0.424915    -0.397258             -0.934908  \n",
      "5                  -0.333647    -0.601525             -1.802872  \n",
      "51                 -0.274268    -0.613970             -2.238572  \n",
      "35                 -0.244711    -0.768050             -3.138591  \n",
      "\n",
      "[66 rows x 6 columns]\n",
      "\n",
      "=== Feature Importance Comparison ===\n",
      "embedding_type                      feature  post_training  pre_training  \\\n",
      "25                   prop_std_hybridization       0.120071      0.018964   \n",
      "16                         prop_avg_valence       0.054925      0.024170   \n",
      "24                   prop_std_formal_charge       0.049290      0.020570   \n",
      "21                    prop_std_contrib_logp       0.065327      0.043580   \n",
      "13                     prop_avg_is_aromatic       0.034258      0.014610   \n",
      "23                          prop_std_degree       0.034289      0.019550   \n",
      "26                     prop_std_is_aromatic       0.030334      0.016227   \n",
      "18                      prop_graph_diameter       0.042964      0.033081   \n",
      "12                   prop_avg_hybridization       0.042813      0.035001   \n",
      "14                     prop_avg_node_degree       0.023332      0.017541   \n",
      "9                       prop_avg_contrib_mw       0.043247      0.039581   \n",
      "10                          prop_avg_degree       0.024688      0.021341   \n",
      "0                       feat_avg_centrality       0.015463      0.015003   \n",
      "1                              feat_density       0.013851      0.013419   \n",
      "11                   prop_avg_formal_charge       0.025157      0.024843   \n",
      "35                        ring_ring_sizes_8       0.000646      0.000368   \n",
      "28                   ring_ring_counts_fused       0.005078      0.004906   \n",
      "2                           feat_has_cycles       0.000000      0.000000   \n",
      "4                         feat_is_connected       0.000000      0.000000   \n",
      "5                       feat_max_centrality       0.010030      0.010165   \n",
      "34                        ring_ring_sizes_7       0.000504      0.001297   \n",
      "17              prop_clustering_coefficient       0.001870      0.003798   \n",
      "31                        ring_ring_sizes_3       0.001047      0.003379   \n",
      "33                        ring_ring_sizes_6       0.010124      0.013798   \n",
      "20                           prop_num_nodes       0.008099      0.013475   \n",
      "3                         feat_is_bipartite       0.003190      0.009171   \n",
      "27                         prop_std_valence       0.049623      0.057647   \n",
      "19                           prop_num_edges       0.017244      0.026240   \n",
      "30                   ring_ring_counts_total       0.006985      0.017616   \n",
      "29                  ring_ring_counts_single       0.014062      0.034542   \n",
      "32                        ring_ring_sizes_5       0.013956      0.035676   \n",
      "8                     prop_avg_contrib_logp       0.043925      0.070435   \n",
      "22                      prop_std_contrib_mw       0.037165      0.065249   \n",
      "7                        prop_assortativity       0.029335      0.057758   \n",
      "6                     func_conjugated_bonds       0.084427      0.125583   \n",
      "15                     prop_avg_path_length       0.042679      0.091417   \n",
      "\n",
      "embedding_type  abs_change  rel_change  \n",
      "25                0.101106    5.331098  \n",
      "16                0.030755    1.272391  \n",
      "24                0.028720    1.396168  \n",
      "21                0.021747    0.499011  \n",
      "13                0.019647    1.344664  \n",
      "23                0.014739    0.753910  \n",
      "26                0.014108    0.869362  \n",
      "18                0.009883    0.298746  \n",
      "12                0.007812    0.223192  \n",
      "14                0.005791    0.330098  \n",
      "9                 0.003665    0.092602  \n",
      "10                0.003347    0.156811  \n",
      "0                 0.000460    0.030651  \n",
      "1                 0.000432    0.032192  \n",
      "11                0.000315    0.012669  \n",
      "35                0.000278    0.751519  \n",
      "28                0.000172    0.035119  \n",
      "2                 0.000000    0.000000  \n",
      "4                 0.000000    0.000000  \n",
      "5                -0.000135   -0.013274  \n",
      "34               -0.000793   -0.610820  \n",
      "17               -0.001928   -0.507447  \n",
      "31               -0.002332   -0.689893  \n",
      "33               -0.003674   -0.266246  \n",
      "20               -0.005375   -0.398887  \n",
      "3                -0.005980   -0.652049  \n",
      "27               -0.008023   -0.139181  \n",
      "19               -0.008995   -0.342802  \n",
      "30               -0.010631   -0.603456  \n",
      "29               -0.020480   -0.592891  \n",
      "32               -0.021720   -0.608806  \n",
      "8                -0.026510   -0.376367  \n",
      "22               -0.028084   -0.430406  \n",
      "7                -0.028422   -0.492084  \n",
      "6                -0.041156   -0.327719  \n",
      "15               -0.048738   -0.533137  \n",
      "\n",
      "=== Sensitivity Ratio Comparison ===\n",
      "embedding_type                 property  post_training  pre_training  \\\n",
      "4                    prop_assortativity       1.121360      0.724898   \n",
      "21               prop_std_hybridization       1.289102      0.975776   \n",
      "8                prop_avg_formal_charge       1.189653      0.957937   \n",
      "3                 func_conjugated_bonds       1.601462      1.372890   \n",
      "19                      prop_std_degree       1.216230      1.007676   \n",
      "28                    ring_ring_sizes_6       1.057621      0.917743   \n",
      "26               ring_ring_counts_total       1.265523      1.149002   \n",
      "20               prop_std_formal_charge       1.027149      0.941100   \n",
      "10                 prop_avg_is_aromatic       0.980018      0.918653   \n",
      "13                     prop_avg_valence       1.143363      1.095391   \n",
      "27                    ring_ring_sizes_5       0.988775      0.944610   \n",
      "14                  prop_graph_diameter       0.974674      0.958482   \n",
      "0                   feat_avg_centrality       0.999298      1.002063   \n",
      "1                          feat_density       0.999298      1.002063   \n",
      "18                  prop_std_contrib_mw       1.010760      1.024062   \n",
      "9                prop_avg_hybridization       0.970308      0.995369   \n",
      "22                 prop_std_is_aromatic       0.998214      1.049232   \n",
      "23                     prop_std_valence       1.007334      1.059663   \n",
      "6                   prop_avg_contrib_mw       0.999113      1.069170   \n",
      "11                 prop_avg_node_degree       0.851998      0.942423   \n",
      "7                       prop_avg_degree       0.851998      0.942423   \n",
      "15                       prop_num_edges       0.981204      1.097991   \n",
      "24               ring_ring_counts_fused       0.943918      1.083114   \n",
      "25              ring_ring_counts_single       0.984600      1.260499   \n",
      "16                       prop_num_nodes       0.935195      1.296369   \n",
      "5                 prop_avg_contrib_logp       0.807430      1.174355   \n",
      "17                prop_std_contrib_logp       0.909608      1.319462   \n",
      "2                   feat_max_centrality       0.826563      1.241182   \n",
      "12                 prop_avg_path_length       0.973431      1.635112   \n",
      "\n",
      "embedding_type  abs_change  rel_change  \n",
      "4                 0.396462    0.546920  \n",
      "21                0.313326    0.321105  \n",
      "8                 0.231716    0.241890  \n",
      "3                 0.228571    0.166489  \n",
      "19                0.208553    0.206964  \n",
      "28                0.139878    0.152415  \n",
      "26                0.116521    0.101410  \n",
      "20                0.086049    0.091435  \n",
      "10                0.061365    0.066799  \n",
      "13                0.047972    0.043794  \n",
      "27                0.044165    0.046755  \n",
      "14                0.016192    0.016893  \n",
      "0                -0.002765   -0.002759  \n",
      "1                -0.002765   -0.002759  \n",
      "18               -0.013303   -0.012990  \n",
      "9                -0.025061   -0.025178  \n",
      "22               -0.051018   -0.048624  \n",
      "23               -0.052329   -0.049383  \n",
      "6                -0.070057   -0.065525  \n",
      "11               -0.090426   -0.095950  \n",
      "7                -0.090426   -0.095950  \n",
      "15               -0.116787   -0.106364  \n",
      "24               -0.139196   -0.128514  \n",
      "25               -0.275900   -0.218881  \n",
      "16               -0.361174   -0.278604  \n",
      "5                -0.366925   -0.312448  \n",
      "17               -0.409854   -0.310622  \n",
      "2                -0.414620   -0.334052  \n",
      "12               -0.661681   -0.404670  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Embedding Visualization ===\n",
      "Visualizing with properties: ['ring_ring_sizes_7', 'ring_ring_sizes_8', 'prop_std_formal_charge', 'prop_std_hybridization']\n",
      "\n",
      "Visualizing pre_training embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Malli\\anaconda3\\envs\\baceenv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Malli\\anaconda3\\envs\\baceenv\\lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualizing post_training embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Malli\\anaconda3\\envs\\baceenv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Malli\\anaconda3\\envs\\baceenv\\lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualizing epoch_50 embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Malli\\anaconda3\\envs\\baceenv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Malli\\anaconda3\\envs\\baceenv\\lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualizing final embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Malli\\anaconda3\\envs\\baceenv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Malli\\anaconda3\\envs\\baceenv\\lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualization figures to ./figures\n",
      "\n",
      "Analysis complete! Results and visualizations have been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Malli\\AppData\\Local\\Temp\\ipykernel_64644\\248614274.py:181: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  radar_df['category'] = radar_df['feature'].apply(group_feature)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced visualizations saved to ./enhanced_figures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Malli\\AppData\\Local\\Temp\\ipykernel_64644\\3425878455.py:119: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional visualizations saved to ./enhanced_figures\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set paths to your specific files\n",
    "    metadata_path = \"./embeddings/metadata/molecule_metadata_20250303_112449.pkl\"\n",
    "    \n",
    "    # Define embedding files with their types\n",
    "    embedding_files = {\n",
    "        'pre_training': \"./embeddings/pre_training_embeddings_20250303_112449.pkl\",\n",
    "        'post_training': \"./embeddings/post_training_embeddings_20250303_112449.pkl\",\n",
    "        'epoch_50': \"./embeddings/epoch_50_embeddings_20250303_112449.pkl\",\n",
    "        'final': \"./embeddings/final_embeddings_20250303_112446.pkl\"\n",
    "    }\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = EmbeddingAnalyzer(metadata_path, embedding_files)\n",
    "    \n",
    "    # Load embeddings\n",
    "    analyzer.load_embeddings()\n",
    "       \n",
    "    # After running the full analysis\n",
    "    analyzer.run_all_analyses()\n",
    "\n",
    "    # Create all visualizations\n",
    "    create_enhanced_visualizations(analyzer)\n",
    "    create_additional_visualizations(analyzer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
